\documentclass[a4paper,onecolumn]{article}
\usepackage{amsmath, amsthm, graphicx, amssymb, wrapfig, fullpage, subfigure, array, algorithm2e}
\usepackage[font=sl, labelfont={sf}, margin=1cm]{caption}
\DeclareMathOperator{\e}{e}
\begin{document}
\setcounter{page}{1}

\title{Sparse Greedy Gaussian Process Regression}
\date{}
\author{Han Chen}
\maketitle
\section{Maximize log posterior}
Finite set inputs $X = \{x_1,\cdots, x_m\}$.
$
    y(x) = t(x)+\xi
$
where $\xi\sim \mathcal{N}(0, \sigma^2)$ and $(t_m) \sim \mathcal{N}(0, K)$.\\

Instead, assume $y$ is generated by
$$
    y = K\alpha + \xi
$$
where $\alpha \sim \mathcal{N}(0, K^{-1})$ and $\xi \sim \mathcal{N}(0,\sigma^2\mathbf{1})$.\\
The posterior $p(\alpha|y, X)$ is proportional to
$$
    \Pi = \exp(-\frac{1}{2 \sigma^2} \|y-K\alpha\|^2) \exp(-\frac{1}{2}\alpha^T K \alpha)
$$
Let the maximizer be $\alpha_{opt}$.
Conditional expectation for $y(x)$ (new $x$) is
$$
    \mathbb{E}[y(x)| y, X] =  k^T \alpha_{opt}\,,
$$
where $k = \big( k(x_1, x), \cdots, k(x_m, x)\big)$.
We have
$$
    -\sigma^2 \log \Pi - \frac{1}{2}y^T y=
    - y^T K\alpha + \frac{1}{2} \alpha^T \big( \sigma^2 K + K^T K \big)\alpha
$$
Therefore, $\alpha_{opt}$ minimizes $-\sigma^2 \log \Pi - \frac{1}{2}y^T y$.\\
Posterior mean $k^T (K+\sigma^2 \mathbf{1})^{-1} y$, posterior variance
$k(x,x)+ \sigma^2 - k^T (K+\sigma^2\mathbf{1})^{-1}k$.\\
We have 
$$
    \alpha_{opt} = (K+\sigma^2 \mathbf{1})^{-1} y
$$
\section{Inequalities}
For any positive semidefinite square matrix $K$, and vectors $v, \xi, \eta$, define
\begin{equation*}\begin{split}
    Q_v(\xi) & \equiv - v^T K \xi + \frac{1}{2} \xi^T (\sigma^2 K + K^T K) \xi\\
    Q^*_v(\eta) &\equiv - v^T \eta + \frac{1}{2}\eta^T (\sigma^2 \mathbf{1} + K)\eta
\end{split}\end{equation*}
For all $\xi, \eta$, we have
\begin{equation*}\begin{split}
    Q_v(\xi) &\ge Q_{v}^{\min} \ge -\frac{1}{2}\|v\|^2 - \sigma^2 Q^*_v(\eta)\\
    Q_v^*(\eta) &\ge Q^{*\, \min}_v \ge \sigma^{-2} \left(- \frac{1}{2}\|v\|^2 - Q_v(\xi)\right)
\end{split}\end{equation*}
Equalities hold when $Q_v(\xi) = Q_v^{\min}$ and $Q_v(\eta) = Q^{*\, \min}_v$, that is
$\alpha=\alpha_{opt}$ (notice $\xi,\eta=\alpha_{opt}$ minimizes both $Q_v(\xi)$ and $Q_v^*(\xi)$). 

\section{Error bounds}
We have 
\begin{equation*}\begin{split}
    & \textrm{Var}\big[ y(x) \Big| y, X \big] 
    = k(x,x)+ \sigma^2 + 2Q^{*\, min}_k \le k(x,x) + \sigma^2 + 2Q^*_k(\eta)
\end{split}\end{equation*}
for any $\eta$, which gives an upper bound of the variance.\\
The lower bound of the variance is given by
$$
    \textrm{Var}\big[ y(x) \Big| y, X \big] \ge k(x,x) + \sigma^2 + 2\sigma^{-2} \left(
     -\frac{1}{2} \|k\|^2 - Q_k(\xi)
    \right)
$$
for any $\xi$.\\
Define ``gap'' to be 
$$
    \frac{\textrm{Upper bound - lower bound}}{\textrm{Average variance reduction computed from upper/lower bound}}
$$
We have
$$
    \textrm{gap}(\xi, \eta) = \frac{2\Big(Q_k(\xi) + \sigma^2 Q^*_k(\eta) + \frac{1}{2}\|k\|^2\Big)}{
    - Q_k(\xi) + \sigma^2Q^*_k(\eta) - \frac{1}{2}\|k\|^2}\,,
$$
which is used as the stopping rule.
\section{Model reduction}
Define $P \in \mathbb{R}^{m\times n}$, $m\ge n$, with $P^T P = \mathbf{1}$. Let
$$
    \alpha_P \equiv P\beta\,,
$$
where $\beta \in \mathbb{R}^n$. The minimizer of $Q_y(\alpha_P)$ and $Q_y^*(\alpha_P)$ is
$$
    \beta_{opt} = \Big(
        P^T (\sigma^2 K + K^T K) P 
    \Big)^{-1} P^T K^Ty = \Big(
        \sigma^2 P^T (KP) + (KP)^T (KP)
    \Big)^{-1} (KP)^T y
$$
$$
    \beta^*_{opt} = (P^T K P + \sigma^2)^{-1} P^T y
$$
If $m=n$ and $P$ is full-rank, then $P\beta_{opt} = \alpha_{opt}$.
Therefore,
\begin{equation*}\begin{split}
    Q_y(P\beta) &= - y^T (KP) \beta + \frac{1}{2} \sigma^2 \beta^T (P^T K P)\beta 
    + \frac{1}{2} \beta^T (KP)^T (KP) \beta\\
    Q_y^*(P\beta) &= - y^T P\beta + \frac{1}{2} \sigma^2 \beta^T \beta + \frac{1}{2}\beta(P^T K P) \beta
\end{split}\end{equation*}
We choose $P$ as a collection of unit vectors $\mathbf{e}_i$ where $(\mathbf{e}_i)_j = \delta_{ij}$.
The statements hold when we replace $y$ with $k$.

\section{Algorithm}
\begin{algorithm}
    \KwData{$X=\{x_1, \cdots, x_m\}$, targets $y$, noise $\sigma^2$, precision $\epsilon$}
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{index sets $I, I^*=\{1, \cdots, m\}$, $S, S^*=\emptyset$}
    \While{
        $Q_k(P\beta_{opt}) + \sigma^2 Q^*_k(P^*\beta^*_{opt}) + \frac{1}{2}\|k\|^2
        \le \frac{\epsilon}{2} \Big(
        - Q_k(P\beta_{opt}) + \sigma^2 Q^*_k(P^*\beta_{opt}) - \frac{1}{2}\|k\|^2
        \Big)$
    }{
        Choose $M\subseteq I, \; M^* \subseteq I^*$\\
        Find $\arg\min_{i\in M} Q_k([P, e_i]\beta_{opt}^i)$ and $\arg\min_{i^*\in M^*} Q_k([P^*, e_i^*]\beta_{opt}^{*\,i})$\\
        Move $i$ from $I$ to $S$, move $i^*$ from $I^*$ to $S^*$.\\
        Set $P := [P, e_i]$, $P^* := [P^*, e_i^*]$.
    }
    \Output{
        $S$, $\beta_{opt}$, $Q_y^*(P^*\beta_{opt}^*)$.
    }
\end{algorithm}
The paper here has many confusions, such as mixing $Q_k$ with $Q_y$, $\beta$ with $\beta^*$, $k$ with $y$. Stop proceeding.
\newpage
\section{Sparse likelihood approximation}
Given samples $S=\left\{ (x_1, y_1), \cdots, (x_n, y_n) \right\}$, introduce latent variables $u$ such that
$P(y|u) = \mathcal{N}(y|u, \sigma^2)$. Denote the latent variables at the training points to be
$u = \Big(u(x_1), \cdots, u(x_n)\Big)$, and the covariance of $u$ to be $\mathbf{K} = (K(x_i, x_j))_{i,j}\in \mathbb{R}^{n,n}$.
We have $P({u}) = \mathcal{N}({u}|\mathbf{0}, \mathbf{K})$.\\
To predict $u_*$ at $x_*$, we have
\begin{equation*}\begin{split}
    \mu_* &= k_*^T (\mathbf{K}+\sigma^2 \mathbf{I})^{-1} y\, , \quad k_* = \big(K(x_*, x_i)\big)_i\\
    \sigma^2_* &= K(x_*, x_*) - k_*^T (\mathbf{K} + \sigma^2 \mathbf{I})^{-1} k_*
\end{split}\end{equation*}
However, this is costly to compute.\\

We replace the likelihood $\mathbb{P}(y|u)$ by (sparse likelihood)
$$
    Q(y|u_I) = \mathcal{N}\Big(y\Big| \mathbf{P}_I^T u_I, \sigma^2 \mathbf{I}\Big),\; \mathbf{P}_I = \mathbf{K}_I^{-1} \mathbf{K}_{I,\cdot}\,,
$$
where $P_I^T u_I = \mathbb{E}[u| u_I]$.
Consider all distributions of the form $\propto \mathbb{P}(u)R(u_I)$, where $\mathbb{P}(u)$ indicates the prior of $u$.
$R(u_I) = Q(y|u_I)$ minimizes the K-L divergence $\mathcal{D}\Big[\mathbb{P}(u)R(u_I) \Big\| \mathbb{P}[u|y]\Big]$.\\

Let 
\begin{equation*}\begin{split}
    \mathbf{K}_I &= \mathbf{L}\mathbf{L}^T \qquad (\textrm{Cholesky})\\
    \mathbf{V} &= \mathbf{L}^{-1} \mathbf{K}_{I, \cdot}\\
    \mathbf{M} &= \sigma^2 \mathbf{I} + \mathbf{V}\mathbf{V}^T
\end{split}\end{equation*}
The approximate posterior of $u_I$ can be obtained from the sparse likelihood:
\begin{equation*}\begin{split}
    &Q(u_I|y) = \mathbb{P}(u_I) Q(y|u_I) \\
    =& \mathcal{N}(u_I| \mathbf{0}, \mathbf{L}\mathbf{L}^T) \cdot
    \mathcal{N}(y| K_{I, \cdot}^T K_I^{-1}u_I, \sigma^2 \mathbf{I})\\
    =& \mathcal{N} \big( u_I\big| \mathbf{L}\mathbf{M}^{-1} \mathbf{V} y, \sigma^2 \mathbf{L}\mathbf{M}^{-1}\mathbf{L}^T\big)
\end{split}\end{equation*}
To compute the approximate posterior of $u_*=u_*(x_*)$ at a new $x_*$, we define
\begin{equation*}\begin{split}
    \mathbf{M} &= \mathbf{L}_M\mathbf{L}_M^T\\
    \beta &= \mathbf{L}_M^{-1} \mathbf{V}y\\
    k_{I*} &= (K(x_i, x_*))_{i\in I}\\
    l_* &= \mathbf{L}^{-1} k_{I*}\\
    l_{M*} &= \mathbf{L}_M^{-1} l_*
\end{split}\end{equation*}
We have
\begin{equation*}\begin{split}
    Q(u_*|y) &= \int_{u_I} \mathbb{P}(u_*|u_I) Q(u_I|y) \;d u_I\\
    &= \int_{u_I} \mathcal{N}\big(
        u_*\big| k_{I*}^T \mathbf{K}_I^{-1} u_I,\, K(x_*, x_*) - k_{I*}^T\mathbf{K}_I^{-1} k_{I*} \big) \cdot
       \mathcal{N}\big(u_I\big| \mathbf{L}\mathbf{M}^{-1} \mathbf{V}y,\, \sigma^2 \mathbf{L}\mathbf{M}^{-1} \mathbf{L}^T\big) \; du_I\\
    &= \mathcal{N}\big(u_*\big|l_{M*}^T \beta,\, K(x_*, x_*) - \|l_*\|^2 + \sigma^2 \|l_{M,*}\|^2\big)
\end{split}\end{equation*}
Notice the posterior mean
$$
    \mu(x_*) = k_{I*}^T \mathbf{L}^{-T}\mathbf{L}_M^{-T} \beta
$$
and the posterior variance
\begin{equation*}\begin{split}
    \sigma^2 (x_*) &= K(x_*, x_*) - k_{I*}^T \mathbf{L}^{-T} \mathbf{L}^{-1} k_{I*} + \sigma^2 k_{I*}^T \mathbf{L}^{-T} \mathbf{M}^{-1} \mathbf{L}^{-1} k_{I*}\\
    &=  K(x_*, x_*) - k_{I*}^T \mathbf{L}^{-T} \mathbf{V} \mathbf{M}^{-1} \mathbf{V}^T \mathbf{L}^{-1} k_{I*} \qquad (\textrm{Woodbury identity})
\end{split}\end{equation*}
Therefore, we need to pre-compute
$ \mathbf{L}^{-T}\mathbf{L}_M^{-T} \beta$ and $ \mathbf{L}^{-T} \mathbf{V} \mathbf{M}^{-1} \mathbf{V}^T \mathbf{L}^{-1}$.\\

\section{Inclusion of a new point}
Define
$$
    {p} = \textrm{diag}(\mathbf{V}^T \mathbf{V})\,, \quad {q} = \textrm{diag}(\mathbf{V}^T \mathbf{M}^{-1}\mathbf{V})
$$
Let $\cdot^\prime$ be the quantity associated with $\{I,i\}$ active set. We have
$$
    \mathbf{L}^\prime_{d+1, \cdot\backslash d+1} = \big(\mathbf{L}^{-1} \mathbf{K}_{I,i}\big)^T \equiv v_i^T
$$
$$
    \mathbf{L}^\prime_{d+1, d+1} = \big( K(x_i, x_i) - v_i^T v_i\big)^{1/2}
$$
$$
    \mathbf{V}^\prime_{1\cdots d,\cdot} = \mathbf{V}
$$
$$
    \mathbf{V}^\prime_{d+1,\cdot} = \frac{1}{\mathbf{L}^\prime_{d+1,d+1}}\big( \mathbf{K}_{\cdot,i} - \mathbf{V}^T v_i \big) 
$$
$$
    p^\prime = p + \Big(\big(\mathbf{V}^\prime_{d+1,j}\big)^2\Big)_j
$$

\section{Application to twin-model-GPO}
Given samples $S=\{(x_1, \xi_1, \xi_{\tilde{\nabla}1}), \cdots, (x_n, \xi_n, \xi_{\tilde{\nabla}n})\}$, introduce
latent variables $u=(\xi,\xi_{\nabla})\in\mathbb{R}^{n(d+1)}$ such that
$$
    \mathbb{P}(\xi_{\tilde{\nabla}}) = \mathcal{N}\big( \xi_{\tilde{\nabla}}\big| \xi_{\nabla}, \bar{\mathbf{G}} \big)\,, 
    \qquad
    \mathbb{P}(\xi, \xi_\nabla) = \mathcal{N}\big(\xi, \xi_{\nabla}\big| 0, 
     \begin{pmatrix} 
         \mathbf{D} & \mathbf{H}\\
         \mathbf{H}^T & \mathbf{E}
     \end{pmatrix} := \mathbf{K}
     \big)
$$
where $\bar{\mathbf{G}}\in\mathbb{R}^{nd\times nd}$.
Consider a subset of indices $I:=\{1,\cdots, n\}\cup I_\nabla $, where 
$I_\nabla \subseteq T_{\nabla}:=\{n+1,\cdots, n+nd\}$.
Denote $u_I, u_{I_\nabla}$ the subset of latent variables of $u$ indexed by $I, I_\nabla$.
We approximate likelihood $\mathbb{P}\big( \xi, \xi_{\tilde{\nabla}} \big| u\big)$ by
$$
    Q\big( \xi, \xi_{\tilde{\nabla}} \big| u_I\big) =
    \mathcal{N}\big(\xi_{\tilde{\nabla}}\big|\mathbf{P}_{I_\nabla}^T u_{I_\nabla}, \bar{\mathbf{G}}\big)
    \,,
$$
where 
$$
    \mathbf{P}_{I_\nabla} = \mathbf{K}_{I_\nabla}^{-1} \mathbf{K}_{I_\nabla, T_\nabla}
$$
Define 
$$
    \mathbf{P}_I = \begin{pmatrix} \mathbf{0}_{n\times nd} \\ \mathbf{P}_{I_\nabla}\end{pmatrix}
$$
Notice 
$$
    \mathbf{P}_I^T u_I =  \mathbf{P}_{I_\nabla}^T u_{I_\nabla}
$$
The approximate posterior of $u_I$ is given by
$$
    Q(u_I|\xi,\xi_{\tilde{\nabla}}) = \delta(u_{\{1,\cdots, n\}}, \xi) \cdot
    \mathcal{N}\Big( u_I\Big| (\mathbf{K}_I^{-1} + \mathbf{P}_I\bar{\mathbf{G}}^{-1} \mathbf{P}_I^T)^{-1}
    \mathbf{P}_I\bar{\mathbf{G}}^{-1}\xi_{\tilde{\nabla}}\,,\;
    (\mathbf{K}_I^{-1} + \mathbf{P}_I \bar{\mathbf{G}}^{-1} \mathbf{P}_I^T)^{-1} \Big)\,.
$$
To evalute the posterior mean and variance at new point $x_*$, define a length $\big|I\big|$ vector:
$k_{I*}$
which indicates the covariance between $\xi(x_*)$ and $u_I = \big(\xi,\xi_\nabla\big)$.
We have
\begin{equation*}\begin{split}
    Q(u_*|\xi,\xi_{\tilde{\nabla}}) &= \int_{u_I} \mathbb{P}(u_*|u_I) Q(u_I|\xi,\xi_{\tilde{\nabla}}) \;d u_I\\
    &= \int_{u_I} \mathcal{N}\big(
        u_*\big| k_{I*}^T \mathbf{K}_I^{-1} u_I,\, K(x_*, x_*) - k_{I*}^T\mathbf{K}_I^{-1} k_{I*} \big) \cdot\\
    \quad &
   \delta(u_{\{1,\cdots, n\}}, \xi) \cdot
    \mathcal{N}\Big( u_I\Big| (\mathbf{K}_I^{-1} + \mathbf{P}_I\bar{\mathbf{G}}^{-1} \mathbf{P}_I^T)^{-1}
    \mathbf{P}_I\bar{\mathbf{G}}^{-1}\xi_{\tilde{\nabla}}\,,\;
    (\mathbf{K}_I^{-1} + \mathbf{P}_I \bar{\mathbf{G}}^{-1} \mathbf{P}_I^T)^{-1} \Big)
\; du_I
\end{split}\end{equation*}
Define
$$
    \mathbf{S} = \big(\mathbf{0}_{|I_\nabla|\times n} , \, \mathbf{I}_{|I_\nabla|} \big)
$$
The posterior mean of $\xi(x_*)$ is
$$
    k_{I*}^T \mathbf{K}_I^{-1} \begin{pmatrix} 
    \xi^T\, \textbf{,}\; \left(\mathbf{S} (\mathbf{K}_I^{-1} + \mathbf{P}_I\bar{\mathbf{G}}^{-1} \mathbf{P}_I^T)^{-1}
    \mathbf{P}_I\bar{\mathbf{G}}^{-1}\xi_{\tilde{\nabla}}\right)^T
    \end{pmatrix}^T\,,
$$
and the posterior variance is
$$
    K(x_*, x_*) - k_{I*}^T\mathbf{K}_I^{-1} k_{I*}  +  k_{I*}^T \mathbf{K}_I^{-1}
    \mathbf{S}^T(\mathbf{K}_I^{-1} + \mathbf{P}_I \bar{\mathbf{G}}^{-1} \mathbf{P}_I^T)^{-1}\mathbf{S}
    \mathbf{K}_I^{-T} k_{I*}
$$

\section{From Lehel Csato's thesis}
$f_\mathbf{x}$ (GP) used as latent variable in a likelihood $P(y|\mathbf{x}, f_\mathbf{x})$.
$K_0(\mathbf{x},\mathbf{x}^\prime) = Cov(f_\mathbf{x}, f_{\mathbf{x}^\prime})$. Training data
$\{\mathbf{x}_n, y_n\}_{n=1}^N$. Assume $f_\mathbf{x}$ has zero mean.
The kernel can be decomposed into dictionary $\phi_i(\mathbf{x})_{i=1}^\infty$. The dictionary defines
the feature space $\mathcal{{F}}$.
$$
    <f(\mathbf{x}, f(\mathbf{x}^\prime))> = K(\mathbf{x}, \mathbf{x}^\prime) = 
    \sum_{i=1}^\infty \sigma^2 \phi_i(\mathbf{x}) \phi_i(\mathbf{x}^\prime)
$$
If there are $k$ elements in the dictionary, the feature space $\mathcal{F}=\mathbb{R}^k$.
The projection function is
$$
    \phi(\mathbf{x}) = \big( \sigma_1 \phi_1(\mathbf{x}), \cdots \big)^T\,,
$$
which is the image of $\mathbf{x}$ in the feature space.
$$
    K(\mathbf{x}, \mathbf{x}^\prime) = \phi_{\mathbf{x}}^T \phi_{\mathbf{x}^\prime}
$$
Let $K_\mathcal{X}$ be the kernel $K_0$ evaluated at sample locations $\mathcal{X}=(\mathbf{x}_1, \cdots)$
\end{document}





