%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Estimate the Gradient by Using the Space-time Solution}
\label{chapter 2}
This chapter develops a method to estimate the gradient by using the space-time solution
of gray-box conservation law simulations.\\

Chapter \ref{chap 1} considered a code which simulates a conservation law 
\eqref{eqn: 1D conceptual} with an unknown $F$,
\begin{equation*}\begin{split}
    &\frac{\partial u}{\partial t} + \frac{\partial F(u)}{\partial x} = c\,,\quad
    x \in [0,1] \;, \;\; t\in[0,1]\,,
    \label{eqn: 1D conceptual repeat}
\end{split}\end{equation*}
with proper initial and boundary conditions, for a control variable
$c$.
Such simulator is named gray-box, and its discretized space-time solution is named gray-box solution. 
It is explained that $F$ can be approximated up to 
a constant for values of $u$
that appeared in the gray-box solution, by utilizing the gray-box solution.
Therefore, a twin model that simulates \eqref{eqn: 1D conceptual inferred},
\begin{equation*}
    \frac{\partial \tilde{u}}{\partial t} + \frac{\partial \tilde{F}(\tilde{u})}{\partial x} = c\,,\quad
    x \in [0,1] \;, \;\; t\in[0,1]\,,
    \label{eqn: 1D conceptual inferred repeat}
\end{equation*}
can be obtained, where $\tilde{F}$ is the approximated flux. 
It is also explained that the adjoint method can be applied to the twin model to estimate the gradient of 
any objective function with respect to $c$. 
Finally, it is envisioned that the adjoint gradient of the twin model can drive the optimization of 
the objective function constrained by the gray-box model.\\


The above example involves only one equation and one dimensional space.
This chapter develops a more general
procedure suitable for systems of equations and for problems with a spatial dimension greater than one.

\section{Approach}
\label{infer}
This section discusses the general approach for training a twin model.
In particular, the metric of solution mismatch is presented. 
We then study what aspects of $F$ can be 
inferred by using the metric for a special case of conservation laws.
Besides, another metric, the integrated truncation error, is proposed.
The latter metric has less theoretical backup but can be cheaper to evaluate and
useful in practice, which
will be demonstrated in Section \ref{sec: twin numerical results}.
The relationship of the two metrics is studied. Finally, we discuss the method
to minimize the two metrics.\\

Consider a gray-box simulator that solves the PDE \eqref{eqn: govern PDE}, 
\begin{equation*}\begin{split}
    \frac{\partial u}{\partial t}+ \nabla \cdot \big( D F(u) \big) = q(u,c)\,,
\end{split}
\end{equation*}
a system of $k$ equations, for $u(t,x)$ with $t\in[0,T]$
and $x\in\Omega$. The PDE has an unknown flux $F$, but known source term $q$, and
known initial and boundary conditions.
Let its discretized space-time solution be $\boldsymbol{u}$.
My thesis introduces an open-box simulator solving another PDE, namely the twin model,
\begin{equation}\begin{split}
    \frac{\partial \tilde{u}}{\partial t}+ \nabla \cdot \big(D \tilde{F}(\tilde{u})\big) = q(\tilde{u},c)\,,
\end{split}
\label{eqn: govern twin model}
\end{equation}
which is also a system of $k$ equations
with the same source term and the same initial and boundary conditions. 
Equation \eqref{eqn: govern twin model}
differs from \eqref{eqn: govern PDE} in its flux. 
For simplicity,
let the solution of the open-box simulator, $\tilde{\boldsymbol{u}}$, be defined on the same 
spatial grid points and time steps of the gray-box simulator. \\

The metric used to measure the difference of the twin model and the gray-box model is the
solution mismatch.
The solution mismatch is defined to be
\begin{equation}
    \mathcal{M}_u(\tilde{F}) = \sum_{i=1}^M \sum_{j=1}^N w_{ij} \big( \tilde{\boldsymbol{u}}_{ij}
     -\boldsymbol{u}_{ij}\big)^2 \,,
    \label{eqn: solution mismatch}
\end{equation}
which approximates
\begin{equation}
    \int_{0}^T\int_\Omega \big(\tilde{u}(t,x) - u(t,x)\big)^2 \, \textrm{d}x\,\textrm{d}t\,.
    \label{eqn: cont sol mismatch}
\end{equation}
In \eqref{eqn: solution mismatch}, $i=1,\cdots, M$ are the indices for time grid, and $j=1,\cdots, N$ are the indices for the space grid.
$w_{ij}$'s are the quadrature weights defined with respect to \eqref{eqn: cont sol mismatch}. 
For example, if a uniform Cartesian space-time grid is used, the quadrature weights equal a constant.
Notice that $\mathcal{M}_u$ depends solely on
$\tilde{F}$ through the twin model solution $\tilde{\boldsymbol{u}}$ if the 
quadrature weights and the gray-box solution are given.\\


Given a function space $\mathcal{S}_F$,
I propose to infer a flux $\tilde{F}$ such that $\mathcal{M}_u$ is minimized,
\begin{equation}
    \tilde{F}^* = \argmin_{\tilde{F} \in \mathcal{S}_F} \mathcal{M}_u\,.
    \label{eqn: twin min mismatch}
\end{equation}
The choice for $\mathcal{S}_F$ will be discussed later in Section \ref{sec: flux param}. 
By setting the $F$ in \eqref{eqn: govern twin model} to be $\tilde{F}^*$,
one obtain a trained twin-model equation
\begin{equation}\begin{split}
    \frac{\partial \tilde{u}}{\partial t}+ \nabla \cdot \big(D \tilde{F}^*(\tilde{u})\big) = q(\tilde{u},c)\,,
\end{split}
\label{eqn: govern twin model trained}
\end{equation}
Let $\tilde{\boldsymbol{u}}^*$ be the space-time solution of the twin model governed by \eqref{eqn: govern twin model trained}.
Given $\tilde{F}^*$, $\tilde{\boldsymbol{u}}^*$ depends on $c$. 
The gradient of any objective function $\xi(\tilde{\boldsymbol{u}}^*, c)$ with respect to $c$
can be obtained by applying the adjoint method to the trained twin model.
The gradient $\frac{d \xi(\tilde{\boldsymbol{u}}^*, c)}{dc}$ can drive the 
gradient-based optimization of 
$\xi({\boldsymbol{u}}, c)$, where ${\boldsymbol{u}}$ is the gray-box space-time solution.\\

%The key to inferring the flux is to leverage the gray-box space-time solution.
%Its inferrability can be 
%loosely explained by the following reasonings. Firstly,
%the conserved quantity $u$ in \eqref{eqn: govern PDE} depends on
%$u$ in a previous time only inside a domain of dependence, illustrated in Figure \ref{fig: locality}.
%Similarly, in discretized PDE simulation, the solution at any gridpoint only depends on a
%numerical domain of dependence.
%Besides, in a PDE simulation, a one-step time marching at any gridpoint can be viewed as
%a mapping whose input only involves the numerical domain of dependence.
%Because the number of state variables in the numerical domain of dependence can be small, 
%inference of the mapping is potentially feasible.
%Secondly, the space-time solution at every space-time gridpoint can be viewed
%as a sample for this mapping. 
%Because the scale of space-time discretization in a conservation law simulation is usually large,
%a large number of samples are available for the inference, thus
%making the inference potentially accurate.\\
%
%\begin{figure}[Htbp]\begin{center}
%    \includegraphics[height=3.4cm]{../locality.png}
%    \caption{Domain of dependence: 
%             $u(t,x)$
%             depends on $u$ at an earlier time within a
%             domain of dependence. The two planes in this figure indicates the spatial 
%             solution at two adjacent timesteps.
%             The domain of dependence can be much smaller
%             than $\Omega$, the entire spatial domain.}
%    \label{fig: locality}
%\end{center}\end{figure}


The key to inferring $F$ is to leverage the gray-box space-time solution.
Compared to the surrogate modeling of the output $\xi$ \cite{surrogate review}, the advantage of
the twin-model approach lies in the usage of the big data, the space-time solution, generated from
gray-box PDE solvers. The usage of the big data may lead to more accurate modeling of $F$ and more
accurate predictions of $\xi$ with fewer runs of the gray-box simulation.\\

For example, the following theorem illustrates what aspect of $F$ can be inferred from the gray-box
solution for a special form of \eqref{eqn: 1D conceptual},
\begin{equation*}\begin{split}
    \frac{\partial u}{\partial t}+ \nabla \cdot \big( F(u) \big) = q(u,c)\,,
\end{split}
\end{equation*}
where the spatial dimension is 1.
%However, the inferrability can be partially justified by the following theorem
%if \eqref{eqn: govern PDE} has only one equation, has one dimensional space, $q=0$, and $D=1$.
\begin{theorem}
    Consider two PDEs
    \begin{equation}
        \qquad\frac{\partial u}{\partial t} + \frac{\partial F(u)}{\partial x} = 0\,,\; \emph{and}
        \label{eqn: easy 1}
    \end{equation}
    \begin{equation}
        \frac{\partial \tilde{u}}{\partial t} + \frac{\partial \tilde{F}(\tilde{u})}{\partial x} = 0\,,
        \label{eqn: easy 2}
    \end{equation}
    with the same initial condition $u(0,x) = u_0(x)$. The spatial domain is $(-\infty, \infty)$. 
    The function $u_0$ is bounded, differentiable, 
    Lipschitz continuous with constant $L_u$, 
    and has a finite support. $F$ and $\tilde{F}$ are both twice-differentiable and Lipschtiz 
    continuous with constant $L_F$.
    Let 
    $$
    B_u \equiv \left\{ u\left| u=u_0(x) \; \emph{for}
    \; x \in \mathbb{R} \; \emph{that satisfies}\; \left|\frac{du_0}{dx}\right|\ge \gamma > 0
    \,,\right.
    \right\}
    \subseteq \mathbb{R}\,.
    $$
    be a non-empty and measurable set.
    We have:\\
    For any $\epsilon >0$, there exist $\delta>0$ and $T>0$ such that 
    \begin{itemize}
        \item if $|\tilde{u}(t,x)-u(t,x)| < \delta$ for all $x\in \mathbb{R}$ and $t\in [0,T]$, then
              $\left|\frac{d\tilde{F}}{du} - \frac{dF}{du}\right| < 
               \epsilon $ for all $u\in B_u\,.$
    \end{itemize}
    \label{theorem: 1}
\end{theorem}
The proof is given in Appendix \ref{proof 1}. $B_u$ consists of the value of $u$ that 
appears in the initial condition $u_0(x)$. In addition, on such value of $u$, the 
initial condition must satisfy $\left|\frac{du_0}{dx}\right|\ge \gamma > 0$.
An example of $B_u$ is given in Figure \ref{fig: excitedDomain}.
The initial condition $u_0$ and its derivative $\frac{du_0}{dx}$ are indicated by the
solid blue and the dashed green lines. Given the value of $\gamma$, 
$B_u$ is shown on the right vertical axis which consists of values of $u$ that appear in $u_0$
and satisfy $\left|\frac{du_0}{dx}\right|\ge \gamma > 0$.\\

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=10cm]{../excitedDomain.png}
        \caption{An illustration of $B_u$ defined in Theorem \ref{theorem: 1}. The blue line is $u_0$ 
        and the green dashed line is $\frac{du_0}{dx}$. $B_u$ is the set of $u_0$ where the derivative 
        $\frac{du_0}{dx}$ has an absolute value larger than $\gamma$.
        The left y-axis is $\frac{du_0}{dx}$, and the right y-axis is $u_0$.
        $B_u$, represented by the bold blue line on the right y-axis,
        is domain of $u$ in which the error of the inferred flux can be bounded by the solution
        mismatch.}
        \label{fig: excitedDomain}
    \end{center}
\end{figure}

Several observations can be made from Theorem \ref{theorem: 1}.
Firstly, if the solutions of \eqref{eqn: easy 1} and \eqref{eqn: easy 2} match closely, i.e.
$|\tilde{u}(t,x)-u(t,x)| < \delta$, then
the derivatives of their flux functions must match closely in $B_u$, i.e.
$\left|\frac{d\tilde{F}}{du} - \frac{dF}{du}\right| < \epsilon$. 
Secondly, only the derivatives of the fluxes are guaranteed to match, i.e.
$\left|\frac{d\tilde{F}}{du} - \frac{dF}{du}\right| < \epsilon$,
rather than the fluxes themselves. 
If $F$ or $\tilde{F}$ is added by a constant, the solution of the gray-box or the twin-model
will not change.
Thirdly, the conclusion 
can only be drawn for values of $u$ which appeared in the initial condition 
$\left(u\in \left\{u_0(x)\, \textrm{for all } \, x\in \mathbb{R}\right\}\right)$, 
and where the initial condition has large enough slope $\left(\left|\frac{du_0}{dx}\right|\ge \gamma > 0\right)$.
Generally speaking, 
we expect that $F$ is only inferrable (up to a constant) in the domain of $u$ covered by the gray-box 
solution, which will be demonstrated in the numerical examples in this chapter.\\
%For the general form \eqref{eqn: govern PDE} which 1) has systems of equations, 2) has higher spatial dimensions, 
%and 3) only discretized space-time solution is available,
%the inferrability is difficult to show theoretically. Instead, it will be demonstrated numerically.\\


If the twin model uses implicit time marching schemes,
the minimization of $\mathcal{M}_u$ can be expensive because the computation of $\mathcal{M}_u$ 
requires to solve a system of equations at every timestep \cite{PDE theory}.
To reduce the computational cost, we introduce another metric:
the integrated truncation error.
Define
\begin{equation}
    \tau = \frac{\partial u}{\partial t} 
    + \nabla \cdot \big(D \tilde{F}(u)\big) - q(u,c)\,,
    \label{eqn: residual}
\end{equation}
to be the residual of \eqref{eqn: govern twin model}
by replacing $\tilde{u}$ with $u$.
Let $\boldsymbol{\tau}$ be the discretized residual obtained by plugging the discretized 
gray-box solution into the twin-model simulator.
The integrated truncation error is defined to be
\begin{equation}
    \mathcal{M}_{\tau}(\tilde{F}) = \sum_{i=1}^M \sum_{j=1}^N w_{ij} \boldsymbol{\tau}_{ij}^2 \,,
    \label{eqn: truncation error}
\end{equation}
which approximates
\begin{equation}
    \int_0^T \int_\Omega \tau^2 \, \textrm{d}x\textrm{d}t\,.
    \label{eqn: integrated residual}
\end{equation}
In \eqref{eqn: truncation error}, 
$i, j, w_{ij}$ are defined in the same way as in \eqref{eqn: solution mismatch}.\\

We study the relationship of $\mathcal{M}_{\tau}$ and $\mathcal{M}_{u}$.
A sufficient condition is studied under which $\mathcal{M}_u$ can be bounded by 
$\mathcal{M}_{\tau}$.\\

\begin{theorem}
    Consider a twin model simulator whose one-step time marching is
    \begin{equation}
        \mathcal{G}_i:\, \mathbb{R}^N\mapsto\mathbb{R}^N,\, \tilde{\boldsymbol{u}}_{i\cdot}\rightarrow 
        \tilde{\boldsymbol{u}}_{i+1\cdot}=\mathcal{G}_i
        \tilde{\boldsymbol{u}}_{i\cdot} \,,\quad i=1,\cdots, M-1\,.
    \end{equation}
    Assume the quadrature weights are time-independent, i.e.
    $w_{ij} = w_{j}$ for all $i,j$.
    If $\mathcal{G}_i$ satisfies
    \begin{equation}
        \|\mathcal{G}_ia-\mathcal{G}_ib\|^2_{W} \le \beta \|a-b\|^2_{W} \,,
        \label{eqn: contractive}
    \end{equation}
    for any $a, b \in \mathbb{R}^N$ and for all $i$,
    then 
    \begin{equation}
        \mathcal{M}_u \le \left(1+ \beta+ \cdots + \beta^{M-1}\right) \mathcal{M}_{\tau}\,,
    \end{equation}
    where
    \begin{equation}
        \|v\|^2_{W} \equiv v^T 
            \begin{pmatrix}
                {w_{1}} && \\
                & \ddots & \\
                && {w_{N}}
            \end{pmatrix} v
    \end{equation}
    for any $v\in \mathbb{R}^N$.
    \label{theorem: 2}
\end{theorem}
The proof is given in Appendix \ref{proof 2}. The theorem implies that,
if the one-step time marching operator of the twin model is Lipschitz continuous, 
as given by \eqref{eqn: contractive}, then the
solution mismatch can be bounded by the integrated truncation error.
Unfortunately, if the Lipschitz constant $\beta>1$ and if 
the number of timesteps $M\gg 1$, then
$\left(1+ \beta+ \cdots + \beta^{M-1}\right)$ can be large. Thus
a small $\mathcal{M}_{\tau}$ does not always guarantee a small $\mathcal{M}_u$.
Therefore, for twin models that has $\beta>1$ and $M \gg 1$, 
if computational budget allows, we recommend minimizing $\mathcal{M}_u$
instead of $\mathcal{M}_u$ for training a twin model. Despite the theoretical
flaw, $\mathcal{M}_\tau$ can be useful in practice when minimizing $\mathcal{M}_u$ is
too computationally expensive.\\

%If $\mathcal{M}_u$ is replaced by $\mathcal{M}_{\tau}$ in \eqref{eqn: twin min mismatch},
%a different twin model can be obtained. The minimization of $\mathcal{M}_{\tau}$ can be
%cheaper than the minimization of $\mathcal{M}_u$ because the computation of the residual 
%$\boldsymbol{\tau}$ does not require the iterative solves.
%However, 
%Indeed, if the twin models is unstable, $\mathcal{M}_u$ may not be bounded even if 
%$\mathcal{M}_{\tau}$ is bounded \cite{PDE theory}.\\


%In practice, we may have $\beta>1$. For $M\gg 1$, small Mtau big Mu. 
%If solution big difference, then adjoint can be different. We recommend using Mu if 
%computation resource allows.
%The usage of $\mathcal{M}_{\tau}$ will be discussed in section 
%\ref{sec: adaptive basis}. 
Let $\mathcal{M}$ denote either $\mathcal{M}_u$
or $\mathcal{M}_{\tau}$.
The minimization of $\mathcal{M}$ can be solved by gradient-based methods.
For $\mathcal{M}=\mathcal{M}_{\tau}$, the adjoint method can be applied to compute 
$\frac{d\mathcal{M}_{\tau}}{d\tilde{F}}$ to drive the optimization.
For $\mathcal{M} = \mathcal{M}_u$, the adjoint method can be 
applied to compute the gradient of $\tilde{\boldsymbol{u}}$ with respect
to $\tilde{F}$. Therefore, the gradient of $\mathcal{M}$ with respect to $\tilde{F}$ can be 
obtained through 
\eqref{eqn: solution mismatch} according to
\begin{equation}
    \frac{d\mathcal{M}}{d\tilde{F}} = \frac{d\mathcal{M}}{d\tilde{\boldsymbol{u}}} \frac{d\tilde{\boldsymbol{u}}}{d\tilde{F}}\,.
    \label{eqn: estimated gradient temp}
\end{equation}\\


The remainder of this chapter is organized as follows.
Section \ref{sec: flux param} discusses the choices of the function space $\mathcal{S}_F$
in \eqref{eqn: twin min mismatch}.
%A suitable parameterization for $\tilde{F}$ will be chosen that takes into account the observations from
%Theorem \ref{theorem: 1}.
A choice of the basis functions, the sigmoid functions, 
is introduced to parameterize $\tilde{F}$. By using a fixed set of basis functions, the 
twin model is demonstrated in a numerical example.
Section \ref{sec: adaptive basis} develops an algorithm that adaptively constructs the
basis functions.
Section \ref{sec: twin numerical results} demonstrates the algorithm in several numerical examples.
Finally, section \ref{sec: chap 2 summary} summarizes the chapter.\\


\section{Choice of Basis Functions}
\label{sec: flux param}
As discussed in Section \ref{sec: adaptive basis review}, 
$F$ can be parameterized by a linear combination of basis functions.
Firstly, consider the case when $\tilde{F}$ is univariate. 
There are many types of basis
functions to parameterize a univariate function, such as polynomial basis, Fourier basis, and
wavelet basis \cite{wavelet mallat}. 
Based on the observations from Theorem \ref{theorem: 1}, $\tilde{F}$ and $F$
are expected to match only on a domain of $u$ 
where the gray-box space-time solution appears and has large enough slope.
%Besides, $\tilde{F}$ may match 
%$F$ better on a domain where the gray-box discretized solution $\boldsymbol{u}$ is more densely sampled.
Therefore, an ideal parameterization should 
admit local refinements so $\tilde{F}$ can match $F$ better at some domain.
%similarly, it should allow local dropouts when some bases are unnecessary at some domain. 
Another observation from Theorem \ref{theorem: 1} is that $F$ can only be estimated up to 
a constant.
This section presents a choice of the parameterization for $\tilde{F}$ that takes into account
such considerations.\\

A parameterization that allows local refinements is the wavelet parameterization
\cite{wavelet mallat}.
The wavelet is a set of basis functions
developed for multi-resolution analysis (MRA) \cite{wavelet mallat}.
MRA introduces an increasing sequence of closed function spaces $\{V_j\}_{j\in \mathbb{Z}}$,
$$\cdots \subset V_{-1} \subset V_0 \subset V_1 \subset \cdots \,,$$
%which can approximate functions with increasing resolutions as $j$ increases 
\cite{wavelet mallat}.
For univariate MRA, $V_j$'s satisfy the following properties known as
self-similarity \cite{wavelet mallat}:
\begin{equation*}\begin{split}
    &f(u) \in V_j \Leftrightarrow f(2u) \in V_{j+1}, \; j\in \mathbb{Z}\\
    &f(u) \in V_j \Leftrightarrow f(u-\frac{\eta}{2^j}) \in V_{j},
                    \; j\in \mathbb{Z},\, \eta\in \left\{0, \pm 1, \pm 2,\cdots\right\}\,.
\end{split}\end{equation*}
The function space $V_j$ is spanned by a set of orthonormal bases called the wavelet
\cite{wavelet mallat}
\begin{equation}
    \hat{\phi}_{j,\eta}(u) = 2^{j/2} \hat{\phi}(2^j u-\eta) \,,\quad 
    \eta\in \left\{0, \pm 1, \pm 2,\cdots\right\}\,,
    \label{eqn: self similar wavelet}
\end{equation}
where $\hat{\phi}$ is called the mother wavelet.
%satisfying $\hat{\phi}(u) \rightarrow 0$ for $ u \rightarrow -\infty $ and $\infty$.
The equation \eqref{eqn: self similar wavelet} is called the self-similar property, 
because any basis $\hat{\phi}_{j,\eta}$ can be obtained through a translation and a dilation 
of the mother wavelet $\hat{\phi}$, where $j$ is called the dilation parameter and $\eta$
is called the translation parameter.
An example mother wavelet, the Meyer wavelet, is shown in Figure \ref{fig: meyer}.\\

\begin{figure}[Htbp]\begin{center}
    \includegraphics[width=5cm, height=4cm]{../meyer_2.png}
    \caption{An example mother wavelet, the Meyer wavelet.}
    \label{fig: meyer}
\end{center}\end{figure}

As discussed in the beginning of this chapter, only the derivative of $F$, rather than 
$F$ itself, can be inferred.
If $\frac{d \tilde{F}}{du}$ is parameterized by the wavelet bases, 
$\tilde{F}$ shall be parameterized by the indefinite integrals of the wavelets, i.e.
\begin{equation}
    \phi_{j,\eta}(u) = \int_{-\infty}^u \hat{\phi}_{j,\eta}(u^\prime) du^\prime\,.
    \label{eqn: integral wavelet}
\end{equation}
$\phi_{j,\eta}$'s are sigmoid functions which satisfy
\begin{equation}
    \frac{d\phi_{j,\eta}}{du} = \hat{\phi}\,,
\end{equation}
and
\begin{equation}
    \phi_{j,\eta}(u) = \left\{
        \begin{split}
            0,&\; u\rightarrow -\infty\\
            1,&\; u\rightarrow \infty
        \end{split}\right.
\end{equation}
due to the normality of the wavelet.\\

Let 
\begin{equation}
    \phi(u) = \int_{-\infty}^u \hat{\phi}(u^\prime) du^\prime\,,
    \label{eqn: integral wavelet basis}
\end{equation}
then
\begin{equation}
    \phi(2^j u-\eta) = \int_{-\infty}^{2^j u-\eta} \hat{\phi}(u^\prime) du^\prime
    = \int_{-\infty}^{u} \hat{\phi} (2^j u^\prime - \eta) du^\prime=
    \int_{-\infty}^u \hat{\phi}_{j,\eta} (u^\prime) du^\prime
    \label{eqn: transform integral wavelet}
\end{equation}
\eqref{eqn: integral wavelet} and \eqref{eqn: transform integral wavelet} show that
$\phi_{j,\eta}$ satisfies the self-similarity property
\begin{equation}
    {\phi}_{j,\eta}(u) = {\phi}(2^j u-\eta) \,,\quad j\in \mathbb{Z} \,,\;\eta \in \mathbb{Z}\,,
    \label{eqn: self similar sigmoid}
\end{equation}
where $\phi$ is called the {mother sigmoid}.\\

\begin{figure}[Htbp]\begin{center}
    \includegraphics[width=5cm, height=4cm]{../basis_combined.png}
    \caption{Red line: the integral \eqref{eqn: integral wavelet} of the Meyer wavelet.
             Black line: the logistic sigmoid function.}
    \label{fig: sigmoid}
\end{center}\end{figure}


There are many choices of sigmoid functions for $\phi$.
My thesis will use the logistic sigmoid function as the mother sigmoid,
\begin{equation}
    \phi(u) = \frac{1}{1+ e^{-u}}\,.
    \label{eqn: logistic sigmoid}
\end{equation}
If $\tilde{F}$ is univariate, the logistic sigmoids $\phi_{j,\eta}$'s, for
$j\in \mathbb{Z}$ and $\eta \in \mathbb{Z}$, are used as the bases.
If $\tilde{F}$ is $k$-variate, the basis can be formed by the tensor product 
of the univariate basis \cite{functional analysis} 
($\phi_{j_1,\eta_1}, \cdots, \phi_{j_k, \eta_k}$, for
$j_1\in \mathbb{Z}, \eta_1\in \mathbb{Z} , \cdots, j_k \in \mathbb{Z}, \eta_k \in \mathbb{Z}$).
In other words, the basis can be
\begin{equation}
    \phi_{\boldsymbol{j}, \boldsymbol{\eta}} (u_1, \cdots, u_k) = \phi_{j_1, \eta_1}(u_1)\cdots
    \phi_{j_k, \eta_k}(u_k)\,,
    \label{eqn: tensor basis}
\end{equation}
where $\boldsymbol{j}=(j_1, \cdots, j_k)\in \mathbb{Z}^k$, 
$\boldsymbol{\eta} = (\eta_1, \cdots, \eta_k) \in \mathbb{Z}^k$. To sum up,
$\tilde{F}$ can be expressed by
\begin{equation}
    \tilde{F} = \sum_{\boldsymbol{j}\in \mathbb{Z}^k, \boldsymbol{\eta}\in \mathbb{Z}^k} 
                       \alpha_{\boldsymbol{j}, \boldsymbol{\eta}}
                       \phi_{\boldsymbol{j}, \boldsymbol{\eta}}\,,
    \label{eqn: linear expansion}
\end{equation}
where $\alpha$'s are the coefficients of the bases.\\

A compact representation of the sigmoid bases is introduced.
A univariate basis function,
\begin{equation*}
    {\phi}_{j,\eta}(u) = {\phi}(2^j u-\eta) \,,\quad j\in \mathbb{Z} \,,\;\eta \in \mathbb{Z}\,,
    \label{eqn: self similar sigmoid}
\end{equation*}
can be represented by a tuple $(j, \eta)$,
where $j$ is the dilation parameter, and $\frac{\eta}{2^j}$ is
the center of the basis.
Similarly, a $k$-variate basis function, $\phi_{\boldsymbol{j}, \boldsymbol{\eta}}$ in
\eqref{eqn: tensor basis}, can be represented by a tuple $\left(\boldsymbol{j}, \,
\boldsymbol{\eta}\right) 
= \left((j_1,\cdots, j_k), \, \left(\eta_1, \cdots, \eta_k\right)\right)$.
Thus, a sigmoid function can be visualized by a point in a $2k$-dimensional space,
which is illustrated in Figure \ref{fig: basis 0} thru. \ref{fig: basis 3} 
for the univariate case.\\
\begin{figure}[htbp]\begin{center}
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=7.5cm]{../basis_0.png}
        \caption{$\left(0,0\right)$}
        \label{fig: basis 0}
    \end{subfigure}
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=7.5cm]{../basis_1.png}
        \caption{$\left(0, -1\right)$}
        \label{fig: basis 1}
    \end{subfigure}
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=7.5cm]{../basis_3.png}
        \caption{$\left(1, 0\right)$}
        \label{fig: basis 2}
    \end{subfigure}
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \includegraphics[width=7.5cm]{../basis_6.png}
        \caption{$\left(1, 1\right)$}
        \label{fig: basis 3}
    \end{subfigure}
    \caption{An illustration of the tuple representation of univariate sigmoid functions.}
\end{center}\end{figure}

There are infinite number of bases involved
in this expression, making it infeasible to be implemented in the computer. To address this issue,
a systematic procedure for choosing
a suitable subset of the bases %$\{\boldsymbol{j}, \boldsymbol{\eta}\}$
will be presented in Section \ref{sec: adaptive basis}.\\


In the remaining part of the section, a numerical example is given to illustrate the inference
of $F$ by using the sigmoid parameterization.
Consider a gray-box model solving the 1-D Buckley-Leverett equation
\cite{Buckley Leverett}
\begin{equation}
    \frac{\partial u}{\partial t} + \frac{\partial}{\partial x}\Big(\underbrace{
    \frac{u^2}{1+ 2(1-u)^2}}_{F} \Big) = c\,,
    \label{eqn: Buckley-Leverett}
\end{equation}
with the initial condition $u(0,x)=u_0(x)$ and the periodic boundary condition $u(t,0)=u(t,1)$. 
$c$ is a constant control variable.
The Buckley-Leverett equation models the two-phase porous media flow where $u$ stands for the
saturation of one phase, and $1-u$ stands for the saturation of another phase. 
Therefore $0 \le u_0(x) \le 1$ for all $x\in [0,1]$. 
$c\in \mathbb{R}$ is a constant-valued control. $F$ is assumed unknown and is inferred by a twin model.
The twin model solves 
\begin{equation}
    \frac{\partial \tilde{u}}{\partial t} + \frac{\partial}{\partial x}\tilde{F}(\tilde{u})
    = c\,,
    \label{eqn: Buckley-Leverett twin}
\end{equation}
with the same $c$ and the same initial and boundary conditions. Parameterize $\tilde{F}$ by
the sigmoid bases \eqref{eqn: self similar sigmoid}
\begin{equation}
    \tilde{F} = \sum_{(j, \eta)\in \mathcal{A}\subset \mathbb{Z}\times \mathbb{Z}} 
                       \alpha_{{j}, {\eta}}
                       \phi_{{j}, {\eta}}\,,
    \label{eqn: linear expansion finite}
\end{equation}
where $\mathcal{A}$ is a finite set that contains the tuples representing
the basis functions. \eqref{eqn: linear expansion finite} differs from 
\eqref{eqn: linear expansion} in that a finite number of basis functions are used so
the parameterization can be implemented in the computer. In this example, the set $\mathcal{A}$
is chosen manually, such that the Buckley-Leverett flux can be well approximated.
The chosen basis are $(j, \eta)$ for $j=3$, $\eta=0,1,\cdots, 8$, which are shown in 
Figure \ref{fig: sigmoid basis ad hoc}.
The topic of
how to algorithmically choose a suitable set of basis will be discussed 
in Section \ref{sec: adaptive basis}.\\
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=5cm]{../fixed_basis_eg.png}
        \caption{The bases chosen manually for the numerical example of Buckley-Leverett equation.}
        \label{fig: sigmoid basis ad hoc}
    \end{center}
\end{figure}


%The basis shall satisfy the following requirement:
%On values of $u$ that appeared in the gray-box solution, 
%there should be at least one basis that has non-zero gradient 
%(sigmoid's gradient 
%is always non-zero, I don't know how to express 'large enough gradient' accurately. If
%the gradient is non-zero but very small in such values of $u$, the inference will be ill-posed.
%I have difficulty explaining this rigorously).
%Otherwise, the $\frac{dF}{du}$ in such values of $u$ can not be represented.
The twin model is trained to minimize $\mathcal{M}_u$.
To avoid overfitting in \eqref{eqn: twin min mismatch}, we consider applying an
$L_1$ regularization on $\boldsymbol{\alpha}$. In other words,
$F$ is inferred by solving the following minimization problem,
\begin{equation}
    \boldsymbol{\alpha}^* = \argmin_{\alpha_{j, \eta}\in \mathbb{R}} \left(
    \sum_{i=1}^M \sum_{j=1}^N w_{ij} \big( \tilde{\boldsymbol{u}}_{ij}
     -\boldsymbol{u}_{ij}\big)^2
 + \lambda\left\|\boldsymbol{\alpha}\right\|_{L_1} \right)
    \,,
    \label{eqn: solution mismatch L1 norm}
\end{equation}
where $\boldsymbol{\alpha} = \left\{\alpha_{j,k}\right\}_{\left(j,k\right)\in \mathcal{A}}$, 
$\left\|\cdot\right\|_{L_1}$ is the $L_1$ norm, and
$\tilde{\boldsymbol{u}}$ is the twin-model space-time solution that depends on the value of
$\boldsymbol{\alpha}$.
$\lambda>0$ is a tunable parameter for
the $L_1$ regularization. As the value of $\lambda$ increases, more entries in $\boldsymbol{\alpha}$
will be suppressed to zero
\cite{Lasso variable selection}. \\


The value of $\lambda$ should be determined by maximizing the 
out-of-sample fit, such as the $k$-fold cross validation
\cite{cross validation}. 
Given a basis dictionary, the $k$-fold cross validation proceeds in the following three steps:
In the first step, the gray-box solution $\boldsymbol{u}$ is shuffled randomly into $k$ disjoint sets
$\left\{\boldsymbol{u}_1 , \boldsymbol{u}_2, \cdots, \boldsymbol{u}_k\right\}$.
An illustration for $k=3$ is shown in Figure \ref{fig: shuffle}.
\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=3.2cm]{../shuffle_1.png}
        \caption{The discretized 
                 gray-box solution is shuffled into $3$ sets, each indicated by a color. 
                 Each block stands for the state variable on a space-time grid point.}
        \label{fig: shuffle}
    \end{center}
\end{figure}

In the second step, $k$ twin models are trained so that their space-time solutions match
all but one sets of the gray-box solution, as shown in \eqref{eqn: cross validation}, where
$T_i$ indicates the $i$th twin model.
\begin{equation}\begin{split}
T_1 &= \texttt{TrainTwinModel}(\boldsymbol{u}_2, \boldsymbol{u}_3, \cdots, \boldsymbol{u}_k)\\
T_2 &= \texttt{TrainTwinModel}(\boldsymbol{u}_1, \boldsymbol{u}_3, \cdots, \boldsymbol{u}_k)\\
&\cdots\\
T_k &= \texttt{TrainTwinModel}(\boldsymbol{u}_1, \boldsymbol{u}_2, \cdots, \boldsymbol{u}_{k-1})
\label{eqn: cross validation}
\end{split}\,,\end{equation}
where each equation requires solving \eqref{eqn: solution mismatch L1 norm}.\\

In the third step, each trained twin model is validated, by computing
the solution mismatch on the remaining set of the gray-box solution, as shown in 
\eqref{eqn: cross validation mismatch}.
\begin{equation}\begin{split}
    \mathcal{M}_u^1 &= \texttt{SolutionMismatch}\left( T_1 , \boldsymbol{u}_1\right)\\
    \mathcal{M}_u^2 &= \texttt{SolutionMismatch}\left( T_2 , \boldsymbol{u}_2\right)\\
    &\cdots\\
    \mathcal{M}_u^k &= \texttt{SolutionMismatch}\left( T_k , \boldsymbol{u}_k\right)
    \label{eqn: cross validation mismatch}
\end{split}\end{equation}
$\lambda$ should be chosen to minimize
the mean value of validation errors
\begin{equation}
    \overline{\mathcal{M}}_u = \frac{1}{k}\left(\mathcal{M}_u^1 + \mathcal{M}_u^2 + \cdots 
    + \mathcal{M}_u^k\right)\,.
    \label{eqn: cross validation error mean}
\end{equation}\\


\eqref{eqn: solution mismatch L1 norm} is solved by the L-BFGS method \cite{LBFGS}, using the
NLopt package \cite{nlopt}.
An example of training the twin model is shown in Figure \ref{fig: insample test}.
Figure \ref{fig: insample test} (a) shows the gray-box solution used to train the twin model. 
Figure \ref{fig: insample test} (b) shows the trained twin model solution by using the same initial condition.
Figure \ref{fig: flux test} shows the gray-box flux $F$ and the trained flux $\tilde{F}$, as well as
$\frac{dF}{du}$ and $\frac{d\tilde{F}}{du}$.\\

In addition, the trained twin model is simulated using out-of-sample initial conditions which
are different from the initial condition of the training solution. 
Two example gray-box
and twin-model solutions are shown in Figure \ref{fig: outsample good test} and Figure \ref{fig: outsample bad test}.
In Figure \ref{fig: outsample good test}, the domain of the gray-box solution is $[0.1, 0.3]$, which
is contained in the the domain of the training solution, $[0,0.48]$.
Therefore, we expect that the gray-box and the twin-model
solutions match closely. In contrast, the domain of the gray-box solution in Figure 
\ref{fig: outsample bad test} is $[0.05,0.9]$. The domain is not contained 
in the domain of the training solution, and a larger solution mismatch is observed.\\

%Figure \ref{fig: leftcol} shows the gray-box space-time solution
%on $x\in[0,1], \, t\in[0,1]$ for $c=0$. 
%The solution is used to train a twin model according to \eqref{eqn: solution mismatch L1 norm}.
%The twin-model space-time solution is shown in 
%Figure \ref{fig: rightcol}. The twin-model solution matches the gray-box solution, which indicates that
%the twin model has been trained successfully.\\

\begin{figure}[htbp]\begin{center}
    \includegraphics[width=10cm]{/home/voila/Documents/2014GRAD/mirror/thesis_add/insample_sol.png}
    \caption{(a) shows the gray-box solution used to train the twin model. (b) shows the trained twin-model 
             solution by using the same initial condition as in the gray-box solution.}
    \label{fig: insample test}
\end{center}\end{figure}

\begin{figure}[htbp]\begin{center}
    \includegraphics[width=10cm]{/home/voila/Documents/2014GRAD/mirror/thesis_add/insample_flux.png}
    \caption{(a) shows the gray-box model's flux $F$ (red) and the trained twin-model flux $\tilde{F}$ (blue).
             (b) shows $\frac{dF}{du}$ (red) and $\frac{d \tilde{F}}{du}$ (blue)}.
    \label{fig: flux test}
\end{center}\end{figure}

\begin{figure}[htbp]\begin{center}
    \includegraphics[width=10cm]{/home/voila/Documents/2014GRAD/mirror/thesis_add/outsample_good_fit.png}
    \caption{(a) shows an gray-box solution. (b) shows the out-of-sample solution of the trained twin model by using the
             same initial condition as in (a).}
    \label{fig: outsample good test}
\end{center}\end{figure}

\begin{figure}[htbp]\begin{center}
    \includegraphics[width=10cm]{/home/voila/Documents/2014GRAD/mirror/thesis_add/outsample_bad_gray.png}
    \caption{(a) shows an gray-box solution. (b) shows the out-of-sample solution of the trained twin model by using the
             same initial condition as in (a).}
    \label{fig: outsample bad test}
\end{center}\end{figure}

%\begin{figure}[htbp]\begin{center}
%    \begin{subfigure}[t]{.4\textwidth}
%        \centering
%        \includegraphics[width=5cm]{../leftcol.png}
%        \caption{Gray-box model.}
%        \label{fig: leftcol}
%    \end{subfigure}
%    \begin{subfigure}[t]{.4\textwidth}
%        \centering
%        \includegraphics[width=5cm]{../rightcol.png}
%        \caption{Trained twin model.}
%        \label{fig: rightcol}
%    \end{subfigure}
%    \caption{Space-time solutions.}
%\end{center}\end{figure}

After training the twin model, the adjoint method can be applied to the twin model
to obtain the gradient of an objective function $\xi$ to 
$c$. The gradient 
$\frac{d\xi(\tilde{\boldsymbol{u}}, c)}{dc}$ approximates $\frac{d\xi(\boldsymbol{u}, c)}{dc}$
for the value of $c$ on which the twin model is trained.
Consider the objective function 
\begin{equation}
    \xi(c) \equiv \int_{x=0}^1 \left(u(1,x; c) - \frac{1}{2}\right)^2 \,\textrm{d}x\,.
    \label{eqn: objective ad hoc}
\end{equation}
Figure \ref{fig: objective ad hoc} shows the objective function, evaluated using 
the gray-box model and the trained twin model, where the twin model is trained
at $c=0$ with the solution shown in Figure \ref{fig: insample test} (a). 
It is observed that the gradients of $\xi$
match closely at $c=0$ where the twin model is trained.\\

\begin{figure}[htbp]\begin{center}
    \includegraphics[width=6.5cm]{../J_twin_vs_primal.png}
    \caption{The objective function $\xi$ evaluated by either the gray-box model
             and the trained twin model.}
    \label{fig: objective ad hoc}
\end{center}\end{figure}

Because $\tilde{F}$ is trained by
the gray-box space-time solution, and because the gray-box space-time solution depends on
the initial condition $u_0(x)$, it is expected
that the trained $\tilde{F}$ depends on $u_0(x)$. 
Figure \ref{fig: combine 3} shows the training results using the gray-box solutions of
three different initial conditions at $c=0$.
Some observations can be made: 1) As expected, the inferred 
$\tilde{F}$ can differ from $F$ by a constant, which
can be observed by in Figure \ref{fig: combine 3} (d), (e), and (f); 
2) $\frac{d\tilde{F}}{du}$ matches $\frac{dF}{du}$ only in a
domain of $u$ where the solution appears, as indicated by the green areas in 
Figure \ref{fig: combine 3} (g), (h), and (i);
3) $\frac{d\tilde{F}}{du}$ does not necessarily match $\frac{dF}{du}$ outside the green area, 
%indicating that
%some bases are redundant thus can be safely dropped out from the parameterization of $\tilde{F}$
%in \eqref{eqn: linear expansion finite}.
The issue can be seen clearly in Figure \ref{fig: combine 3} (c), (f), and (i);
4) In some regions of $u$, the bases are too coarse. The issue appears
in Figure \ref{fig: combine 3} (g), where $\frac{d\tilde{F}}{du}$ exhibits
a wavy deviation from $\frac{dF}{du}$. At such regions of $u$, the basis dictionary may be enriched 
by additional bases to enable a more accurate approximation of $F$.
Addressing these issues in a systematic way is crucial to the rigorous development of
the twin model method. This topic is discussed in the next section.\\

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=16cm]{../combine_3_inits_new.png}
    \caption{(a,b,c) shows the three different initial conditions used to generate
             the gray-box space-time solution. (d,e,f) compares the trained $\tilde{F}$ 
             (blue) and the Buckley-Leverett $F$ (red). (g,h,i) compares
             the trained $\frac{d\tilde{F}}{du}$ (blue) and the Buckley-Leverett 
             $\frac{dF}{du}$ (red). The green background highlights the domain of $u$ where
             the gray-box space-time solution appears.}
    \label{fig: combine 3}
\end{center}
\end{figure}


\section{Adaptive Basis Construction}
\label{sec: adaptive basis}
This section addresses the problem of adaptively choosing a finite set of basis functions
for the parameterization of $\tilde{F}$.
Assume all possible choices of basis functions forms a countable set.
Algorithm \ref{algo: outline heuristics} has outlined an iterative approach 
to construct the basis dictionary.
%\cite{adaptive basis 1, adaptive basis 2, adaptive basis 3}. 
Starting from an initial set of basis functions,
the basis dictionary is built up progressively 
by iterating over a forward step and a backward step
\cite{adaptive basis 1, adaptive basis 2, adaptive basis 3}.
The forward step searches over a candidate set of bases, and appends the most useful bases 
to the dictionary
\cite{adaptive basis 1, adaptive basis 2, adaptive basis 3}.
The backward step searches over the current dictionary, and removes the 
unnecessary bases from the dictionary
\cite{adaptive basis 1, adaptive basis 2, adaptive basis 3}.
The iteration stops only when no alternation is made to the dictionary or when a criterion, such
as a targeted approximation accuracy,
is achieved \cite{adaptive basis 1, adaptive basis 2, adaptive basis 3}. My thesis applies 
this approach to the adaptive construction of the bases for the parameterization of $\tilde{F}$,
which is sketched in Algorithm \ref{algo: outline heuristics}. Here
$\mathcal{A}$ stands for a finite set of the tuple representation of the bases in the
dictionary.\\

\begin{algorithm}
    \begin{algorithmic}[1]
        \REQUIRE{Gray-box solution $\boldsymbol{u}$; Initial basis dictionary 
                 $\mathcal{A}$.}
        \LOOP
            \STATE Minimize solution mismatch:
                   $\boldsymbol{\alpha} \leftarrow \argmin_{\boldsymbol{\alpha}} \mathcal{M}
                    \left( \tilde{F}(\mathcal{A}, \boldsymbol{\alpha}), \boldsymbol{u}\right)$
            \IF {No more basis is needed}
                \STATE \textbf{Break}
            \ELSE
                \STATE Enrich $\mathcal{A}$ by an additional basis.
            \ENDIF
            \IF {No basis shall be deleted}
                \STATE \textbf{Continue}
            \ELSE 
                \STATE Delete a basis from $\mathcal{A}$.
            \ENDIF
        \ENDLOOP
        \ENSURE $\mathcal{A}$, $\boldsymbol{\alpha}$.
    \end{algorithmic}
    \caption{The outline of the algorithm for training a twin model with an adaptive basis.
             $\mathcal{A}$ indicates the basis dictionary.  
             $\boldsymbol{\alpha}$ indicates the bases' coefficients.
             Starting from an initial dictionary, the algorithm iterates over the forward and
             the backward step to adaptively construct the dictionary and find the optimal 
             coefficients.
             As explained in the previous section, the solution mismatch is a function that depends
             on $u$ and $\tilde{F}$, where $\tilde{F}$ depends on the dictionary $\mathcal{A}$
             and its coefficients $\alpha$.} 
    \label{algo: outline heuristics}
\end{algorithm}


%\begin{figure}
%    \begin{center}
%        \includegraphics[height=6cm]{../algo1_2.png}
%        \caption{The outline of the algorithm for training a twin model with an adaptive basis.
%                 $\boldsymbol{\phi}$ is the basis dictionary, $\boldsymbol{\phi}_0$ is the 
%                 initial basis dictionary, $\boldsymbol{\alpha}$ indicates the bases' coefficients.
%                 As explained in the previous section, the solution mismatch is a function that depends
%                 on $u$ and $\tilde{F}$, where $\tilde{F}$ depends on the bases $\boldsymbol{\phi}$
%                 and its coefficients $\boldsymbol{\alpha}$.} 
%        \label{fig: outline heuristics}
%    \end{center}
%\end{figure}

Some components of the algorithm requires measuring how significant a basis is
in training a twin model. Two criteria are needed. 
The first criterion determines \textbf{which} basis shall be chosen as the candidate basis 
to add to or delete from the dictionary, based on a metric of the significance of 
the basis.
The second criterion determines \textbf{whether} the current dictionary contains too few or
too many bases, based
on whether the approximation is sufficiently accurate.
The two criteria are developed in this section. \\

%(I think you will question the Figure \ref{fig: outline heuristics}. For example, you may ask 
%what does "no more basis is needed" mean? I have to say that 
%this figure is only a sketch of the complete twin model
%algorithm presented in the next section. I want the reader to have an overall idea of the motivation of
%why I talks about things like ``the significance of a basis'' in the following. 
%I can only discuss the details in the following part of the section. 
%Do you have a better idea how to explain this figure clearly in a high level?)
%This section develops several key 
%elements that lead to the implementation of the algorithm in Figure \ref{fig: outline heuristics}.
%Firstly, a formulation is provided to efficiently assess the 
%significance of each candidate basis; Secondly, 
%the neighborhood of a sigmoid basis is defined;
%Thirdly, a metric is developed that determines when to add or remove a candidate basis.\\
%The three elements are employed to build the twin model algorithm in Section \ref{sec: twin algo}.\\

To define the first criterion, 
a formulation is developed to efficiently assess the significance of a candidate basis.
Given a basis dictionary $\mathcal{A}$, define the minimal mismatch
\begin{equation}
    \mathcal{M}^*(\mathcal{A}) = \min_{\boldsymbol{\alpha}_\mathcal{A} \in \mathbb{R}^{|\mathcal{A}|}}
    \mathcal{M}\left( \sum_{(\boldsymbol{j}, \boldsymbol{\eta})\in \mathcal{A}}
    {\alpha}_{\boldsymbol{j}, \boldsymbol{\eta}} {\phi}_{\boldsymbol{j}, 
    \boldsymbol{\eta}} \right)\,,
    \label{eqn: minimal mismatch}
\end{equation}
where $\mathcal{M}$ can be either the solution mismatch $\mathcal{M}_u$ 
or the integrated truncation error $\mathcal{M}_{\tau}$.
%to be the minimal solution mismatch \eqref{eqn: solution mismatch} if $\tilde{F}$ were parameterized
%by the bases indexed by $\mathcal{A}$.
%The basis functions are represented by $\left(\boldsymbol{j},
%\boldsymbol{\eta}\right)$'s, i.e. the dilation and translation parameters of the sigmoid bases.
Given the gray-box solution, the minimal mismatch 
is a function of $\mathcal{A}$.
%$\boldsymbol{\alpha}_{\mathcal{A}} = \{\alpha_i\}_{i\in \mathcal{A}}$ is the coefficient for 
%$\boldsymbol{\phi}_\mathcal{A}$. 
Let $\boldsymbol{\alpha}_{\mathcal{A}}^* =\{\alpha_{\boldsymbol{j}, \boldsymbol{\eta}}^*\}_{
(\boldsymbol{j}, \boldsymbol{\eta})\in \mathcal{A}}$
be the optimal coefficients that solves \eqref{eqn: minimal mismatch}, and let
$\tilde{F}^*_\mathcal{A} = \sum_{(\boldsymbol{j}, \boldsymbol{\eta})\in \mathcal{A}} 
\alpha^*_{\boldsymbol{j}, \boldsymbol{\eta}} \phi_{\boldsymbol{j}, \boldsymbol{\eta}}$.
Consider appending $\mathcal{A}$ 
by an additional basis
$l\equiv \left(\boldsymbol{j}_l, \boldsymbol{\eta}_l\right)$, 
and let 
%$\boldsymbol{\phi}_{\mathcal{A}^\prime} = \left\{
%\boldsymbol{\phi}_{\mathcal{A}}, \phi_l \right\}$,
$\mathcal{A}^\prime$ be the appended basis dictionary.
%$\left\{ \mathcal{A}, l \right\}$.
The minimal mismatch for the appended basis dictionary
$\mathcal{A}^\prime$ is
\begin{equation}
    \mathcal{M}^*(\mathcal{A}^\prime) 
    = \min_{\boldsymbol{\alpha}_{\mathcal{A}^\prime} \in \mathbb{R}^{|\mathcal{A}|+1}}
    \mathcal{M}\left( \sum_{(\boldsymbol{j}, \boldsymbol{\eta})\in \mathcal{A}^\prime}
    {\alpha}_{\boldsymbol{j}, \boldsymbol{\eta}} {\phi}_{\boldsymbol{j}, \boldsymbol{\eta}} 
    \right)\,,
    \label{eqn: append minimal mismatch}
\end{equation}
If the coefficients for the bases $\mathcal{A}^\prime\backslash (\boldsymbol{j}_l, \boldsymbol{\eta}_l)$
are set to be $\boldsymbol{\alpha}^*_{\mathcal{A}}$ while the coefficient for the basis
$(\boldsymbol{j}_l, \boldsymbol{\eta}_l)$ is set to be $0$, then 
$\mathcal{M}(\mathcal{A}^\prime) = \mathcal{M}^*(\mathcal{A})$. 
Therefore, $\mathcal{M}^*(\mathcal{A}^\prime) \le \mathcal{M}^*(\mathcal{A})$. The appension of
an additional basis never increases the minimal mismatch.\\

Consider setting the coefficients of $\mathcal{A}^\prime$ to be
$\left\{  \boldsymbol{\alpha}^*_{\mathcal{A}}, \, \epsilon  \right\}$.
For $\epsilon\rightarrow 0$, apply first-order approximation, we have
\begin{equation}\begin{split}
    & \mathcal{M} \left( 
    \sum_{(\boldsymbol{j}, \boldsymbol{\eta}) \in \mathcal{A}}
    \alpha^*_{\boldsymbol{j}, \boldsymbol{\eta}} \boldsymbol{\phi}_{\boldsymbol{j}, \boldsymbol{\eta}}
    \right)
    - \mathcal{M} \left( 
    \sum_{(\boldsymbol{j}, \boldsymbol{\eta}) \in \mathcal{A}}
    {\alpha}^*_{\boldsymbol{j}, \boldsymbol{\eta}} 
    \boldsymbol{\phi}_{\boldsymbol{j}, \boldsymbol{\eta}}
    + \epsilon \phi_l
    \right)   \\
    &\approx -\left(\int_{u\in \mathbb{R}^k} \left.\frac{d\mathcal{M}}{d \tilde{F}}
    \right|_{\tilde{F}_\mathcal{A}^*} \phi_l
    \; \textrm{d} u \right) \epsilon \,.
    \label{eqn: epsilon mismatch}
\end{split}\end{equation}
%Define the \textbf{mismatch improvement} to be
%\begin{equation}
%    \Delta \mathcal{M}^*\left(\mathcal{A}, l\right) = \mathcal{M}^*(\mathcal{A}) - 
%    \mathcal{M}^*(\mathcal{A}^\prime)\ge 0
%    \label{eqn: mismatch improvement}
%\end{equation} 
%Let $\boldsymbol{\alpha}^*_{\mathcal{A}^\prime} = \left\{
%\hat{\boldsymbol{\alpha}}^*_{\mathcal{A}}, \, \alpha_l^*
%\right\}$ be the optimal coefficients that solves \eqref{eqn: append minimal mismatch}.
%View $\Delta \mathcal{M}^*\left(\mathcal{A}, l\right)$ as a function of $\alpha_l$,
%and assume its Hessian is positive definite at 
%$\alpha_l^*$. The first-order optimality condition gives
%\begin{equation}\begin{split}
%    \Delta\mathcal{M}^*\left(\mathcal{A}, l\right) &
%    = \mathcal{M} \left( 
%    \sum_{(\boldsymbol{j}, \boldsymbol{\eta}) \in \mathcal{A}}
%    \alpha^*_{\boldsymbol{j}, \boldsymbol{\eta}} \boldsymbol{\phi}_{\boldsymbol{j}, \boldsymbol{\eta}}
%    \right)
%    - \mathcal{M} \left( 
%    \sum_{(\boldsymbol{j}, \boldsymbol{\eta}) \in \mathcal{A}}
%    \hat{\alpha}^*_{\boldsymbol{j}, \boldsymbol{\eta}} 
%    \boldsymbol{\phi}_{\boldsymbol{j}, \boldsymbol{\eta}}
%    + \alpha_l^* \phi_l
%    \right)\\
%    &\approx 
%    \mathcal{M} \left(
%    \sum_{(\boldsymbol{j}, \boldsymbol{\eta}) \in \mathcal{A}}
%    \alpha^*_{\boldsymbol{j}, \boldsymbol{\eta}} \boldsymbol{\phi}_{\boldsymbol{j}, \boldsymbol{\eta}}
%    \right) 
%    - \mathcal{M} \left( 
%    \sum_{(\boldsymbol{j}, \boldsymbol{\eta}) \in \mathcal{A}}
%    \hat{\alpha}^*_{\boldsymbol{j}, \boldsymbol{\eta}} 
%    \boldsymbol{\phi}_{\boldsymbol{j}, \boldsymbol{\eta}}
%    \right)
%    -\left(\int_{u\in \mathbb{R}^k} \left.\frac{d\mathcal{M}}{d \tilde{F}}
%    \right|_{\tilde{F}_\mathcal{A}^*} \phi_l
%    \; \textrm{d} u \right) \alpha_l^*\\
%    &\approx -\left(\int_{u\in \mathbb{R}^k} \left.\frac{d\mathcal{M}}{d \tilde{F}}
%    \right|_{\tilde{F}_\mathcal{A}^*} \phi_l
%    \; \textrm{d} u \right) \alpha_l^* \,,
%    \label{eqn: taylor expansion}
%\end{split}\end{equation}
%where $\frac{d\mathcal{M}}{d\tilde{F}}$ is the derivative of $\mathcal{M}(\tilde{F})$ 
%with respect to $\tilde{F}$, evaluated on $\tilde{F} = \tilde{F}^*_\mathcal{A}$.
%(I am not sure how to justify $\boldsymbol{\alpha}^*_{\mathcal{A}} \approx 
%\hat{\boldsymbol{\alpha}}^*_{\mathcal{A}}$. Without this approximation, \eqref{eqn: taylor expansion} 
%will be much more costly to evaluate, because \eqref{eqn: append minimal mismatch} 
%must be solved for every
%candidate $\phi_l$ in order to evaluate $\Delta \mathcal{M}^*$. Using this approximation, \eqref{eqn: append minimal mismatch} does not need to
%be solved. Do you have a better way of explaining this?)
The absolute value of the coefficient for $\epsilon$,
\begin{equation}
    s_l(\mathcal{A}) \equiv \left|\int_{u\in \mathbb{R}^k} \left.\frac{d\mathcal{M}}{d \tilde{F}}
    \right|_{\tilde{F}_\mathcal{A}^*} \phi_l \; \textrm{d} u \right|
    \label{eqn: basis significance}
\end{equation}
is the rate of change of $\mathcal{M}$ by perturbing the coefficient of $\phi_l$, which
estimates the significance of the appended basis \cite{weight selection}. 
If there are multiple candidate bases, their significance can be sorted by
\eqref{eqn: basis significance}.\\

%For a twin model that consists of a system of $k$ equations,
%$\tilde{F}$ is a function of $u\in\mathbb{R}^k$, thus 
%$\frac{d\mathcal{M}}{d\tilde{F}}$ is also a function of $u\in\mathbb{R}^k$.
%As discussed in the previous sections, $\frac{d \mathcal{M}}{d\tilde{F}}$ is non-zero only inside a
%range where there is solution. Thus \eqref{eqn: basis significance} 
%can be computed by numerical quadrature over the bounded domain.
%%For example, for a uniform Cartesian space-time grid, the quadrature weights are equal to a constant.
%\\

In practice, \eqref{eqn: basis significance} is not computable
for \textbf{all} the candidate bases $(\boldsymbol{j}, \boldsymbol{\eta})$
for $\boldsymbol{j}\in \mathbb{Z}^k$ and
$\boldsymbol{\eta}\in \mathbb{Z}^k$, because the number of bases is infinite. 
Therefore, at every iteration in Algorithm \ref{algo: outline heuristics},
\eqref{eqn: basis significance} shall only be evaluated on a finite number of bases. 
To address this issue, we define the neighborhood of a sigmoid basis. 
For univariate basis, the neighborhood of $(j,\eta)$ is defined to be a set of the sigmoid bases:
\begin{equation}
    \mathcal{N}\left[ \left(j,\eta\right) \right]
    = \left\{ 
        \left( j+1, \eta\right),\,
        \left( j, \eta\pm 1 \right)
    \right\}\,.
    \label{eqn: neighborhood 1D}
\end{equation}
The neighborhood contains three basis functions: one basis 
$\left( j+1, \eta \right)$ 
whose dilation parameter is incremented by one; and
two basis $\left( j, \eta\pm 1 \right)$
whose dilation parameter keeps the same but the translation parameter is shifted by $\pm 1$.
For illustration, the neighborhood of $\left(0,0\right)$
is shown in Figure \ref{fig: basis neighbor}.
The definition can be extended to the multivariate sigmoid. 
The neighborhood of a multivariate sigmoid is defined to be
\begin{equation}\begin{split}
    &\mathcal{N}
    \left[
         \left(
               \boldsymbol{j}, \boldsymbol{\eta}
         \right)
    \right] = 
    \mathcal{N}\left[ \big((j_1, \cdots, j_k) , \left(
    \eta_1, \cdots, \eta_k \right) \big) \right]\\
    = & \bigg\{
            \big( \left( j_1+1,\cdots, j_k\right),
                   \left( \eta_1, \cdots, \eta_k \right)
            \big) ,\, \cdots \, ,
            \big( \left( j_1,\cdots, j_k+1\right),
                   \left( \eta_1, \cdots, \eta_k \right)
            \big),\,   \\
          & 
            \big( \left( j_1,\cdots, j_k\right),
                   \left( \eta_1\pm 1, \cdots, \eta_k \right)
            \big) ,\, \cdots \, ,
            \big( \left( j_1,\cdots, j_k\right),
                   \left( \eta_1, \cdots, \eta_k\pm 1 \right)
            \big) \bigg\}\,,
    \label{eqn: neighborhood kD}
\end{split}\end{equation}
which consists of $k$ bases whose dilation parameters are shifted by $1$, 
and $2k$ bases whose translation parameters are incremented by $\pm 1$.
%It is easy to see that a basis $(\boldsymbol{j}_0, \frac{\boldsymbol{\eta}_0}{2^{\boldsymbol{j}_0}})$ 
%can be connected to any basis $(\boldsymbol{j}, \frac{\boldsymbol{\eta}}{2^{\boldsymbol{j}}})$ 
%with $\boldsymbol{j} \ge \boldsymbol{j}_0$ \footnote{ Define $\boldsymbol{j} \ge \boldsymbol{j}_0$
%to be $\boldsymbol{j}_i \ge \boldsymbol{j}^\prime$ for all $i=1, \cdots, k$.}
%through a chain of neighborhoods.
\begin{figure}[Htbp]\begin{center}
    \begin{subfigure}[p]{1.\textwidth}
        \centering
        \includegraphics[width=10cm]{../basis_neighbor.png}
        \caption{$\mathcal{N}\left[\left(0,0\right)\right]$}
        \label{fig: basis neighbor}
    \end{subfigure}
    \begin{subfigure}[p]{1.\textwidth}
        \centering
        \includegraphics[width=10cm]{../basis_neighbor_2.png}
        \caption{$\mathcal{N}\left[ \big(0, 0\big), \big(1, -1\big)
                 \right]$}
        \label{fig: union neighbor}
    \end{subfigure}
    \caption{Neighborhood for univariate bases. 
             $(a)$ shows the neighborhood (blue)
             of a single basis (red).  $(b)$ shows the neighborhood (blue) 
             of several bases (red).}
\end{center}\end{figure}
In addition, define the neighborhood of a set of sigmoid functions to be the union
of the neighborhoods of all member bases minus the set itself, 
\eqref{eqn: multiple neighbor}. The neighborhood of 
a set of sigmoid functions
is illustrated in Figure \ref{fig: union neighbor}.
\begin{equation}\begin{split}
    &\mathcal{N}\left[(\boldsymbol{j}_1, \boldsymbol{\eta}_1), \cdots, 
    ( \boldsymbol{j}_n, \boldsymbol{\eta}_n) \right]\\
    = &\left( \mathcal{N}\left[(\boldsymbol{j}_1, \boldsymbol{\eta}_1)\right]\bigcup \cdots 
      \bigcup \mathcal{N}\left[(\boldsymbol{j}_n, \boldsymbol{\eta}_n)\right]\right)
      \bigg\backslash \left\{(\boldsymbol{j}_1, \boldsymbol{\eta}_1), \cdots, 
      ( \boldsymbol{j}_n, \boldsymbol{\eta}_n) \right\}\,.
\label{eqn: multiple neighbor}
\end{split}\end{equation}\\

Using the definition of neighborhood and the significance metric, we can determine which basis
to add and delete in the Algorithm \ref{algo: outline heuristics}. 
To add a basis, the basis significance, \eqref{eqn: basis significance},
is computed for all the bases in the neighborhood of the current dictionary. At each iteration,
the basis with the largest significance will be considered for addition. Similarly, 
to delete a basis, the basis significance is computed for all the bases in the current dictionary.
At each iteration, the basis with the smallest significance will be considered for deletion.
Nonetheless, another criterion is needed to determine whether a basis should indeed
be added or removed from the basis dictionary. \\
%Although the mismatch improvement, $\Delta \mathcal{M}^*\left(\mathcal{A}, l\right)$, is always
%non-negative, it is inadvisable to cram the basis dictionary with too many bases,
%otherwise a twin model can be overfitted.
%Therefore, a criterion is required to determine if a candidate basis shall be added to or 
%removed from the basis dictionary. 

To develop the second criterion, the technique of $k$-fold cross validation can be applied.
The $k$-fold cross validation has been discussed in Section \ref{sec: flux param}, where
%This can be achieved by cross validation,
%in particular, $k$-fold cross validation \cite{cross validation}.
%Given a basis dictionary, the $k$-fold cross validation proceeds in the following three steps:
%In the first step, the gray-box solution $\boldsymbol{u}$ is shuffled randomly into $k$ disjoint sets
%$\left\{\boldsymbol{u}_1 , \boldsymbol{u}_2, \cdots, \boldsymbol{u}_k\right\}$, which
%is illustrated in Figure \ref{fig: shuffle}.
%An illustration for $k=3$ is shown in Figure \ref{fig: shuffle}.
%\begin{figure}[htbp]
%    \begin{center}
%        \includegraphics[width=3.2cm]{../shuffle_1.png}
%        \caption{The discretized 
%                 gray-box solution is shuffled into $3$ sets, each indicated by a color. 
%                 Each block stands for the state variable on a space-time grid point.}
%        \label{fig: shuffle}
%    \end{center}
%\end{figure}
$k$ twin models are trained and validated on randomly shuffled disjoint sets of the
gray-box solution.
%The $k$ twin models are trained so that their space-time solutions match
%all but one sets of the gray-box solution, as shown in \eqref{eqn: cross validation}, where
%$T_i$ indicates the $i$th twin model.
%%\begin{equation}\begin{split}
%%T_1 &= \texttt{TrainTwinModel}(\boldsymbol{u}_2, \boldsymbol{u}_3, \cdots, \boldsymbol{u}_k)\\
%%T_2 &= \texttt{TrainTwinModel}(\boldsymbol{u}_1, \boldsymbol{u}_3, \cdots, \boldsymbol{u}_k)\\
%%&\cdots\\
%%T_k &= \texttt{TrainTwinModel}(\boldsymbol{u}_1, \boldsymbol{u}_2, \cdots, \boldsymbol{u}_{k-1})
%%\label{eqn: cross validation}
%%\end{split}\end{equation}
%In the third step, each trained twin model is validated, by computing
%the solution mismatch on the remaining set of the gray-box solution, as shown in 
%\eqref{eqn: cross validation mismatch}.
%%\begin{equation}\begin{split}
%%    \mathcal{M}_1 &= \texttt{MismatchValidation}\left( T_1 , \boldsymbol{u}_1\right)\\
%%    \mathcal{M}_2 &= \texttt{MismatchValidation}\left( T_2 , \boldsymbol{u}_2\right)\\
%%    &\cdots\\
%%    \mathcal{M}_k &= \texttt{MismatchValidation}\left( T_k , \boldsymbol{u}_k\right)
%%    \label{eqn: cross validation mismatch}
%%\end{split}\end{equation}
The mean value of validation errors \eqref{eqn: cross validation error mean},
\begin{equation*}
    \overline{\mathcal{M}} = \frac{1}{k}\left(\mathcal{M}_1 + \mathcal{M}_2 + \cdots 
    + \mathcal{M}_k\right)
\end{equation*}
can be used to measure the performance of the basis dictionary. A basis shall be added to or removed from
the dictionary only if such action reduces $\overline{\mathcal{M}}$.\\

Based upon the above developments, Algorithm \ref{alg: train twin} gives the details need in
Algorithm \ref{algo: outline heuristics} to adaptively construct the basis dictionary.
The main part of the algorithm is the forward-back iterations that
determines which and whether a basis is added or deleted 
in the dictionary, by using the metric $\mathcal{M}$ and the significance $s$.
The metric $\mathcal{M}$ and the significance $s$ can be defined either according to the
solution mismatch $\mathcal{M}_u$, or according to the integrated truncation error
$\mathcal{M}_{\tau}$. We suggest using $\mathcal{M}_{\tau}$ due to less computational cost.
The algorithm starts from training a twin model using 
an arbitrary basis dictionary $\mathcal{A}$. 
%Usually the starting
%dictionary contains one basis for each dimension with very low resolution.
%The choice of the starting basis dictionary will be discussed in Section 
%\ref{sec: twin numerical results} along with numerical examples.
The main part of the algorithm iterates over a forward step (line 3-9) and a backward step (line
11-19). The forward step firstly finds the most significant candidate 
in the neighborhood of the current dictionary according to \eqref{eqn: basis significance}.
If the addition indeed reduces the cross validation error, the candidate is added 
to the dictionary; otherwise it is rejected. 
If the basis is added, the coefficients are updated by minimizing the solution mismatch, which can be
implemented by the Broyden-Fletcher-Goldfarb-Shannon (BFGS) algorithm \cite{quasiNewton}.
The backward step finds the most significant
candidate in the current dictionary for deletion. 
If the deletion reduces the cross validation error, the candidate
is removed from the dictionary. 
If the basis is indeed deleted, the coefficients are updated by BFGS again.
The iteration exits when the most significant addition no longer 
reduces the validation error. In the end, the 
coefficients are tuned to minimize the solution mismatch $\mathcal{M}_u$, which ensures that
$\mathcal{M}_u$ is minimized.
The output of the algorithm are the basis dictionary $\mathcal{A}$ and the
coefficients $\boldsymbol{\alpha}_{\mathcal{A}}$.\\



\begin{algorithm}
\begin{algorithmic}[1]
\REQUIRE{Initial basis dictionary $\mathcal{A}$, 
        Validation error $\overline{\mathcal{M}}_0 = \infty$,
        Gray-box solution $\boldsymbol{u}$ }.
\STATE Minimize solution mismatch 
       $
           \boldsymbol{\alpha}_{\mathcal{A}} \leftarrow \argmin_{\boldsymbol{\alpha}} 
           \mathcal{M}\left(\sum_{
           (\boldsymbol{j}, \boldsymbol{\eta}) \in \mathcal{A}} 
           \alpha_{\boldsymbol{j}, 
           \boldsymbol{\eta}} \phi_{\boldsymbol{j}, \boldsymbol{\eta}}\right)
       $ 
\LOOP 
\STATE $$l^* \leftarrow \argmax_{l \in \mathcal{N}(\mathcal{A})} s_l(\mathcal{A})\;,\;\;
       \mathcal{A} \leftarrow 
       \mathcal{A} \bigcup \{l^*\}$$
       %\;, \alpha_{l^*} = 0\;,
       %\boldsymbol{\alpha}_{\mathcal{A}} \leftarrow \{\boldsymbol{\alpha}_{\mathcal{A}}, 
       %\alpha_{l^*}\}$$
\STATE Compute $\overline{\mathcal{M}}$ by $k$-fold cross validation.
\IF {$\overline{\mathcal{M}} < \overline{\mathcal{M}}_0$} 
    \STATE $$
               \overline{\mathcal{M}}_0\leftarrow \overline{\mathcal{M}}\;,\;
               \boldsymbol{\alpha}_{\mathcal{A}} \leftarrow \argmin_{\alpha} 
               \mathcal{M}\left(\sum_{(\boldsymbol{j}, \boldsymbol{\eta}) 
               \in \mathcal{A}} \alpha_{\boldsymbol{j}, \boldsymbol{\eta}} 
               \phi_{\boldsymbol{j}, \boldsymbol{\eta}}\right)
           $$
\ELSE \STATE 
       $$\mathcal{A} \leftarrow 
       \mathcal{A} \backslash \{l^*\}$$
       %\;,\;\boldsymbol{\alpha}_{\mathcal{A}} \leftarrow \boldsymbol{\alpha}_{\mathcal{A}} \backslash 
       %\{\alpha_{l^*}\}$$
       \textbf{break}
\ENDIF
\STATE  
       $$g^* \leftarrow \argmin_{g \in \mathcal{A}} s_g(\mathcal{A})$$
\IF {${g^*} \neq l^*$}
\STATE  
       $$\mathcal{A} \leftarrow 
       \mathcal{A} \backslash \{g^*\}$$
       %\;,\;
       %\boldsymbol{\alpha}_{\mathcal{A}} \leftarrow \boldsymbol{\alpha}_{\mathcal{A}}\backslash \{\alpha_{g^*}\} $$
\STATE Compute $\overline{\mathcal{M}}$ by $k$-fold cross validation.
\IF{  $\overline{\mathcal{M}} < \overline{\mathcal{M}}_0$ }
\STATE $$\overline{\mathcal{M}}_0 \leftarrow \overline{\mathcal{M}}\;,\;\;
         \boldsymbol{\alpha}_{\mathcal{A}} \leftarrow 
         \argmin_{\alpha} \mathcal{M}\left(\sum_{(\boldsymbol{j}, \boldsymbol{\eta}) 
         \in \mathcal{A}} \alpha_{\boldsymbol{j}, \boldsymbol{\eta}} 
         \phi_{\boldsymbol{j}, \boldsymbol{\eta}}\right)
       $$
\ELSE \STATE
       $$\mathcal{A} \leftarrow \mathcal{A}\bigcup \{g^*\}$$
       % \;,\;
       %\boldsymbol{\alpha}_{\mathcal{A}} \leftarrow 
       %\boldsymbol{\alpha}_{\mathcal{A}}\bigcup \{\alpha_{g^*}\}$$
\ENDIF
\ENDIF
\ENDLOOP
\STATE
       $$
           \boldsymbol{\alpha}_{\mathcal{A}} \leftarrow \argmin_{\alpha} \mathcal{M}_u
           \left(\sum_{(\boldsymbol{j}, \boldsymbol{\eta}) \in \mathcal{A}} 
           \alpha_{\boldsymbol{j}, \boldsymbol{\eta}} 
           \phi_{\boldsymbol{j}, \boldsymbol{\eta}}\right)
       $$
\ENSURE $\mathcal{A}$, $\alpha_{\mathcal{A}}$.
\end{algorithmic}
\caption{Training twin model with adaptive basis construction.}
\label{alg: train twin}
\end{algorithm}



%The algorithm requires to train multiple twin models at each iteration. For
%$k=2$, $6$ twin models will be trained if both the forward and the backward steps are acceptive.
%In practice, the trained coefficients at the last iteration may provide good initial guess
%for the next iteration. However, the algorithm can still be costly if the dictionary turns out
%to have a large number of bases, which results in a large number of iterations. 
%To address this issue, a numerical shortcut is provided in Section 
%\ref{sec: trunc error} that significantly reduces the cost.\\
%
%
%The benefit of the two-step procedure is as follows:
%The first step does not involve the full space-time simulation of the twin model, because
%the solution mismatch $\mathcal{M}$ is replaced by the integrated truncation error $\mathcal{T}$.
%Thereby the computational cost of the forward-backward iteration is reduced.
%The second step only involves 
%the minimization of $\mathcal{M}$, by using the basis dictionary obtained from the first step.
%Although the full space-time simulation of the twin model is required in the minimization
%of $\mathcal{M}$,
%the second step does not involve any iteration on the basis selection. To sum up, the two-step procedure
%allows the separation of the basis selection and the minimization of the solution mismatch,
%therefore is computationally preferrable.\\


%\subsection{Minimizing the Truncation Error}
%\label{sec: trunc error}
%In the previous sections, the twin model method was developed. The method infers the unknown
%$F$ in a gray-box simulator by training a twin model that minimizes the 
%space-time solution mismatch.
%The training can be expensive: In the algorithm \ref{alg: train twin}, each iteration 
%involves $k$ minimization problems of the solution mismatch in the forward step, 
%and $k$ minimization problems of the solution mismatch in the backward step,
%where each minimization requires at least one twin-model simulation.
%Instead of the solution mismatch, this section develops another metric to minimize in order
%to reduce the computational cost.\\
%
%%To reduce the computational cost, a ``pre-training'' step is proposed
%%where an ``integrated truncation error'' is minimized. 
%%A pre-trained twin model is then ``fine tuned'' to minimize the solution mismatch.
%%The applicability of the pre-training is studied; in particular, I study
%%under what condition can the solution mismatch be bounded by the integrated truncation error.
%%Finally, a stochastic gradient descent approach is adopted that efficiently 
%%minimizes the integrated truncation error.\\
%
%
%Define
%%\begin{equation}
%%    \tau := \frac{\partial u}{\partial t} 
%%    + \nabla \cdot \big(D \tilde{F}(u)\big) - q(u,c)\,,
%%    \label{eqn: residual}
%%\end{equation}
%to be the residual of the twin-model PDE \eqref{eqn: govern twin model},
%\begin{equation*}\begin{split}
%    \frac{\partial \tilde{u}}{\partial t}+ \nabla \cdot \big(D \tilde{F}(\tilde{u})\big) = q(\tilde{u},c)\,,
%\end{split}
%\end{equation*}
%by replacing $\tilde{u}$ with $u$, where $u$ is the gray-box solution satisfying 
%\eqref{eqn: govern PDE}
%\begin{equation*}\begin{split}
%    \frac{\partial u}{\partial t}+ \nabla \cdot \big( D F(u) \big) = q(u,c)\,.
%\end{split}
%\end{equation*}
%%Let its twin-model discretization be $\boldsymbol{\tau}$ (I think this is not clearly explained, 
%%but can't find a good way of expressing this).
%Assume the gray-box simulator and its twin model use the same space-time grid. 
%Define $\boldsymbol{\tau}$ to be the discretized residual obtained by plugging the discretized 
%gray-box solution into the twin-model simulator.
%Define the integrated truncation error to be
%%\begin{equation}
%%    \mathcal{T}(\tilde{F}) = \sum_{i=1}^M \sum_{j=1}^N w_{ij} \boldsymbol{\tau}_{ij}^2 \,,
%%    \label{eqn: truncation error}
%%\end{equation}
%where $w_{ij}$ are the quadrature weights defined in \eqref{eqn: solution mismatch}. 
%$i,j$ are the indices for time and space grid.\\
%
%The previous section discussed that the minimization of $\mathcal{M}$ can be computationally expensive.
%To reduce the computational cost, I propose to train a twin model 
%by minimizing the integrated truncation error.
%In other words, the coefficients of a twin model can be determined by
%\begin{equation}
%    \alpha_{\mathcal{A}} \leftarrow \argmin_{\alpha} \mathcal{T}\left(\sum_{i \in \mathcal{A}}
%    \alpha_i \phi_i\right)\,.
%    \label{eqn: minimize truncation error}
%\end{equation}
%
%\eqref{eqn: minimize truncation error} can be solved using algorithm \ref{alg: train twin} by
%replace $\mathcal{M}$ with $\mathcal{T}$. As a consequence, in the algorithm  \ref{alg: train twin}, 
%the estimator for the 
%significance of a candidate basis, $s_l(\mathcal{A})$, is replaced by 
%\begin{equation}
%    s_l^t(\mathcal{A}) \equiv \left|\int_{u\in \mathbb{R}^k} \left.\frac{d\mathcal{T}}{d \tilde{F}}
%    \right|_{\tilde{F}_\mathcal{A}^*} \phi_l \; \textrm{d} u \right|\,.
%    \label{eqn: basis significance 2}
%\end{equation}
%In addition, the validation error, $\overline{\mathcal{M}}$, is replaced by
%\begin{equation}
%    \overline{\mathcal{T}} = \frac{1}{k}\left( \mathcal{T}_1 + \mathcal{T}_2 + \cdots + 
%    \mathcal{T}_k\right)\,,
%    \label{eqn: pre train validation error}
%\end{equation}
%where
%\begin{equation}
%    \mathcal{T}_i = \texttt{IntegratedTruncationError}(T_i, \boldsymbol{u}_i)\,.
%\end{equation}
%for $i=1, \cdots, k$.
%To sum up, \eqref{eqn: minimize truncation error} can be solved by the following algorithm.
%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\REQUIRE{Initial basis dictionary $\boldsymbol{\phi}_\mathcal{A}$, 
%        coefficients $\alpha_{\mathcal{A}}^t= \mathbf{0}$},
%        Validation error $\overline{\mathcal{T}}_0 = \infty$,
%        Gray-box solution $\boldsymbol{u}$.
%\STATE Minimize solution mismatch 
%       $
%           \alpha^t_{\mathcal{A}} \leftarrow \argmin_{\alpha} \mathcal{T}\left(\sum_{i \in \mathcal{A}} \alpha_i \phi_i\right)
%       $ 
%\LOOP 
%\STATE Find $\phi_{l}\in \mathcal{N}(\boldsymbol{\phi}_{\mathcal{A}}) \backslash 
%       \boldsymbol{\phi}_{\mathcal{A}}$ with the maximal $s_l^t(\mathcal{A})$ \\
%       $\mathcal{A} \leftarrow 
%       \mathcal{A} \bigcup \{l\}$, $\boldsymbol{\phi}_{\mathcal{A}} \leftarrow 
%       \boldsymbol{\phi}_{\mathcal{A}}
%       \bigcup \{\phi_{l}\}$, $\alpha^t_l = 0$,
%       $\alpha^t_{\mathcal{A}} \leftarrow \{\alpha^t_{\mathcal{A}}, \alpha^t_l\}$
%\STATE Compute $\overline{\mathcal{T}}$ by $k$-fold cross validation.
%\IF {$\overline{\mathcal{T}} < \overline{\mathcal{T}}_0$} 
%    \STATE $\overline{\mathcal{T}}_0\leftarrow \overline{\mathcal{T}}$\\
%       $
%           \alpha^t_{\mathcal{A}} \leftarrow \argmin_{\alpha} \mathcal{T}\left(\sum_{i \in \mathcal{A}} \alpha_i \phi_i\right)
%       $
%\ELSE \STATE 
%       $\mathcal{A} \leftarrow 
%       \mathcal{A} \backslash \{l\}$, $\boldsymbol{\phi}_{\mathcal{A}} \leftarrow 
%       \boldsymbol{\phi}_{\mathcal{A}}
%       \backslash \{\phi_{l}\}$, $\alpha^t_{\mathcal{A}} \leftarrow \alpha^t_{\mathcal{A}} \backslash 
%       \{\alpha^t_l\}$
%      \textbf{break}
%\ENDIF
%\STATE Find $\phi_{l^\prime}\in 
%       \boldsymbol{\phi}_{\mathcal{A}}$ with the least $s_{l^\prime}^t(\mathcal{A})$  \\
%\IF {${l^\prime} \neq {l}$}
%\STATE  
%       $\mathcal{A} \leftarrow 
%       \mathcal{A} \backslash \{l^\prime\}$,
%       $\boldsymbol{\phi}_{\mathcal{A}} \leftarrow \boldsymbol{\phi}_{\mathcal{A}}
%        \backslash\{\phi_{l^\prime}\}$, 
%       $\alpha^t_{\mathcal{A}} \leftarrow \alpha^t_{\mathcal{A}}\backslash \{\alpha^t_{l^\prime}\}$
%\STATE Compute $\overline{\mathcal{T}}$ by $k$-fold cross validation.
%\IF{  $\overline{\mathcal{T}} < \overline{\mathcal{T}}_0$ }
%\STATE $\overline{\mathcal{T}}_0 \leftarrow \overline{\mathcal{T}}$\\
%       $
%           \alpha^t_{\mathcal{A}} \leftarrow \argmin_{\alpha} \mathcal{T}\left(\sum_{i \in \mathcal{A}} \alpha_i \phi_i\right)
%       $
%\ELSE \STATE
%       $\mathcal{A} \leftarrow \mathcal{A}\bigcup \{l^\prime\}$,
%       $\phi_{\mathcal{A}}\leftarrow \phi_{\mathcal{A}} 
%       \bigcup \{\phi_{l^\prime}\}$, $\alpha^t_{\mathcal{A}} \leftarrow 
%       \alpha^t_{\mathcal{A}}\bigcup \{\alpha^t_{l^\prime}\}$
%\ENDIF
%\ENDIF
%\ENDLOOP
%\ENSURE $\mathcal{A}$, $\phi_{\mathcal{A}}$, $\alpha^t_{\mathcal{A}}$.
%\end{algorithmic}
%\caption{Training a twin model by minimizing the integrated truncation error.}
%\label{alg: train twin trunc}
%\end{algorithm}
%
%
%Using algorithm \ref{alg: train twin trunc}, a two-step
%procedure is proposed to train the twin model. In the first step, the twin model
%is pre-trained using algorithm \ref{alg: train twin trunc}.
%In the second step, the $\phi_{\mathcal{A}}$, obtained from algorithm 
%\ref{alg: train twin trunc}, is used as the basis dictionary.
%Its coefficients, $\alpha_\mathcal{A}^t$, are fine tuned by minimizing the 
%solution mismatch $\mathcal{M}$.\\
%
%The benefit of the two-step procedure is as follows:
%The first step does not involve the full space-time simulation of the twin model, because
%the solution mismatch $\mathcal{M}$ is replaced by the integrated truncation error $\mathcal{T}$.
%Thereby the computational cost of the forward-backward iteration is reduced.
%The second step only involves 
%the minimization of $\mathcal{M}$, by using the basis dictionary obtained from the first step.
%Although the full space-time simulation of the twin model is required in the minimization
%of $\mathcal{M}$,
%the second step does not involve any iteration on the basis selection. To sum up, the two-step procedure
%allows the separation of the basis selection and the minimization of the solution mismatch,
%therefore is computationally preferrable.\\
%
%However, the minimization of $\mathcal{T}$ does not guarantee a bounded $\mathcal{M}$.
%In the following, I study the condition under which $\mathcal{M}$ can be bounded by $\mathcal{T}$.
%A sufficient condition for the bound is provided
%by Theorem \ref{theorem: 2}.\\
%
%\begin{theorem}
%    Consider a twin model simulator whose one-step time marching is
%    \begin{equation}
%        \mathcal{G}_i:\, \mathbb{R}^N\mapsto\mathbb{R}^N,\, \tilde{\boldsymbol{u}}_{i\cdot}\rightarrow 
%        \tilde{\boldsymbol{u}}_{i+1\cdot}=\mathcal{G}_i
%        \tilde{\boldsymbol{u}}_{i\cdot} \,,\quad i=1,\cdots, M-1\,.
%    \end{equation}
%    Assume the quadrature weights are time-independent, i.e.
%    $w_{ij} = w_{j}$ for all $i,j$.
%    If $\mathcal{G}_i$ satisfies
%    \begin{equation}
%        \|\mathcal{G}_ia-\mathcal{G}_ib\|^2_{W} \le \beta \|a-b\|^2_{W} \,,
%        \label{eqn: contractive}
%    \end{equation}
%    with $\beta<1$,
%    for any $a, b \in \mathbb{R}^N$ and for all $i$,
%    then 
%    \begin{equation}
%        \mathcal{M} \le \frac{1}{1-\beta} \mathcal{T}\,,
%    \end{equation}
%    where
%    \begin{equation}
%        \|v\|^2_{W} \equiv v^T 
%            \begin{pmatrix}
%                {w_{1}} && \\
%                & \ddots & \\
%                && {w_{N}}
%            \end{pmatrix} v
%    \end{equation}
%    for any $v\in \mathbb{R}^N$.
%    \label{theorem: 2}
%\end{theorem}
%The proof is given in Appendix \ref{proof 2}. The theorem implies that,
%if the twin model is a contractive dynamical 
%system \cite{contractive system}, as given by \eqref{eqn: contractive}, then the
%solution mismatch can be bounded by the integrated truncation error. In contrast, 
%the bound may not exist for non-contractive dynamical systems, for example for systems 
%that exhibit bifurcation \cite{dynamical system}. It is a future work to further investigate the applicability of the pre-training
%theoretically, in particular, to investigate the necessary and sufficient condition for the bound. \\

%Because the residual $\boldsymbol{\tau}$ can be evaluated explicitly given the gray-box
%solution, 
%the evaluation can be decoupled for different space-time grid points $\{i,j\}$ 
%(Do you have a better way to expressing this?). By viewing
%the truncation error at each $\{i,j\}$ as a stochastic sample, \eqref{eqn: minimize truncation error}
%can be solved by stochastic gradient descent, Algorithm \ref{alg: 2}.\\
%\begin{algorithm}[htbp]
%\begin{algorithmic}[1]
%    \REQUIRE $\alpha = \alpha_0$
%    \FOR{$(i,j)=(1,1)$ \TO $(M,N)$ }
%         \IF {not converged}
%             \STATE $\alpha \leftarrow \alpha -\lambda w_{ij}\left.\frac{\partial }{\partial \alpha} 
%             \boldsymbol{\tau_{ij}} \right.$
%         \ELSE
%             \STATE \textbf{break}
%         \ENDIF
%    \ENDFOR
%    \ENSURE $\alpha$
%\end{algorithmic}
%\caption{Minimizing the integrated truncation error by stochastic gradient descent.}
%\label{alg: 2}
%\end{algorithm}
%
%$\lambda>0$ is a tunable step size. $\lambda$ can tuned manually to 
%increase convergence speed while avoiding divergence \cite{stochastic search}.
%In practice, it is beneficial to compute the gradient
%against more than one grid points (called a ``mini-batch'') at each iteration. This is because
%the code can take advantage of vectorization libraries rather than computing the residual 
%at each grid point separately.\\

\newpage
\section{Numerical Results}
\label{sec: twin numerical results}
This section demonstrates the twin model on the estimation of the gradients for
several numerical examples.


\subsection{Buckley-Leverett Equation}
\label{sec: chap 2 BL}
Section \ref{sec: flux param} has applied a sigmoid parameterization to the gray-box model
governed by the Buckley-Leverett equation \eqref{eqn: Buckley-Leverett}
\begin{equation*}
    \frac{\partial u}{\partial t} + \frac{\partial}{\partial x}\Big({
    \frac{u^2}{1+ 2(1-u)^2}} \Big) = c\,,
\end{equation*}
In this section, the same problem 
is studied but using the adaptive basis construction developed 
in Section \ref{sec: adaptive basis}. 
The initial dictionary, $\mathcal{A}$, is selected to contain a single basis
$\left(1, 0\right)$. The choice of initial dictionary is not unique.
We choose $\left(1, 0\right)$ because it has a low resolution 
and is centered inside $\left[u_{\min}, u_{\max}\right]$ of the gray-box solution.\\

Figure \ref{fig: basis pnt} shows the selected bases
for the three solutions in Figure \ref{fig: combine 3}, respectively, obtained by 
algorithm \ref{alg: train twin}.
As $\left[u_{\min}, u_{\max}\right]$ shrinks, the number of bases in the dictionary decreases
but the resolution of the bases increases. Figure \ref{fig: basis history} shows
the dictionary after each forward-backward iteration for Figure \ref{fig: basis pnt 3}.\\

\begin{figure}[htbp]\begin{center}
    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/case_1.png}
        \caption{Solution 1}
        \label{fig: basis pnt 1}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}     
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/case_2.png}
        \caption{Solution 2}
        \label{fig: basis pnt 2}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/case_3.png}
        \caption{Solution 3}
        \label{fig: basis pnt 3}
    \end{subfigure}
    \caption{The basis dictionary for the three solutions in Figure \ref{fig: combine 3}.
             The iteration starts from the initial basis $(1,0)$. The bases in the
             dictionary are indicated by red dots, and the deleted bases are indicated
             by blue crosses.}
    \label{fig: basis pnt}
\end{center}\end{figure}

%(Because the trained flux is visually indifferent to Figure 2-11, the flux is not plotted. 
%Do you think I still need to plot them?)

\begin{figure}[htbp]\begin{center}
    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/figure_1.png}
        \caption{Iteration 1}
        \label{fig: basis history 1}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}     
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/figure_2.png}
        \caption{Iteration 2}
        \label{fig: basis history 2}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/figure_3.png}
        \caption{Iteration 3}
        \label{fig: basis history 3}
    \end{subfigure}\\

    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/figure_4.png}
        \caption{Iteration 4}
        \label{fig: basis history 4}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}     
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/figure_5.png}
        \caption{Iteration 5}
        \label{fig: basis history 5}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/figure_6.png}
        \caption{Iteration 6}
        \label{fig: basis history 6}
    \end{subfigure}\\

    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/figure_7.png}
        \caption{Iteration 7}
        \label{fig: basis history 7}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}     
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/figure_8.png}
        \caption{Iteration 8}
        \label{fig: basis history 8}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.5cm]{/home/voila/Documents/2014GRAD/test/forwardbackward/figure_9.png}
        \caption{Iteration 9}
        \label{fig: basis history 9}
    \end{subfigure}   
    \caption{The basis dictionary at each forward-backward iteration in Figure \ref{fig: basis pnt 3}. 
             The red dots indicate the bases in the dictionary, the blue crosses indicate the deleted
             basis.}
    \label{fig: basis history}
\end{center}\end{figure}


Consider a time-space-dependent control $c=c(t,x)$ in \eqref{eqn: Buckley-Leverett} 
\begin{equation*}
    \frac{\partial u}{\partial t} + \frac{\partial}{\partial x}\Big(
    \frac{u^2}{1+ 2(1-u)^2} \Big) = c\,,
\end{equation*}
and 
\eqref{eqn: Buckley-Leverett twin}
\begin{equation*}
    \frac{\partial \tilde{u}}{\partial t} + \frac{\partial}{\partial x}\tilde{F}(\tilde{u})
    = c\,.
\end{equation*}
The gradient of $\xi$, \eqref{eqn: objective ad hoc}, 
\begin{equation*}
    \xi(c) \equiv \int_{x=0}^1 \left(u(1,x; c) - \frac{1}{2}\right)^2 \,\textrm{d}x\,,
\end{equation*}
can be estimated by the trained twin model. The estimated gradients are compared with the true adjoint
gradients of the gray-box model, and the errors are shown in Figure \ref{fig: adap basis grad err BL}.
The adaptive basis construction improves the accuracy of the gradient estimation.
Table \ref{table: BL grad error} shows the error in the estimated gradient for
the three solutions.
The error in the estimated gradient is given by
\begin{equation*}
    \left(\int_0^1 \int_0^1 \left|\frac{d\tilde{\xi}}{dc} -\frac{d\xi}{dc} \right|^2 \; dxdt\right)^{1/2}\,,
\end{equation*}
where $\xi$ is the objective function evaluated by the gray-box model, and 
$\tilde{\xi}$ is the objective function evaluated by the twin model.
We compare the result by using the manually chosen bases in Figure
\ref{fig: sigmoid basis ad hoc} and by using the bases constructed adaptively.\\

\begin{figure}[htbp]\begin{center}
    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.8cm, height=4.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/1_adj_err_mod.png}
        \caption{Solution 1}
        \label{fig: grad BL 1}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}     
        \centering
        \includegraphics[width=4.8cm, height=4.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/6_adj_err.png}
        \caption{Solution 2}
        \label{fig: grad BL 2}
    \end{subfigure}
    \begin{subfigure}[t]{.32\textwidth}
        \centering
        \includegraphics[width=4.8cm, height=4.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/7_adj_err_mod.png}
        \caption{Solution 3}
        \label{fig: grad BL 3}
    \end{subfigure}
    \caption{The error of the estimated gradient, $\left|\frac{d\tilde{\xi}}{dc} - \frac{d\xi}{dc}
             \right|$, for the three solutions. The basis dictionary is constructed adaptively.}
    \label{fig: adap basis grad err BL}
\end{center}\end{figure}


\begin{center}
    \begin{tabular}{|c|c|c|c|}
       \hline\hline
         & Solution 1 & Solution 2 & Solution 3\\
       \hline
       Manual basis & $2.5\times 10^{-3}$& $6.6\times 10^{-4}$ & $7.3\times 10^{-5}$ \\
       \hline
       Adaptive basis & $4.2\times 10^{-6}$& $1.5\times 10^{-6}$ & $8.9\times 10^{-7}$ \\
       \hline\hline
    \end{tabular}
    \captionof{table}{The error of the estimated gradients for the three solutions.
                      The adaptively constructed bases reduce the estimation error.}
    \label{table: BL grad error}
\end{center}


\subsection{Navier-Stokes Flow}
\label{sec: chap2 num example NS}
Consider
a compressible internal flow in a 2-D return bend channel 
driven by the pressure difference between the inlet and the outlet.
The geometry of the return bend is given in Figure \ref{fig: NS mesh}.
The return bend is bounded by 
no-slip walls. The inlet static pressure and the outlet pressure are fixed.
The inner and outer boundaries of the bending section are each generated by 6 control points
using quadratic B-spline.\\
\begin{figure}[htbp]\begin{center}
    \includegraphics[width=10.5cm]{/home/voila/Documents/2014GRAD/numpad/test/ns_cases_spline/results/mesh/spline.png}
    \caption{The return bend geometry and the mesh for the simulation. The control points
             for the inner and outer boundaries are indicated by the red dots.}
    \label{fig: NS mesh}
\end{center}\end{figure}

The flow is governed by Navier-Stokes equations.
Let $\rho$, $u$, $v$, $E$, and $p$ denote the density, Cartesian velocity components, 
total energy, and pressure.
The steady-state Navier-Stokes equation is \cite{aero book}
\begin{equation}
    \frac{\partial}{\partial x} 
    \begin{pmatrix}
        \rho u\\
        \rho u^2 + p - \sigma_{xx}\\
        \rho uv - \sigma_{xy}\\
        u(E\rho+p) - \sigma_{xx} u - \sigma_{xy} v
    \end{pmatrix}
    + \frac{\partial}{\partial y}
    \begin{pmatrix}
        \rho v\\
        \rho uv-\sigma_{xy}\\
        \rho v^2+p-\sigma_{yy}\\
        v(E\rho+p) - \sigma_{xy} u -\sigma_{yy}v
    \end{pmatrix} 
    = \boldsymbol{0}\,,
    \label{NSeqn}
\end{equation}
where
\begin{equation}\begin{split}
    \sigma_{xx} &= \mu \left(2 \frac{\partial u}{\partial x} - \frac{2}{3} \left(\frac{\partial u}{\partial x} 
    + \frac{\partial v}{\partial y}\right)\right)\\
    \sigma_{yy} &= \mu \left(2 \frac{\partial v}{\partial y} - \frac{2}{3} \left(\frac{\partial u}{\partial x} 
    + \frac{\partial v}{\partial y}\right)\right)\\
    \sigma_{xy}&=\mu\left(\frac{\partial u}{\partial y} + \frac{\partial v}{\partial x}\right)
\end{split}\,.\end{equation}
The Navier-Stokes equation requires an additional equation, the state equation, for closure 
\cite{aero book}.
The state equation has the form
\begin{equation}
    p = p(U, \rho)\,,
    \label{state equation}
\end{equation}
where $U$ denotes the internal energy per unit volume \cite{aero book},
\begin{equation}
    U = \rho\left(E-\frac{1}{2}(u^2+v^2)\right)\,.
\end{equation}

Many models have been developed for the state equation, such as the ideal gas equation, the
van der Waals equation, and the Redlich-Kwong equation \cite{state eqns}.
We assume the true state equation in the gray-box simulator is unknown. 
The state equation will be inferred from the gray-box solution. 
Let $\rho_\infty$ be the steady state density,
$\boldsymbol{u}_\infty = (u_\infty, v_\infty)$ be the steady state Cartesian velocity,
and $E_\infty$ be the steady state energy density.
The steady state mass flux is
\begin{equation}
    \xi = - \int_{\textrm{outlet}} \rho_\infty u_\infty \big|_{\textrm{outlet}} \; dy=
    \int_{\textrm{inlet}} \rho_\infty u_\infty\big|_{\textrm{inlet}} \; dy
    \label{eqn: mass flux}
\end{equation}
The goal is to estimate the gradient of $\xi$
to the red control points' coordinates.
\\

Two state equations are tested: the van der Waals equation and the Redlich-Kwong equation
\cite{aero book, Redlich Kwong},
\begin{equation}\begin{split}
    %p_{ig} &= (\gamma-1) U\\
    p_{vdw} &= \frac{(\gamma-1)U}{1-b_{vdw}\rho} - a_{vdw}\rho^2\\
    p_{rk} &= \frac{(\gamma-1)U}{1-b_{rk}\rho} - 
    \frac{a_{rk}\rho^{5/2}}{((\gamma-1)U)^{1/2}(1+b_{rk}\rho)}
\end{split}\label{NS state equations}
\end{equation}
where we set $a_{vdw}=10^4$, $b_{vdw}=0.1$, $a_{rk}=10^7$ and $b_{rk}=0.1$.\\

The solution mismatch, \eqref{eqn: solution mismatch}, is given by
\begin{equation*}\begin{split}
    \mathcal{M} = &w_\rho \int_\Omega \left|\tilde{\rho}_{\infty} - \rho_{\infty}\right|^2 d\boldsymbol{x}
                + w_u
                \int_\Omega \left|\tilde{u}_{\infty}- u_{\infty}\right|^2 d\boldsymbol{x} \\
                + &w_v
                \int_\Omega \left| \tilde{v}_{\infty}- v_{\infty}\right|^2 d\boldsymbol{x}
                + w_E
                \int_\Omega \left|\tilde{E}_{\infty} - E_\infty\right|^2 d\boldsymbol{x}\,,
    \label{NS mismatch}
\end{split}\end{equation*}
where $w_\rho$, $w_u$, $w_v$, and $w_E$ are non-dimensionalization constants.
Figure \ref{fig: grayErrSol Ubend} shows the gray-box solution and the solution mismatch
after training the twin model.\\

%\footnote{Both the solution and the mismatch are normalized.}.

\begin{figure}[htbp]\begin{center}
    \begin{subfigure}[t]{.49\textwidth}
        \centering
        \includegraphics[height=8cm]{../graysol_Ubend.png}
        \label{fig: graysol Ubend}
    \end{subfigure}
    \begin{subfigure}[t]{.49\textwidth}
        \centering
        \includegraphics[height=8cm]{../err_Ubend.png}
        \label{fig: errsol Ubend}
    \end{subfigure}
    \caption{Left column: an example gray-box solution for a given geometry. Right column:
             the solution mismatch after training a twin model.}
    \label{fig: grayErrSol Ubend}
\end{center}\end{figure}

The selected bases in the
dictionary, represented by $(j_U, j_{\rho}, \eta_U, \eta_\rho)$, are listed in Table 
\ref{tab: basis list 1} and \ref{tab: basis list 2}. Figure \ref{fig: NS Mtau history}
shows the cross validation error $\mathcal{M}_{\tau}$ at each forward-backward iteration.
Figure \ref{fig: state err Ubend} compares the true state equation and the corresponding trained 
state equation. The convex hull of $({U}_\infty, \rho_\infty)$, the internal energy
and the density of the gray-box solution,
is shown by the dashed red line.
Because the state equation is expected to be inferrable only inside the domain of the gray-box 
solution, large deviation is expected outside the convex hull.\\


\begin{center}\begin{tabular}{ccccc}
(2, 1, 9 , 1) & (2, 1, 11, 2) & (2, 1, 8 , 0) & (2, 1, 10, 2) & (2, 2, 8 , 3) \\
(2, 2, 9 , 0) & (2, 2, 9 , 1) & (2, 2, 9 , 3) & (2, 2, 9 , 4) & (2, 2, 10, 0) \\
(2, 2, 10, 1) & (2, 2, 10, 2) & (2, 2, 10, 3) & (2, 2, 11, 0) & (2, 2, 11, 3) \\
(2, 2, 12, 0) & (2, 2, 12, 4) & (2, 2, 13, 3) & (3, 2, 16, 4) & (3, 2, 18, 2) \\
(3, 2, 20, 1) & &&&
\end{tabular}
\captionof{table}{List of the dictionary for the van der Waals gas, $(j_{U}, j_{\rho},
\eta_{U}, \eta_{\rho})$.}
\label{tab: basis list 1}
\end{center}


\begin{center}\begin{tabular}{ccccc}
(2, 2, 8, 0) & (2, 2, 8, 2) & (2, 2, 8, 4) & (2, 2, 9, 0) & (2, 2, 9, 1)\\
(2, 2, 9, 2) & (2, 2, 9, 3) & (2, 2, 10, 2) & (2, 2, 10, 3) & (2, 2, 11, 0) \\
(2, 2, 11, 2) & (2, 2, 11, 3) & (2, 2, 11, 4) & (2, 2, 12, 0) & (2, 2, 12, 1) \\
(2, 2, 12, 2) & (2, 2, 12, 3) & (2, 2, 13, 1) &(2, 2, 13, 3) & (2, 2, 14, 0) \\
(2, 2, 14, 1) & (3, 2, 18, 2) & &\\
\end{tabular}
\captionof{table}{List of the dictionary for the Redlich-Kwong gas, $(j_{U}, j_{\rho},
\eta_{U}, \eta_{\rho})$.}
\label{tab: basis list 2}\end{center}

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=8cm]{../NS_Mtau_history.png}
        \caption{The cross validation error $\overline{\mathcal{M}}_{\tau}$ at 
                 each forward-backward iteration. The y-axis is scaled by a constant
                 so the $\overline{\mathcal{M}}_\tau$ at the first forward-backward
                 iteration equals $1$.}
        \label{fig: NS Mtau history}
    \end{center}
\end{figure}

\begin{figure}[htbp]\begin{center}
    \begin{subfigure}[t]{.99\textwidth}
        \centering
        \includegraphics[height=5.cm]{../state_eqn_vdw.png}
        \caption{(a)}
        \label{fig: graysol Ubend}
    \end{subfigure}\\
    \begin{subfigure}[t]{.99\textwidth}
        \centering
        \includegraphics[height=5.cm]{../state_eqn_rk.png}
        \caption{(b)}
        \label{fig: errsol Ubend}
    \end{subfigure}
    \caption{The state equation for the van der Waals gas (a), and for the
             Redlich-Kwong gas (b).
             The left column shows the trained state equation, and the right
             column shows the gray-box state equation. The trained state equation
             is added by a constant so the pressure matches the pressure of the
             gray-box equation at $U=2.5$ and $\rho=0.7$. The dashed red line shows
             the convex hull of the gray-box solution.}
    \label{fig: state err Ubend}
\end{center}\end{figure}


The trained twin model enables the adjoint gradient estimation.
Figure \ref{fig: geo grad all} shows the
estimated gradient of $\xi$ with respect to the control points coordinates. It also
compares the estimated gradient with the true gradient. 
The error of the gradient estimation is given in Table \ref{tab: idea gas gradient}.

\begin{figure}[htbp]\begin{center}
    \begin{subfigure}[t]{.45\textwidth}
        \centering
        \includegraphics[height=7.5cm]{/home/voila/Documents/2014GRAD/numpad/test/ns_cases_spline/results/outflux_geo_grad/geo_grad_0_spline.png}
        \caption{
        The gradient of $\xi$ to the control points for the 
        Redlich-Kwong gas. 
        The wide gray arrow is the gradient evaluated by the gray-box model, while
        the thin black arrow is the gradient evaluated by the twin model using finite difference.
        }
        \label{fig: geo grad}
    \end{subfigure}
    \hspace{.5cm}
    \begin{subfigure}[t]{.45\textwidth}
        \centering
        \includegraphics[height=7.5cm]{/home/voila/Documents/2014GRAD/numpad/test/ns_cases_spline/results/outflux_geo_grad/perturb_0.png}
        \caption{
        The boundary perturbed according to the gradient. 
        The blue dashed line is computed by finite difference of the gray-box model, while the
        red dashed line is computed by the twin model's gradient.
        }
        \label{fig: geo grad perturb}
    \end{subfigure}
    \caption{A comparison of the estimated gradient and the true gradient.}
    \label{fig: geo grad all}
\end{center}\end{figure}

\begin{center}
\begin{tabular}{|c|c|C{12mm}|C{12mm}|C{12mm}|C{12mm}|C{12mm}|C{12mm}|C{12mm}|C{12mm}|}
\hline\hline
{Gas}&\multicolumn{4}{c|}{Interior control points}&\multicolumn{4}{c|}{Exterior control points}\\
\cline{1-9}
van der Waals &0.13 & 0.04 & 0.05 & 0.32 &0.16 & 0.15 & 0.07 & 0.02\\
\cline{1-9}
Redlich-Kwong& 0.32 & 0.03 & 0.07 & 0.50 & 0.40 & 0.12 & 0.06 & 0.05\\
\hline\hline
%y&Graybox&-0.971&-2.852&1.792&1.472&1.190&0.596&-1.854&-3.097\\
%\cline{2-10}
%&Twin model&-0.972&-2.849&1.792&1.473&1.189&0.595&-1.853&-3.094\\
%\hline\hline

%\multicolumn{2}{|c|}{VDW gas}&\multicolumn{4}{c|}{gradient at inner boundary}&\multicolumn{4}{c|}{gradient at outer boundary}\\
%\cline{1-10}
%x&Graybox&-2.389&-9.091&-7.736&-2.073&0.660&6.003&7.756&1.846\\
%\cline{2-10}
%&Twin model&-2.392&-9.085&-7.739&-2.078&0.659&6.006&7.759&1.846\\
%\hline
%y&Graybox&-0.942&-2.848&1.785&1.443&1.192&0.593&-1.858&-3.105\\
%\cline{2-10}
%&Twin model&-0.948&-2.846&1.789&1.452&1.188&0.593&-1.858&-3.104\\
%\hline\hline
%
%\multicolumn{2}{|c|}{RK gas}&\multicolumn{4}{c|}{gradient at inner boundary}&\multicolumn{4}{c|}{gradient at outer boundary}\\
%\cline{1-10}
%x&Graybox&-2.429&-9.064&-7.749&-2.122&0.663&6.045&7.773&1.837\\
%\cline{2-10}
%&Twin model&-2.412&-9.039&-7.702&-2.092&0.660&6.000&7.731&1.832\\
%\hline
%y&Graybox&-1.010&-2.848&1.820&1.536&1.183&0.603&-1.851&-3.081\\
%\cline{2-10}
%&Twin model&-0.995&-2.844&1.812&1.519&1.190&0.596&-1.845&-3.076\\
%\hline\hline
\end{tabular}
\captionof{table}{The error of the gradient estimation, in percentage.}
\label{tab: idea gas gradient}
\end{center}


\subsection{Polymer Injection in Petroleum Reservoir}
\label{sec: chap 2 reservoir}
Water flooding is a technique to enhance the secondary recovery in petroleum reservoirs, as illustrated
in Figure \ref{fig: polymer sketch}. 
Injecting pure water can be cost-inefficient due to low water viscosity
and high water cut. Therefore, water-solvent polymer can be utilized to increase the water-phase
viscosity and to reduce the residual oil.\\

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=6cm]{/home/voila/Documents/mrst-2015b/results/waterflooding.png}
        \caption{Water flooding in petroleum reservoir engineering (from PetroWiki). Polymer 
                 solved in the water phase can be injected into the reservoir to enhance the
                 production of oil.}
        \label{fig: polymer sketch}
    \end{center}
\end{figure}

Consider a reservoir governed by the two-phase porous media flow equations
\begin{equation}\begin{split}
    \frac{\partial }{\partial t} \left(\rho_\alpha \phi S_\alpha \right) + \nabla \cdot
    \left( \rho_\alpha \vec{v}_{\alpha} \right) &= 0\,, \quad \alpha \in \{w,o\}\\
    \frac{\partial}{\partial t}\left( \rho_w \phi S_w c \right) + \nabla \cdot
    \left( c \rho \vec{v}_{wp}\right) &= 0        
    \end{split}\,,
    \label{eqn: two phase polymer}
\end{equation}
for $x\in \Omega$ and $t\in [0,T]$,
where the phase velocities are given by the Darcy's law
\begin{equation}\begin{split}
    \vec{v}_\alpha &= - {M_\alpha} k_{r\alpha} \boldsymbol{K} \cdot (\nabla p - \rho_w g \nabla z), \, \quad \alpha \in \{w,o\}\\
    \vec{v}_{wp} &= -{M_{wp}} k_{rw} \boldsymbol{K} \cdot (\nabla p - \rho_{w} g \nabla z)
\end{split}\,.
\label{eqn: darcy law}
\end{equation}
$w, o$ indicate the water and oil phases.
$\rho$ is the phase density. $\phi$ is the porosity. $S$ is the phase saturation where
$S_w+S_o=1$.
$c$ is the polymer concentration in the water phase. $v_{w}$, $v_{o}$, 
$v_{wp}$ are the componentwise velocities of water, oil, and polymer. 
$\boldsymbol{K}$
is the permeability tensor. $k_{r}$ is the relative permeability. $p$ is the pressure. $z$ is the depth.
$g$ is the gravity constant. The mobility factors, $M_o, M_w, M_{wp}$, 
model the modification of the componentwise mobility due to the presence of polymer.
In the sequel, the models for the mobility factors are unknown. The only 
knowledge about the mobility factors is that they depend on $S_w, p$, and $c$.\\

\emph{PSim}, the simulator aforementioned in Section \ref{sec: motivation}, is
used as the gray-box simulator, which uses the IMPES time marching 
\cite{reservoir sim book}, i.e. implicit in pressure
and explicit in saturation, as well as the upwind scheme.
Its solution, $S_w$, $c$, and $p$ can be used to train the 
twin model. The twin model uses fully implicit time marching and the upwind scheme.
The solution mismatch is defined by
\begin{equation}
    \mathcal{M} = w_{S_w}\int_0^T\int_\Omega |S_w-\tilde{S}_w|^2 d\boldsymbol{x} dt
                + w_{c}\int_0^T\int_\Omega |c-\tilde{c}|^2 d\boldsymbol{x} dt 
                + w_{p}\int_{0}^T\int_\Omega |p-\tilde{p}|^2 d\boldsymbol{x} dt\,,
    \label{eqn: polymer sol mismatch}
\end{equation}
where $w_{S_w}$, $w_c$, and $w_p$ are non-dimensionalization constants.\\

%\begin{center}
%    \includegraphics[height=2.1cm]{/home/voila/Documents/mrst-2015b/results/grid1D.png}
%\end{center}
%\includegraphics[width=5.2cm]{/home/voila/Documents/mrst-2015b/results/sW_1D.png}
%\includegraphics[width=5.2cm]{/home/voila/Documents/mrst-2015b/results/c_1D.png}

Consider a reservoir setup shown in Figure \ref{fig: reservoir 3D mesh}, which
is a 3D block with two injectors and one producer. The permeability
is 100 milli Darcy, and the porosity is 0.3. A constant injection rate of $10^6 
\texttt{ft}^3/\texttt{day}$ is used at both the injectors.
The reservoir is simulated for $t\in [0,50] \texttt{day}$.
The solution of $S_w$ is illustrated in Figure \ref{fig: reservoir 3D solutions} for the untrained 
twin model, the gray-box model, and the trained twin model, respectively. 
After the training, the twin-model solution matches the gray-box solution closely.\\

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[height=5.cm]{/home/voila/Documents/mrst-2015b/results/grid.png}
        \caption{The geometry of the petroleum reservoir.}
        \label{fig: reservoir 3D mesh}
    \end{center}
\end{figure}

\begin{figure}[htbp]\begin{center}
    \begin{subfigure}[t]{.99\textwidth}
        \centering
        \includegraphics[height=5.cm]{/home/voila/Documents/mrst-2015b/results/iso_sW_notrain_2D.png}
        \label{fig: reservoir 3D untrained}
        \caption{Untrained twin model.}
    \end{subfigure}\\
    \begin{subfigure}[t]{.99\textwidth}
        \centering
        \includegraphics[height=5.cm]{/home/voila/Documents/mrst-2015b/results/iso_sW_gray_2D.png}
        \label{fig: reservoir 3D untrained}
        \caption{PSim.}
    \end{subfigure}\\
    \begin{subfigure}[t]{.99\textwidth}
        \centering
        \includegraphics[height=5.cm]{/home/voila/Documents/mrst-2015b/results/iso_sW_twin_2D.png}
        \label{fig: reservoir 3D untrained}
        \caption{Trained twin model.}
    \end{subfigure}
    \caption{The isosurfaces of $S_w=0.25$ and $S_w=0.7$ at $t=30$ days. After the training,
             the twin model solution matches the gray-box solution.}
    \label{fig: reservoir 3D solutions}
\end{center}\end{figure}

Let the objective function be the residual oil at $T=50 \, \texttt{day}$,
\begin{equation}
    \xi = \int_\Omega \rho_o(T) \phi S_o(T) \,\textrm{d} \boldsymbol{x} \,.
    \label{eqn: chap2 reservoir xi day 50}
\end{equation}
The gradient of $\xi$ with respect to the time-dependent injection rate is computed.
The gradient estimated by the twin model is shown in Figure \ref{fig: reservoir 3D gradient},
where the red and blue lines indicate the gradient for the two injectors. In comparison, 
the star markers show the true gradient at day $2$, $16$, $30$, and $44$, evaluated by finite difference.
Clearly, a rate increase at the injector 1 leads to more residual oil reduction than the injector 2.
This is because the injector 2 is closer to the producer, where a larger rate accelerates the 
water breakthrough that impedes further oil production.
It is observed that the estimated gradient closely matches the true gradient.
%, although
%the error slightly increases for ea$t$, 
%because of the different numerical schemes
%used in the twin and gray-box models. 
The error is given in Table \ref{tab: reservoir 3D grad error}.\\

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=8cm]{/home/voila/Documents/mrst-2015b/results/grad_2D.png}
        \caption{The gradient of $\xi$ with respect to rates at the two injectors.
                 The lines indicate the gradients estimated by the twin model, while
                 the stars indicate the true gradient evaluated by finite difference.}
        \label{fig: reservoir 3D gradient}
    \end{center}
\end{figure}

\begin{center}
    \begin{tabular}{|c|c|c|c|c|}
       \hline\hline
         Error & $t=0.04$ & $t=0.32$ & $t=0.6$ & $t=0.88$\\
       \hline
       Inj 1 & 1.7  & 1.0 & 0.6 & 0.2 \\
       \hline
       Inj 2 & 2.2 &  1.9 & 0.7 & 0.2 \\
       \hline\hline
    \end{tabular}
    \captionof{table}{The error of estimated gradient at day $2$, $16$, $30$, and $44$, in percentage.}
    \label{tab: reservoir 3D grad error}
\end{center}


\section{Chapter Summary}
\label{sec: chap 2 summary}
This chapter develops a method for gradient estimation by using the space-time solution of
gray-box conservation law simulations, at a cost independent of the dimensionality of the gradient.
The key to inferring $F$ is to leverage the gray-box space-time solution.
My method uses the big data, the gray-box space-time solution, to estimate the unknown 
component in the gray-box model and to estimate the gradient.
The twin model is an adjoint-enabled conservation law simulator, and can be trained 
to minimize a metric measuring its difference against the gray-box simulator.
Two metrics, the solution mismatch and the integration truncation error, are proposed.
%The inferrability of the twin model is studied theoretically for a simple PDE with only one equation
%and one dimensional space. 
To enable the training computationally, a sigmoid parameterization
is presented. 
Then the twin model method is demonstrated in the Buckley-Leverett equation
by using a set of manually chosen bases.
To further exploit the information contained in the
gray-box solution,
an adaptive basis construction procedure is presented. The adaptive 
procedure iterates over a forward step and a backward step to append and delete basis
in the basis dictionary.\\
%builds upon three key elements: the approximated basis significance,
%the basis neighborhood, and the cross validation. The algorithm for training the twin model
%is summarized. To alleviate the training cost, a pre-train step is suggested
%that minimizes the integrated truncation error instead of the solution mismatch.\\

The proposed twin model algorithm is demonstrated on 
a variety of numerical examples. The first example is the Buckley-Leverett equation,
whose flux function is inferred. The trained twin model accurately estimates 
the gradient of an objective to the source term. 
The second example is the steady-state Navier-Stokes equation in a return bend,
whose state equation is inferred. The inferred state equation allows 
estimating the gradient of mass flux to the control surface geometry.
The third example is the petroleum reservoir with polymer injection, where
the mobility factors are inferred. The gradient of the residual oil to
the injection rate is estimated.
With the aid of the estimated gradient, the objective can be optimized more efficiently,
which will be discussed in the next chapter.

