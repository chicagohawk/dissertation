\documentclass[a4paper,onecolumn]{article}
\usepackage{amsmath, amsthm, graphicx, amssymb, wrapfig, fullpage, subfigure, array,float}
\usepackage[]{algorithm2e}
\usepackage[toc, page]{appendix}
\usepackage{pdfpages, nomencl, pifont}
\usepackage[nottoc, numbib]{tocbibind}
\usepackage{tikz}
\usetikzlibrary{positioning,shadows,arrows}
\usepackage[font=sl, labelfont={sf}, margin=1cm]{caption}
\DeclareMathOperator{\e}{e}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{theorem}{Theorem}
\newtheorem{remarker}{Remark}
\newcommand{\doublerightharpoonup}{%
  \rightharpoonup\mkern-10mu\rightharpoonup%
}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\makenomenclature
\hyphenpenalty=10000
\linespread{2}
\setlength\parindent{20pt}

\begin{document}
\title{Bayesian optimization constrained by gray-box conservation law \\ with unknown
       flux functions}
\author{Han Chen, Qiqi Wang}
\date{}
\maketitle
\section{Abstract}

\indent
Many engineering applications can be formulated as optimizations constrained by conservation laws.
Such optimizations can be solved by the adjoint method, which computes the gradient of the objective
to the design variables.
Traditionally, the adjoint method has not been able to be implemented in many ``gray-box'' conservation law
simulators. In gray-box simulators, the analytical and the numerical form of the conservation law is unknown,
though the full solution of relevant flow quantities is available.
We have introduced the twin model method to estimate the gradient by using the gray-box simulator's space-time
solution.
In this paper, we develop a Bayesian optimization framework that uses both the objective and its estimated gradient
to improve optimization performance.
We also give theoretical results for the method's convergence.
The mthod is demonstrated in the optimization of several numerical examples, showing superior optimization performance.


\section{Background}
\label{background}
\noindent Optimization problems are of great interest in the engineering community. We consider an 
optimization problem to be
constrained by conservation laws.
For example, oil reservoir simulations may employ PDEs of various flow models, 
in which different fluid phases and components satisfy a set of conservation laws.
Such simulations can be used to facilitate the oil reservoir management,
including optimal well placement \cite{adjoint well placement} 
and optimal production control \cite{water flooding control,first reservoir opt}.
Another example is the cooling of turbine airfoils. 
We are interested in optimizing the interior flow path
of turbine airfoil coolant to minimize pressure loss
\cite{ubend rans opt 1, ubend rans opt 2}.\\

\indent We are interested in the optimization problem of the following form
\begin{equation}\begin{split}
    \min_{c\in\mathcal{C}} & \xi(u,c)\\
    \textrm{where}\; u\; \textrm{satisfies}& \;\; \mathcal{R}(u,c) = 0\\
    \textrm{subject to} \; c\in [0,1]^n
\end{split}\label{general opt}
\end{equation}
where $t\in[0,T], \,x\in\Omega$.
$u(t,\mathbf{x})$\,is the space-time solution of a conservation law PDE abstracted as
$\mathcal{R}$, with given initial and boundary conditions. The design variable is $c$.\\

\indent 
In many cases, the simulation of $\mathcal{R}(u,c)=0$ can be computationally costly, potentially 
due to the complex computational models involved, and the
large-scale time and space discretization. Furthermore, 
the dimensionality of the design space, $d$, can be high. 
A tool to enable efficient high-dimensional optimization is adjoint sensitivity analysis
[8-11]
, which efficiently computes $\frac{d\xi}{dc} \in\mathbb{R}^d$, the gradient of the objective to the design variables. 
\\

\indent Many conservation law simulators do not have the adjoint implemented.
Besides, we are not able to implement the adjoint method
when the governing PDE and its numerical implementation is unavailable:
for example, when the source code is proprietary or legacy.
However, many conservation law simulators can provide the space-time solution as an output.
If the simulation solves for time-independent problems, the simulation can
provide the steady-state spatial solution. When a simulator satisfies such conditions, we call the 
simulator ``gray-box'' \cite{twin model}.\\

\indent To enable the adjoint computation for gray-box simulations, we proposed the twin model method  
\cite{twin model}.
The method infers the gray-box simulator's governing PDE, $\mathcal{R}(u,c)=0$,
 by using its space-time or spatial solution.
The inferred model, $\tilde{\mathcal{R}}(\tilde{u},c)=0$, is called the ``twin model''. 
We can simulate the twin model to evaluate $\tilde{\xi} = \xi(\tilde{u},c)$ and its adjoint.
The twin model's gradient, $\frac{d \tilde{\xi}}{d c}$, can be used as an estimation for 
$\frac{d\xi}{dc}$.\\

\indent We are able to evaluate $\xi$ from the gray-box simulation, and to estimate
$\frac{\partial \xi}{\partial c}$ from the twin model's adjoint. The optimization
of $\xi$ fits naturally into a multi-fidelity optimization (MFO) framework,
where the gray-box and the twin-model simulators can be viewed as two models of different fidelity.
MFO methods can be categorized into local and global approaches:\\

Local MFO methods aim at finding a local optimum in the search domain, including the pattern-search 
and the trust-region MFO methods.
In pattern search methods, the objective function $J$ is evaluated at a set of
trial points, known as the `mesh', adjacent to the current best design point in the design space.
If the objective improves, the current best design is updated; otherwise, the mesh size
is shrinked \cite{Pattern Search Convergence}. 
Booker et al. \cite{Pattern Search Convergence MFO} extends the pattern search method to MFO and
proves its convergence.
In trust region methods, a surrogate is constructed in the neighborhood
of the current best design, known as the trust region.
Then the surrogate is optimized within the trust-region to generate the next
candidate design point. 
Depending on the availability of the gradient samples,
the surrogate can be either constructed using only the function value data \cite{jones1998, trustregionconn, trustregionwild},
or constructed also using the gradient data \cite{Alexandrov trust region, inexactgradient1}.\\

On the other hand, global MFO methods aim at finding the global optimum in the search domain.
An important class of global MFO methods is based on the Bayesian modelling of 
the objective function. 
Mockus et al. \cite{Mockus Bayesian opt} 
assumes the objective function is sampled from a Gaussian process. 
A posterior distribution for the function is maintained and updated when new data is sampled.
The posterior distribution is used to pick the next sample point. Popular choices include
the expected improvement (EI) method \cite{Mockus Bayesian opt, jones1998} and the upper confidence bound
(UCB) method \cite{GP bandit}. It has been shown both methods converges to the optimum 
under mild assumptions of the objective function \cite{convergen EI, converge Bull, GP bandit}.
When multi-fidelity models are available, Kennedy et al. \cite{KennedyOhagan1} 
calibrates the model inadequecy between different models by GP, and constructs
the posterior using samples from all the models. 
When gradient samples are available, the posterior can be constructed using the co-Kriging
method \cite{coKriging1, coKriging2}.\\


\indent Bayesian optimization uses all available model evaluations
to construct the posterior. 
In many cases, the conservation law simulation is computationally much more costly than
the optimization algorithm. Therefore, we aim at reducing the number of model evaluations,
while neglecting the expense of running the optimization algorithm.
A good choice is Bayesian optimization, which has been shown to outperform other state of art
global optimization algrithms in reducing the number of model evaluations.\\

\indent In this paper, we propose an adjoint-based optimization framework for gray-box 
conservation law simulation by combining the twin model method and the Bayesian MFO.
The Bayesian optimization uses both the objective function evaluated by the gray-box simulation and 
the gradient estimated by the twin model method. In the remainder of the paper, Section 
\ref{section graybox and twin} reviews the definition of
the gray-box simulation and the twin model method.
Section \ref{GPO} discusses the Bayesian optimization, especially Gaussian-process (GP) optimization. 
We are going to discuss two GP optimization methods:
the expected improvement (EI) method and the upper confidence bound (UCB) method.
Section \ref{GPtwin} combines the Bayesian optimization methods with the twin model method.
The proposed optimization framework takes samples from both the gray-box model's objective
function and the twin model's estimated gradient.
We also give the convergence proof for the optimization framework.
Section \ref{GPconv} demonstrate the optimization framework on three numerical testcases: (1) maximizing a 2-D Rosenbrock function,
(2) optimizing the control of a 1-D controlled porous media flow, and (3) optimizing the return bend geometry
of a 2-D Navier-Stokes flow.

\section{The gray-box simulation and the twin model method}
\label{section graybox and twin}
\noindent Many conservation law simulations are gray-box. 
By gray-box, we mean a conservation law simulation
 without the adjoint method implemented. Furthermore, we are not able to implement the adjoint method
when the governing PDE for the conservation law and its numerical implementation is unavailable:
for example, when the source code is proprietary or legacy.
Another defining property of gray-box simulation is that it can provide the space-time solution of the conservation law.
If the simulation solves for time-independent problems, a gray-box simulation should be able to 
provide the steady state solution.
In contrast, we define a simulator to be a blackbox, if neither the adjoint nor the solution is available.
The only output of such simulations is the value of the objective function to be optimized.
If the adjoint method is implemented or is able to be implemented,
we call such simulations open-box.
We summarize their differences in Table \ref{tab: boxes}.\\

\renewcommand{\arraystretch}{0.7}
\begin{center}
    \captionof{table}{Comparison of black-box, gray-box, and open-box simulations}
    \label{tab: boxes}
    {\setlength{\extrarowheight}{5pt}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
                   & PDE and implementation & {Space (or space-time) solution} & 
                   Adjoint\\[5pt] \hline
        Black-box  & \ding{56}       & \ding{56}    & \ding{56}  \\ \hline
        Gray-box   & \ding{56}
                   & \ding{52}    & \ding{56}   \\ \hline
        Open-box   & \ding{52}    &\ding{52}         &   \ding{52}      \\ \hline
    \end{tabular}}
\end{center}

\indent For example, consider a gray-box simulation for the conservation law $\mathcal{R}(u,c) = 0$:
\begin{equation}
    \dot{\boldsymbol{u}} + \nabla \cdot \overset{\doublerightharpoonup}{\boldsymbol{F}}
    (\boldsymbol{u})
    = \boldsymbol{q}(\boldsymbol{u},c)
    \label{first equation}
\end{equation}
where $\boldsymbol{u}$ is a vector representing flow quantities,
$\dot{\boldsymbol{u}}$ is the derivative of $\boldsymbol{u}$ with respect to time, $t\in[0,T]$.
$c$ represents the design variables,
and $x\in \Omega \subseteq \mathbb{R}^{n}$ is the spatial coordinate.
$\Omega$ may depend on the design variables, $c$.
$\overset{\doublerightharpoonup}{\boldsymbol{F}}$ is the flux tensor. The exact form 
of $\overset{\doublerightharpoonup}{\boldsymbol{F}}$ is unknown.
$\boldsymbol{q}$ is a source vector that may also depend on $c$.
The boundary and initial conditions are known.
The discretized space-time solution of Eqn.\eqref{first equation} given by the simulator
is written as $\hat{u}(t_i, \mathbf{x}_i; c)\,, i=1,\cdots,N$, where
$t=\left\{t_1,\cdots, t_N\right\}$ indicates the discretized time, and 
$\mathbf{x}_i$ indicates the spatial discretization at time $t_i$.
Because the flux is unknown, the adjoint method is not able
to be applied to Eqn.\eqref{first equation} directly.\\

\indent To enable the adjoint method for the gray-box simulation, we 
developed the twin model method to infer the PDE.
Firstly we proposed a parameterized PDE, the ``twin model'',
\begin{equation}
    \dot{ \tilde{\boldsymbol{u}}} + \nabla \cdot
    G(\tilde{\boldsymbol{u}}, \eta)
    = \boldsymbol{q}(\tilde{\boldsymbol{u}},c)
    \label{first equation 2}
\end{equation}
with a flux $G$ parameterized by $\eta \in\mathbb{R}^s$.
Given the same inputs (design variables, initial conditions, and boundary conditions), 
a twin model should yield a space-time solution $\tilde{\boldsymbol{u}}$ 
close to $\boldsymbol{u}$. 
Suppose that the twin model and the 
primal model use the same discretization; we use the following expression to quantify
the solution mismatch:
\begin{equation}
    \mathcal{M} = \frac{1}{T}
    \sum_{i=1}^{N}\sum_{k=1}^{M} \left(\tilde{\boldsymbol{u}}_{ik} 
    - \boldsymbol{u}_{ik}\right)^2 \Delta t_k
    \left| \Delta \mathbf{x}_i \right|
    \label{minimizer twin model discrete}
\end{equation}
where $\Delta t_k$ indicates the $k$th time step, and
$\left| \Delta \mathbf{x}_i \right|$ indicates the lengths (1-D), areas (2-D), or volumes (3-D) 
of the grid.\\

We obtain the optimal $\eta$ by
\begin{equation}
    \eta^* = 
    \arg\min_{\eta} \left\{
    \mathcal{M}
    + \lambda \|\eta\|^p  \right\}
    \label{objective twin model}
\end{equation}
where $\tilde{\boldsymbol{u}}$
is the discretized space-time solution of the twin model.
 $\lambda\|\eta\|^p$
is an $L_p$ norm regularization, and $\lambda>0$.\\
%}}

\indent With $\eta^*$ inferred, we obtain a twin model simulating
\begin{equation}
    \dot{ \tilde{\boldsymbol{u}}} + \nabla \cdot
    G(\tilde{\boldsymbol{u}}, \eta^*)
    = \boldsymbol{q}(\tilde{\boldsymbol{u}},c)
    \label{first equation 2}
\end{equation}
Given a design $c$, we can obtain $\tilde{u}(c)$ and $\tilde{\xi}\equiv \xi(\tilde{u},c)$.
Because the twin model is an open box, we are able to evaluate 
$\frac{d \tilde{\xi}}{d c}$ by the adjoint method, which gives
an estimate for $\frac{d \xi}{d c}$.

\section{Bayesian and Gaussian process optimization}
\label{GPO}
Bayesian optimization works by firstly assuming the objective function to be sampled
from a stochastic process. Let $\xi: \mathcal{C}\rightarrow \mathbb{R}$
be the objective function defined on the search space $\mathcal{C}$, and let
$\Omega$ be a sample space, $\Sigma$ be a $\sigma$-algebra over $\Omega$, and
$\mathbb{P}$ be a probability measure. A stochastic process is a function
\begin{equation}\begin{split}
    \xi \; :\; & \mathcal{C} \times \Omega \rightarrow \mathbb{R}\\
         & (c, \omega) \rightarrow \xi(c,\omega) \equiv \omega(c)
\end{split}\label{stochastic process}
\end{equation}
that for any $c\in \mathcal{C}$, the function $\xi(c, \cdot)$ is a random variable
on $(\Omega, \Sigma, \mathbb{P})$. To simplify the notation, we denote $\xi(c,\omega)$ 
by $\omega(c)$.\\

In Bayesian optimization, $\xi$ is considered as a sample function $\xi(\cdot, \omega)$ from 
the stochastic process. The prior distribution of the objective function is taken into account by a
probability measure $P$ of the sample paths. The prior is update at each search step when when new data 
is sampled, through the computation of the posterior.
Let $\mathcal{F}_n$ be the $\sigma$-algebra generated by $\xi(c_1), \cdots, \xi(c_N)$.
We denote the posterior of $\xi$ to be $P\left[\cdot| \mathcal{F}_n\right]$.
The posterior is used to choose the next sample point.
Let $\mathbb{R}^\mathcal{C}$ be the power set of functions mapping from $\mathcal{C}$ to 
$\mathbb{R}$. 
Bayesian optimization maps $\mathbb{R}^\mathcal{C}$ to a search sequence in $\mathcal{C}$:
\begin{equation}
    \underline{C}(\xi) := \left(c_1(\xi), c_2(\xi), \cdots \right)
\end{equation}
with the Markov property that, $c_{N+1}(\xi)$ depends only on the previous samples
$\xi(c_1),\cdots, \xi(c_N)$.\\

\indent In order to choose $c_{N+1}$ from the previous samples, 
Bayesian optimization introduces an acquisition function, 
$\rho: \mathcal{C}\rightarrow \mathbb{R}^+$, which evaluates
the expected utility of investing the next sample at $c$ given the posterior
$P[\cdot|\mathcal{F}_n]$. The next sample point 
should be chosen to maximize the acquisition function.
\begin{equation}
    c_{N+1} \in \arg\max_{c\in \mathcal{C}} \rho(c)
\end{equation}
There are several 
popular criterions for the acquisition function, such as the expected improvement (EI) 
\cite{review EI} and
the upper confidence bound (UCB) \cite{GP bandit}.
Let $c_N^*$ be a best design of the existing sample points, i.e.
\begin{equation}
    c_N^* \in \arg\max_{c\in \underline{c}_N} \xi(c)
\end{equation}
EI uses the acquisition function:
\begin{equation}
    \rho_{\textrm{EI}}(c) = \mathbb{E}
    \left[ \max\left(\xi(c) - \xi(c^*_N), 0\right) | \mathcal{F}_n \right]\,,
    \label{EI form}
\end{equation}
where the expectation is taken on the posterior.
UCB uses the acquisition function 
\begin{equation}
    \rho_{UCB}(c) = \mathbb{E}[\xi(c)|\mathcal{F}_n] + \kappa \sigma[c| \mathcal{F}_n]\,,
    \label{UCB form}
\end{equation}
where $\sigma$ indicates the standard deviation. $\kappa$ is a tunable parameter
to balance exploitation against exploration.
In this paper, we will focus on both the GP-EI and GP-UCB methods.\\

Gaussian process is a special case of stochastic processes. For any design 
points $c_1, \cdots, c_N$, the random variables defined by
\begin{equation}\begin{split}
   g_i&=\xi(c_i, \cdot): \omega \in \Omega \rightarrow \mathbb{R}\\
   \mathbf{g} &= (g_1, \cdots, g_N)
\end{split}\end{equation}
has a joint Gaussian distribution. GP is solely determined by its mean 
function $m(c)$ and its covariance function $K(c,c^\prime)$. 
\begin{equation}\begin{split}
    m(c) &= \mathbb{E}[\xi(c)]\\
    K(c, c^\prime) &= \mathbb{E}[(\xi(c)-m(c))(\xi(c^\prime)-m(c^\prime))]\,,
\end{split}\end{equation}
for $\forall\, c, c^\prime \in \mathcal{C}$.
Such a GP can be written as
$\xi \sim\mathcal{N}(m(c), K(c,c^\prime))$.
GP allows us to compute conditionals in closed form. 
Assume we have evaluated $\xi$ at $\underline{c}_n=\left\{c_1, \cdots, c_n\right\}$, then 
$P[\xi(c)|\mathcal{F}_n]$ can be constructed from the joint distribution
\begin{equation}
    \begin{pmatrix}
        \xi(c)\\ \xi(\underline{c}_n)
    \end{pmatrix} \sim
    \mathcal{N}\left( 
        \begin{pmatrix}
            m(c)\\
            m(\underline{c}_n)
        \end{pmatrix} \,,\,
        \begin{pmatrix}
            K(c,c) & K(c, \underline{c}_n)\\
            K(\underline{c}_n, c) & K(\underline{c}_n,\underline{c}_n)
        \end{pmatrix}
    \right)
\end{equation}
Conditioned on the samples $\xi(\underline{c}_n)$, the posterior is still a
GP, with the mean and the variance being
\begin{equation}\begin{split}
    \mathbb{E}\left[\left. \xi(c)\right| \xi(\underline{c}_n)\right] & 
    = m(c) + K(c,\underline{c}_n)K(\underline{c}_n,\underline{c}_n)^{-1}
    \left(\xi(\underline{c}_n) - m(\underline{c}_n)\right)\\
    Var\left[\left. \xi(c)\right| \xi(\underline{c}_n) \right] &=
    K(c,c) - K(c,\underline{c}_n) K(\underline{c}_n, \underline{c}_n)^{-1} K(\underline{c}_n,c)
\end{split}\end{equation}
The closed form posterior of GP is very useful computationally, and we will use GP to model the 
objective function in this paper.
Without loss of generality, we model the prior to have zero mean and to be 
stationary, i.e.
$m(c)=0$ , and $K(c,c^\prime) = K(c-c^\prime, 0)$.\\

\indent GP is able to express a rich distribution of functions, depending on the choice of the covariance $K$.
Example covariance functions include the squared exponential kernel and the Matern kernels. 
For a detailed review of such covariances, see \cite{practical Bayesian}.\\

\indent For GP optimization, the 
acquisition function is computed solely from the posterior mean $\mu$ and
variance $\sigma$. Specifically, Eqn.\eqref{EI form} has an analytical form
\begin{equation}
    \rho_{EI}(c) = \sigma(c) \left[ \left( \frac{\mu(c)-f(c_N^*)}{\sigma(c)} \right)
    \Phi\left( \frac{\mu(c)-f(c_N^*)}{\sigma(c)} \right) + 
    \phi\left(\frac{\mu(c)-f(c_N^*)}{\sigma(c)}\right) \right]
\end{equation}
where $\phi$ is the PDF, $\Phi$ is the CDF of standard normal distribution.
\\

\section{GP optimization with the estimated gradient}
\label{GPtwin}
\indent Assume the gray-box simulator evaluates the objective function $\xi$ accurately, and
assume $\xi$ to be differentiable. We consider 
optimizing $\xi$ by sampling the gray-box simulator's $\xi$ and the twin 
model's estimated gradient $\frac{d\tilde{\xi}}{d c}$. 
The type of error introduced by the gray-box model's gradient estimation is treated
as model inadequacy. The notion of model inadequacy is introduced by Kennedy and O'Hagan 
\cite{KennedyOhagan1}.
as the ``difference between the true value and the code output''. In our problem, we assume the 
gray-box simulator provides a deterministic space-time solution given an input $c$.
In other words, the twin model and its adjoint
is solely determined by the space-time solution, thus determined by $c$. 
Review: Kenedy Ohagan bayesian calibration,
DACE model jones,

The deterministic error $\epsilon$ can be treated as the model inadequacy.
The model inadequacy can be modeled as an additive term [jones 1998],
\begin{equation}
    \nabla \tilde{\xi}(c) = \nabla \xi(c) + \mathbf{\epsilon}(c)\,,
    \label{base Bayes model}
\end{equation}
$\epsilon$ is a $d$-dimension vector representing the 
error of estimating $\nabla \xi(c)$ by the twin model.
The components of $\epsilon$ are modelled as a realization of a GP with zero mean:
\begin{equation}
    \begin{pmatrix}
        \epsilon_i(c)\\
        \epsilon_j(c^\prime)
    \end{pmatrix} \sim \mathcal{N}\left( \textbf{0}, G_{ij}(c,c^\prime) \right)\,,
    \label{GP noise}
\end{equation}
for $i,j=1,\cdots, d$. Let $\mathbf{G}$ be the $d$-by-$d$ matrix with the $i,j$th components 
being $G_{ij}$.
For simplicity, we assume no correlation between different components. In other words,
$\mathbf{G}$ is modeled as a diagonal matrix. 
\begin{equation}
    \mathbf{G}(c,c^\prime) = 
    \begin{pmatrix}
        G_1(c,c^\prime) & &\\
        & \ddots & \\
        & & G_d(c,c^\prime)
    \end{pmatrix}\,,
    \label{error independence}
\end{equation}
This assumption reduces the modeling cost of the model discrepancy.\\

Furthurmore, we assume the model inadequacy to be uncorrelated with the gray-box model, i.e.
\begin{equation}\begin{split}
    cov(\nabla \xi(c_1), \epsilon(c_2)) &= 0\\
    cov(\xi(c_1), \epsilon(c_2)) &= 0\,,
    \label{zero corr}
\end{split}\end{equation}
for any $c_1$, $c_2 \in \mathcal{C}$.\\

\indent Let
\begin{equation}
    \xi(\underline{c}_n)=\left(\xi(c_1), \cdots, \xi(c_N)\right)
\end{equation}
be the sampled function value,
\begin{equation}
    \xi_\nabla(\underline{c}_n) = \left(\nabla {\xi}(c_1) ,\cdots, \nabla {\xi}(c_N)\right)
\end{equation}
be the true function gradient, and
\begin{equation}
    \xi_{\tilde{\nabla}}(\underline{c}_n) = \left(\nabla \tilde {\xi}(c_1) ,\cdots, \nabla \tilde {\xi}(c_N)\right)
\end{equation}
be the sampled function gradient.
From Eqn.\eqref{base Bayes model} and \eqref{zero corr}, the joint distribution
of $\xi(c)$ and the sampled data
is given by 

\begin{equation}
    \begin{pmatrix}
        \xi(c) \\ \xi(\underline{c}_n) \\ \xi_{\tilde{\nabla}}(\underline{c}_n)
    \end{pmatrix}
    \sim
    \mathcal{N}\left(
    \begin{pmatrix}
        0\\
        \textbf{0}\\
        \boldsymbol{0}
    \end{pmatrix} ,
    \begin{pmatrix}
        K(c,c) & \mathbf{v} & \mathbf{w}\\
        \mathbf{v}^T & \mathbf{D} & \mathbf{H}\\
        \mathbf{w}^T & \mathbf{H}^T & \mathbf{E} +\bar{\mathbf{G}}
    \end{pmatrix}
    \right)\,,
    \label{joint dis}
\end{equation}
where
\begin{equation}\begin{split}
    \mathbf{v} & = \left(K(c, c_1), \cdots, K(c,c_N) \right)\\
    \mathbf{w} & = \left(\nabla_{c_1} K(c, c_1),\cdots, \nabla_{c_N} K(c, c_N) \right)\\
    \mathbf{D} & = \begin{pmatrix}
        K(c_1, c_1) & \cdots & K(c_1, c_N) \\
        \vdots & \ddots & \vdots \\
        K(c_N, c_1) & \cdots & K(c_N, c_N)
    \end{pmatrix}\\
    \mathbf{H} & = \begin{pmatrix}
        \nabla_{c_1^\prime} K(c_1, c_1^\prime) & \cdots & \nabla_{c_N^\prime} K(c_1, c_N^\prime) \\
        \vdots & \ddots & \vdots\\
        \nabla_{c_1^\prime} K(c_N, c_1^\prime) & \cdots & \nabla_{c_N^\prime} K(c_N, c_N^\prime) 
    \end{pmatrix}\\
    \mathbf{E} & =
    \begin{pmatrix}
        \nabla_{c_1} \nabla_{c_1^\prime} K(c_1, c_1^\prime) & \cdots &
        \nabla_{c_1} \nabla_{c_N^\prime} K(c_1, c_N^\prime)\\
        \vdots & \ddots & \vdots \\
        \nabla_{c_1} \nabla_{c_N^\prime} K(c_N, c_1^\prime) & \cdots &
        \nabla_{c_N} \nabla_{c_N^\prime} K(c_N, c_N^\prime)\\
    \end{pmatrix}\\
    \bar{\mathbf{G}} & =
    \begin{pmatrix}
        \mathbf{G}(c_1, c_1^\prime) & &\\
        & \ddots & \\
        & & \mathbf{G}(c_N, c_N^\prime)
    \end{pmatrix}
\end{split}
\end{equation}
$v$ is a row vector with length $N$. w is a vector with length $Nd$, $\mathbf{D}$ is a 
$N$-by-$N$ matrix, $\mathbf{H}$ is a $N$-by-$Nd$ matrix, and $\mathbf{E},\bar{\mathbf{G}}$ are a $Nd$-by-$Nd$ matrices.
$\underline{c}_n=\underline{c}_n^\prime$. Using Eqn.\eqref{joint dis}, we
can obtain the posterior distribution of $\xi(c)$.
\\

\indent GP depends on the form of the covariance functions $K$ and $G$.
These functions imply assumptions on the continuity and the smoothness for the realizations
of the GP. A popular choice for the covariance functions is the Matern kernel $K_\nu$,
where $\nu$ is a parameter controlling the smoothness of the GP realizations.
For example, a popular choice is
$\nu=\infty$, which corresponds to the square exponential kernel.
The square exponential kernel is
\begin{equation}
    K(c, c^\prime) = \sigma^2 \exp\left(- \frac{\left\|c-c^\prime\right\|^2}{2L^2}\right)\,,
    \label{exp kernel}
\end{equation}
where $\sigma^2$ is the variance, $L$ is the characteristic length scale, $\|\cdot\|$ indicates
the L-2 norm.
The realizations of GP with the square exponential kernel is infinitely differentiable.
Another choice, $\nu=5/2$,  corresponds to the Matern $5/2$ kernel:
\begin{equation}
    K(c, c^\prime) = 
    \sigma^2 \left(1+\frac{\sqrt{5} \|c-c^\prime\|}{L}
    + \frac{5\|c-c^\prime\|^2}{3 L^2}\right) \exp\left(-\frac{\sqrt{5}\|c-c^\prime\|}{L}\right)\,,
    \label{Matern kernel}
\end{equation}
The realizations of the GP with the Matern $5/2$ kernel is once differentiable.
In this paper, we will consider both kernels. \\

\indent The kernels $K$ and $G_1, \cdots, G_d$ depend on their parameters such as the 
variance and the correlation length,
also known as the hyperparameters. Denote these hyperparameters by $\theta$.
These hyperparameters can be selected by the 
maximum marginal likelihood method \cite{jones1998}. 
The method uses a point estimate of the hyperparameters in order to maximize the likelihood of 
observing the sampled data. Given $\theta$, the likelihood of observing 
$\xi(\underline{c}_n)$ 
and $\xi_{\tilde{\nabla}}(\underline{c}_n)$ is :
\begin{equation}\begin{split}
    p\left(\xi(\underline{c}_n), \xi_{\tilde{\nabla}}(\underline{c}_n) |\theta \right) 
    =& \int p(\xi(\underline{c}_n), \xi_{\tilde{\nabla}}(\underline{c}_n) ,
              \xi_{\nabla}(\underline{c}_n) | \theta) d Y_{\nabla}\\
    =& \int p\left(\xi(\underline{c}_n),\xi_{\nabla}(\underline{c}_n) | \theta\right) 
      p\left( \xi_{\tilde{\nabla}}(\underline{c}_n) | \xi(\underline{c}_n),\xi_{\nabla}(\underline{c}_n)
      ;\theta \right)
      d\xi_{\nabla}(\underline{c}_n)\,,
    \label{marginal likelihood}
\end{split}\end{equation}
The marginalization is done over $\xi_{\nabla}(\underline{c}_n)$.
Under the GP assumption, we have
\begin{equation}\begin{split}
    \xi(\underline{c}_n),\xi_{\nabla}(\underline{c}_n) \Big| \theta \sim \mathcal{N}\left(
        \begin{pmatrix}
            \mathbf{0}\\
            \mathbf{0}
        \end{pmatrix},
        \begin{pmatrix}
            \mathbf{D} & \mathbf{H} \\
            \mathbf{H}^T & \mathbf{E}
        \end{pmatrix}
    \right)
\end{split}
\label{first term}
\end{equation}
and
\begin{equation}
    \xi_{\tilde{\nabla}}(\underline{c}_n) | \xi(\underline{c}_n),
    \xi_\nabla(\underline{c}_n); \theta
    \sim 
    \mathcal{N} 
    \left(
            \xi_{\nabla}(\underline{c}_n)
        ,
            \bar{\mathbf{G}}
    \right)
\label{second term}
\end{equation}
Eqn.\eqref{first term} and \eqref{second term} yields a closed form of the log marginal likelihood:
\begin{equation}\begin{split}
    & \log p(\xi(\underline{c}_n), \xi_{\tilde{\nabla}}(\underline{c}_n) | \theta) =\\
    & - \frac{1}{2} 
    \begin{pmatrix}
        \xi(\underline{c}_n)\\ \xi_{\tilde{\nabla}}(\underline{c}_n)
    \end{pmatrix}^T
    \begin{pmatrix}
        \mathbf{D}& \mathbf{H}\\
        \mathbf{H}^T & \mathbf{E}+\bar{\mathbf{G}}
    \end{pmatrix}^{-1}
    \begin{pmatrix}
        \xi(\underline{c}_n)\\ \xi_{\tilde{\nabla}}(\underline{c}_n)
    \end{pmatrix}
    - \frac{1}{2}
    \log \left(\textrm{det}
        \begin{pmatrix}
            \mathbf{D}& \mathbf{H}\\
            \mathbf{H}^T & \mathbf{E}+\bar{\mathbf{G}}
        \end{pmatrix}       
    \right)
    - \frac{N(d+1)}{2} \log(2\pi)
\end{split}\label{likelihood eqn}
\end{equation}
where the matrix inversion can be performed by Cholesky decomposition.
The closed form of the marginal likelihood enables efficient optimization of the hyperparameters.
In this paper, we will use this method to select the hyperparameters.
Using the formulation discussed above, we can evaluate the posterior of $\xi(c)$.
The posterior is then used to 
evaluate the acquisition function, and to obtain the next optimal design point.
In addition, we include a stopping rule to our optimization algorithm. If the maximum expected
improvement is less than a threshold value of the current best function value, we stop \cite{jones1998}.

\section{Convergence properties}
\label{GPconv}
In this section, we give some results on the convergence properties 
of the GP optimization framework discussed in the previous section.
Researchers have explored the convergence properties of GP optimization using only the 
objective function sampling.
M. Locatelli \cite{Locatelli} proves that the search sequence produced by the GP-EI
method is dense in $\mathcal{C}$ for $n\rightarrow \infty$ for the 1-D optimization problem
$c^*=\max_{c\in[0,1]} \xi(c)$, if $\xi$ is a realization of the Wienner process.
E. Vazquez \cite{convergen EI} generalizes the results, by showing that the sequence is still dense
for higher dimensional space and for a general class of stochastic processes.
Further, A. Bull \cite{converge Bull} provides a convergence rate at
$\mathcal{O}(n^{-\nu/d})$ for the GP-EI method,
where $\nu>0$ is a constant parameter controlling the smoothness
of the RKHS. Similar results are also given for GP-UCB. N. Srinivas \cite{GP bandit} bounds the convergence rate
from above at $n^{- \frac{\nu}{2\nu+d(d+1)}}$ for GP-UCB, and also establish the 
relationship between the convergence rate and the information gain due to sampling.
In this section, we will analyze the convergence properties of GP-EI when 
the estimated gradient sampling is also available.
Under the assumptions in section\ref{GPtwin}, our main result is: 1.
GP-EI optimization using the estimated gradient produces a dense search sequence
when $n\rightarrow \infty$. (2. GP-UCB optimization using the estimated gradient
converges no slower than without using the estimated gradient.)\\
%    \item GP-UCB optimization with the estimated gradient sampling converges, with a convergence
%    rate no slower than without the estimated gradient sampling.

We first reiterate some notations and assumptions. 
Without loss of generality \cite{w.o.l.g}
, we assume the objective function to be a realization of a
Gaussian process with zero mean. Specifically, the objective function
belongs to the reproducing kernel Hilbert space $\mathcal{H}_K$ generated by the 
kernel $K: \mathcal{C}\times \mathcal{C} \rightarrow \mathbb{R}^+\bigcup 0$.
Assume $K$ to be differentiable, then the gradient of all functions in $\mathcal{H}_K$
forms a reproducing kernel Hilbert space $\mathcal{H}_{K_\nabla}$ with
the kernel $K_\nabla(c_1,c_2) \equiv \nabla_{c_1}\nabla_{c_2}K(c_1, c_2)$ 
for all $c_1,c_2\in \mathcal{C}$ \cite{derivative RKHS}.
The gradient estimation error is defined as in Eqn.\eqref{GP noise} and 
Eqn.\eqref{error independence}. In other words, the $i$th component ($i=1,\cdots, d$)
of $\epsilon$ belongs to the reproducing kernel Hibert space $\mathcal{H}_{G}^i$ generated by the 
kernel $G_i: \mathcal{C}\times \mathcal{C} \rightarrow \mathbb{R}^+\bigcup 0$, and the components are mutually independent. 
Denote $\mathcal{H}_G \equiv \mathcal{H}_{G}^1 \otimes \cdots \otimes \mathcal{H}_G^d$.\\

Denote the stochastic dependence of $\xi$ by $\omega_\xi$, and the stochastic dependence of 
$\epsilon_i$ by $\omega_\epsilon^i$ for $i=1,\cdots, d$.
Let $(\Omega_\xi, \Sigma_\xi, \mathbb{P}_\xi)$ be the probability space for $\omega_\xi$, and let
 $(\Omega_\epsilon^i, \Sigma_\epsilon^i, \mathbb{P}_\epsilon^i)$ be the probability space for $\omega_\epsilon^i$ for
$i=1,\cdots,d$.
We have
\begin{equation}\begin{split}
    \xi\; : \; & \mathcal{C} \times \Omega_\xi \rightarrow \mathbb{R}\\
               & (c, \omega_\xi) \rightarrow \xi(c; \omega_\xi)
\end{split}\end{equation}
\begin{equation}\left.\begin{split}
    \epsilon \; :\; & \mathcal{C} \times \Omega_\epsilon^i \rightarrow \mathbb{R}^d \\
                    & (c, \omega_\epsilon^i) \rightarrow \epsilon(c; \omega_\epsilon^i)
\end{split}\right.
\quad \textrm{for}\; i=1,\cdots, d
\end{equation}
Let $\omega_\epsilon=(\omega_\epsilon^1,\cdots, \omega_\epsilon^d)$ and $\Omega_\epsilon =\Omega_\epsilon^1\otimes \cdots
\otimes \Omega_\epsilon^d$.
The true objective function is modelled as $\xi(c; \omega_\xi^*)$ for $\omega_\xi^*\in \Omega_\xi$, and
the true estimated gradient error is modelled as $\epsilon(c;\omega_\epsilon^*)$ for $\omega_\epsilon^* \in \Omega_\epsilon$.
In other words $\xi(c;\omega_\xi^*) = \xi(c)$ and $\epsilon(c;\omega_\epsilon^*)=\epsilon(c)$.
Conditioned on the existing samples, GP optimization generates the next search point deterministically.
We denote the search sequence
$(c_n)_{n\ge 1}$ generated from the optimization strategy by $\underline{c}_n$.
We also denote the function value, the estimated gradient sample, the true gradient, 
and the gradient estimation error on $\underline{c}_n$ by
$\xi(\underline{c}_n)$, $\xi_{\tilde{\nabla}}(\underline{c}_n)$, $\xi_\nabla(\underline{c}_n)$,
and $\epsilon(\underline{c}_n)$.
%Therefore,
%the search sequence $\underline{c}_n$ can be seen as a mapping $\underline{C}$ from the 
%power set $\mathbb{R}^\mathcal{C} \otimes (\mathbb{R}^{d})^\mathcal{C}$ to the power set
%$\mathcal{C}^\mathbb{N}$,
%where $\xi \in \mathbb{R}^\mathcal{C}$ and $\epsilon \in (\mathbb{R}^{d})^\mathcal{C}$.
Given the initial design $c_0$, the search sequence can be seen as a mapping
\begin{equation}
     \underline{C}(\omega_\xi, \omega_\epsilon) = \left(C_1(\omega_\xi, \omega_\epsilon),
     C_2(\omega_\xi, \omega_\epsilon), \cdots\right)
\end{equation}
The search strategy $\underline{C}$ generates a random search sequence $C_1, C_2, \cdots$ 
in $\mathcal{C}$,
with the property that $C_{n+1}$ is $\mathcal{F}_n$-measurable, where 
$\mathcal{F}_n$ is the $\sigma$-algebra generated by
$\xi(\underline{c}_n)$ and $\xi_{\tilde{\nabla}}(\underline{c}_n)$.\\

At the $n$-th search step in the optimization algorithm, 
the posterior mean and variance of $\xi(c)$ conditioned on 
$\xi(\underline{c}_n)$ and $\xi_{\tilde{\nabla}}(\underline{c}_n)$ are written as
\begin{equation} 
    \hat{\xi}_n(c; \underline{c}_n) 
    = \mathbb{E}_{\omega_\xi, \omega_\epsilon}\left[ \xi(c, \omega_\xi) \Big|
                          \underline{c}_n, \xi(\underline{c}_n), 
                          \xi_{\tilde{\nabla}}(\underline{c}_n) \right]\,,
    \label{cond expectation}
\end{equation}
and
\begin{equation}
    \sigma_n^2(c;\underline{c}_n) = \mathbb{E}_{\omega_\xi, \omega_\epsilon}\left[\left(
        \xi(c) - \hat{\xi}_n(c)\right)^2 \Big| \underline{c}_n,
        \xi(\underline{c}_n), \xi_{\tilde{\nabla}}(\underline{c}_n)
        \right]\,.
\end{equation}
Notice $\sigma_n^2(c;\underline{c}_n)$ only depends on $\underline{c}_n$, and is independent 
of $\xi(\underline{c}_n), \xi_{\tilde{\nabla}}(\underline{c}_n)$ because of the GP 
assumption.



\subsection{GP-EI generates a dense search sequence}
In this section, we proves that the GP-EI algorithm generates a dense search sequence 
in the design space. The proof is similar to the proof given by E. Vazquez \cite{convergen EI}.
The contribution of this paper is to extend the proof to the case where
the estimated gradient sample is also available.\\

Firstly, we have (Chapter 1, Theorem 4.1, \cite{RKHS book}).\\
{\textbf{Lemma 1}}
{\emph
    Let $K_1, K_2$ be the reproducing kernels of functions on $\mathcal{C}$ with
    norms $\|\cdot\|_{\mathcal{H}_1}$ and $\|\cdot\|_{\mathcal{H}_2}$ respectively. Then $K=K_1+K_2$ is
    the reproducing kernel of the space 
    $$
        \mathcal{H} = \mathcal{H}_1\oplus \mathcal{H}_2 =
        \left\{ 
            f = f_1 + f_2, \; f_1\in \mathcal{H}_1, \; f_2\in\mathcal{H}_2
        \right\}
    $$
    with norm $\|\cdot\|_\mathcal{H}$ defined by 
    $$
        \forall f\in \mathcal{H} \quad \|f\|_{\mathcal{H}}^2 
        = \min_{
                f=f_1+f_2,\; 
                f_1\in \mathcal{H}_1, f_2\in \mathcal{H}_2
        }
        \left(\|f_1\|_{\mathcal{H}_1^2} + \|f_2\|_{\mathcal{H}_2}^2\right)
    $$
}

Using Lemma 1, We prove the following inequality:\\
{\textbf{Theorem 1}}
$$
  \left|\xi(c,\omega_\xi) - \hat{\xi}(c;\underline{c}_n) \right|^2
  \le \left(\left(1+\frac{4d}{3}\right)  \left\| \xi(c; \omega_\xi) \right\|_{\mathcal{H}_K}
      + \frac{4d}{3} \left\| \nabla_c \xi(c;\omega_\xi)
        \right\|_{\mathcal{H}_{K_\nabla}}
      +\frac{4}{3}\sum_{i=1}^d  
       \left\| \epsilon_i(c;\omega_\epsilon^i) \right\|_{\mathcal{H}_G^i} \right)
      \sigma^2(c; \underline{c}_n)
$$
\emph{Proof of Theorem 1}
Define a vector 
\begin{equation}
    u = \left(u_1, \cdots, u_d\right)^T\,,
\end{equation}
where $u_1\in [0,1], \cdots, u_d\in[0,1]$.
We write the domain for $u$ as $\mathcal{U}$.
Define an auxiliary function
\begin{equation}
    F(c,u;\omega_\xi, \omega_\epsilon) = \left(1-\sum_{i=1}^d u_i\right) \xi(c, \omega_\xi) + u^T
    \left[\nabla_c\xi(c,\omega_\xi) + \epsilon(c;\omega_\epsilon) \right]\,,
\end{equation}
$u_1, \cdots, u_d$ can be viewed as realizations of a reproducing kernel
Hilbert space $\mathcal{H}_u$ on $\mathcal{U} = [0,1]$. 
For example, $\mathcal{H}_u$ can be the Sobolev space
$W^{1,2}$ defined on $\mathcal{U}$, equipped with the inner product
\begin{equation}
    \left< \phi, \psi \right>  = \int_{\mathcal{U}} \phi\psi + (\nabla\phi)^T (\nabla \psi)  \; du
\end{equation}
and the reproducing kernel
\begin{equation}
    K_u(\phi, \psi) = \frac{1}{2} \exp\left(- \left|\phi-\psi \right|\right)
\end{equation}
Given $\omega_\xi$ and $\omega_\epsilon$,
$F(c,u;\omega_\xi,\omega_\epsilon)$ can be viewed as a realization from a reproducing kernel Hilbert space,
$\mathcal{H}_F$, defined on $\mathcal{C}\times \mathcal{U}$. Let the kernel function of
$\mathcal{H}_F$ be
\begin{equation}\begin{split}
    K_F\;:\;  &  \mathcal{C}\times \mathcal{U}, \mathcal{C}\times \mathcal{U} \rightarrow \mathbb{R}\\
              & (c_1, u_1), (c_2, u_2)\rightarrow K_F((c_1, u_1), (c_2, u_2))
\end{split}\end{equation}
Notice 
\begin{equation}
    F(c, \mathbf{0};\omega_\xi, \omega_\epsilon) = \xi(c, \omega_\xi)
\end{equation}
is the objective function, and
\begin{equation}
    \left( F(c, e_1;\omega_\xi,\omega_\epsilon), \cdots, F(c, e_d;\omega_\xi, \omega_\epsilon)\right) 
    = \nabla_c\xi(c;\omega_\xi) + \epsilon(c;\omega_\epsilon)
\end{equation}
is the estimated gradient,
where $e_i, i=1,\cdots, d$ indicates the $i$th unit Cartesian basis vector in $\mathbb{R}^d$.
Conditioned on the sampling $\xi(\underline{c}_n)$ and $\xi_{\tilde{\nabla}}(\underline{c}_n)$,
we can bound the error of the estimation of $F(c,\mathbf{0};\omega_\xi,\omega_\epsilon)$ by
the Cauchy-Scharz inequality \cite{RKHS book} in $\mathcal{H}_F$,
\begin{equation}
    \left|F(c,\mathbf{0};\omega_\xi,\omega_\epsilon) - \hat{F}(c,\mathbf{0};\underline{c}_n)\right| =
    \left|\xi(c;\omega_\xi) - \hat{\xi}_n(c;\underline{c}_n)\right| \le
    \sigma(c;\underline{c}_n) \|F\|_{\mathcal{H}_F}
\end{equation}
Besides,
\begin{equation}\begin{split}
    \|F\|_{\mathcal{H}_F} &= \left\|
        \left(1-\sum_{i=1}^d u_i\right) \xi(c; \omega_\xi) + u^T
        \left[\nabla_c\xi(c;\omega_\xi) + \epsilon(c;\omega_\epsilon) \right]
    \right\|_{\mathcal{H}_F}\\
    &\le \left\| \xi(c; \omega_\xi) \right\|_{\mathcal{H}_K}
      + \left(\sum_{i=d}^d \|u_i\|_{\mathcal{H}_u} \right)
        \left\|\xi(c;\omega_\xi)\right\|_{\mathcal{H}_K}
      + \left(\sum_{i=d}^d \|u_i\|_{\mathcal{H}_u} \right)
        \left\|\nabla_c \xi(c;\omega_\xi)\right\|_{\mathcal{H}_{K_\nabla}}
      + \sum_{i=1}^d \left\| u_i \epsilon_i(c;\omega_\epsilon^i)
        \right\|_{\mathcal{H}_u\bigotimes \mathcal{H}_G^i}\\
    &= \left\| \xi(c, \omega) \right\|_{\mathcal{H}_K}
      + \frac{4d}{3} \left\|\xi(c,\omega)\right\|_{\mathcal{H}_K}
      + \frac{4d}{3}\left\| \nabla_c \xi(c;\omega_\xi)
        \right\|_{\mathcal{H}_{K_\nabla}}
      + \frac{4}{3}\sum_{i=1}^d \left\| \epsilon_i(c;\omega_\epsilon^i) \right\|_{\mathcal{H}_G^i}
\end{split}\label{eqn: CS ineq}
\end{equation}
The second line of Eqn.\eqref{eqn: CS ineq} uses Lemma 1, and the third line uses
the fact that $\mathcal{H}_u$ can be chosen as $W^{1,2}$. Thus we proved Theorem 1.\\

Using theorem 1, we can prove\\
\textbf{Theorem 2}
Let $(\underline{c}_n)_{n\ge 1}$ and $(\underline{a}_n)_{n\ge 1}$ be two sequences in $\mathcal{C}$.
Assume that the sequence $(a_n)$ is convergent, and denote by $a^*$ its limit. Then each of the
following conditions implies the next one:
\begin{enumerate}
    \item $a^*$ is an adherent point of $\underline{c}_n$ (there exists a subsequence in 
          $\underline{c}_n$ that converges to $a^*$) ,
    \item $\sigma^2(a_n;\underline{c}_n)\rightarrow 0$  when $n\rightarrow \infty$,
    \item $\hat{\xi}(a_n; \underline{c}_n) \rightarrow \xi(a^*, \omega)$ when $n\rightarrow \infty$,
          for all $\xi \in \mathcal{H}_K\,,\, \epsilon \in \mathcal{H}_G$.
\end{enumerate}
The proof of theorem 2 is the same as the proposition 8 in \cite{convergen EI}, 
except that the posterior is
defined using the estimated gradient sampling. 
Theorem 2 can be proven by replacing the Cauchy-Schwarz
inequality used in \cite{convergen EI} by Theorem 1.\\

For the stationary process $\xi\in\mathcal{H}_K$ defined on the domain $\mathbb{R}^d$, define 
$\Phi(c) \equiv K(c, 0)$.
Let the Fourier transform of $\Phi(c)$ be $\hat{\Phi}(\eta)$. 
We assume $\hat{\Phi}$ satisfies the property:\\
\textbf{Assumption}
There exist $C\ge 0$ and $k \in\mathbb{N}^+$, such that
$(1+|\eta|^2)^k \big|\hat{\Phi}(\eta)\big|\ge C$ for all $\eta\in \mathbb{R}^d$.\\

For any $\xi\in \mathcal{H}_K$ and its Fourier transform $\hat{\xi}$, 
we have \cite{converge Bull}
\begin{equation}\begin{split}
    \big\|\xi\big\|_{W^{k,2}} = 
    \int (1+|\eta|^2)^k \big| \hat{\xi} \big|^2 \; d\eta 
    \ge C \int \big|\hat{\Phi}(\eta)\big|^{-1} \big|\hat{\xi}(\eta)\big|^2 \; d\eta
    = C \sqrt{(2\pi)^d} \big\|\xi\big\|_{\mathcal{H}_K}\,,
\end{split}
\label{space equal}
\end{equation}
where $W^{k,2}$ is the Sobolev space whose weak derivatives up to order $k$ 
have a finite $L^2$ norm. Therefore, $W^{k,2}\subseteq \mathcal{H}_K$.
The result can be extended to $\xi \in \mathcal{H}_K(\mathcal{C})$ defined on the domain 
$\mathcal{C}\in \mathbb{R}^d$, because $\mathcal{H}_K(\mathcal{C})$ embeds isometrically
into $\mathcal{H}_K(\mathbb{R}^d)$ \cite{RKHS aronszajn}. Besides, we have that
$C_c^\infty$ is dense in $W^{k,2}$ (Chapter 2, Lemma 5.1 \cite{Hilbert space Showalter Book}),
where $C_c^\infty$ is the $C^\infty$ functions with compact support on $\mathcal{C}$.
As a consequence, $\mathcal{C}^\infty_c\subseteq \mathcal{H}_K$ \cite{convergen EI}.\\

It can be shown that (3) results in (1) if only $\xi(\underline{c}_n)$
are available \cite{convergen EI}. Indeed, if (1) is false, then there exist a neighborhood
$U$ of $a^*$ that does not intersect $\underline{c}_n$. There exist $\xi\in \mathcal{H}_K$
that is compactly supported in $U$. It follows that $\hat{\xi}(a^*; \underline{c}_n) = 0$ whereas
$\xi(a^*) \neq 0$.
The argument can be extended to the case when both $\xi(\underline{c}_n)$ and 
$\xi_{\tilde{\nabla}}(\underline{c}_n)$ are
available: there exist $\xi = 0$ and $\epsilon = \textbf{0}$, such that 
$\hat{\xi}(a^*; \underline{c}_n) = 0$ whereas $\xi(a^*) \neq 0$.
Thus (1), (2), and (3) in Theorem 2 are equivalent.\\

Finally, we have the theorem:\\
\textbf{Theorem 3} (E. Vazquez, Theorem 5 \cite{convergen EI})$\quad$ If the 3 conditions in Theorem 2 
are equivalent, then for all $c_{init}\in \mathcal{C}$ and all $\omega \in \mathcal{H}$, the sequence 
$\underline{c}_n$ generated by the GP-EI algorithm is dense in $\mathcal{C}$.\\

\section{Numerical examples}
In this section, we demonstrate the optimization method on several numerical examples. The first example
is to minimize $n$-dimensional Rosenbrock functions, the second example is to optimize the control 
of a 1-D porous media flow problem, 
and the third example is to optimize the boundary geometry of a 2-D return
bend channel.

\subsection{Optimize generalized Rosenbrock functions}
The inclusion of estimated gradient in addition to the function evaluation  
can potentially accerlerate the Bayesian optimization. 
In this numerical example, we apply the framework proposed in section \ref{GPtwin} to
the minimization of $d$-D generalized Rosenbrock functions.
Generalized Rosenbrock functions are non-convex test functions to 
check optimization algorithms' performance \cite{Rosenbrock}, and it has been used to test
Bayesian optimization algorithms \cite{RemiRosenbrock}.
The $n$-D Rosenbrock function is defined by
\begin{equation}
    \xi(\mathbf{x}) = \sum_{i=1}^{N-1} b (x_{i+1} - x_i^2)^2 + (1- x_i)^2\,,
\end{equation}
where $\mathbf{x} = (x_1, \cdots, x_N) \in \mathbb{R}^N$. By convention we set $a=1, b=100$.
For all $d>2$,
$\xi(\mathbf{x})$ has the global minimum $0$ at $\mathbf{x}=(1, 1, \cdots, 1)$.
We apply the optimization framework to minimize the $d$-D Rosenbrock functions. Because no PDE 
simulation is involved in the evaluation of $\xi(\mathbf{x})$ thus no twin model is involved,
we simulate the gradient estimation by 
Eqn.\eqref{base Bayes model}. The covariance of $\epsilon$ is sampled by Eqn.\eqref{GP noise} and 
\eqref{error independence}, while $G_i, i=1,\cdots,d$ are assumed to be i.i.d. Gaussian processes 
with Matern 5/2 kernel \eqref{Matern kernel} with correlation length $L=1$.\\

Firstly, we minimize the 2-D Rosenbrock function in $\mathbf{x}\in [-3,3]^d$, while
using various $\sigma^2$. Thus we evaluate 
how the optimization performance is affected by the choise of $\sigma^2$.
We set $\sigma$ to be (1) $\sigma=10$, (2) $\sigma=100$, and (3) $\sigma=\infty$
respectively. (1) corresponds to a noisy gradient, (2) corresponds to more noisy gradient,
and (3) corresponds to no gradient evaluation. Because of the randomness involved in the 
optimization of the acquisition function, we optimize $\xi$ for 100 times, and compute
the averaged, current best $\xi^*_n$ after $n$ function evaluations.
The result is shown in Fig.\ref{fig:compare_ratio}. The Bayesian optimization 
performance improves as the noise in the gradient reduces.\\
\begin{figure}\begin{center}
    \includegraphics[width=13cm]{compare_ratio.png}
    \caption{The current best $\xi^*_n$ after $n$ function evaluations. The minimization is 
    performed under 3 scenarios: noisy gradient ($\sigma=10$), more noisy gradient ($\sigma=100$), 
    and no gradient ($\sigma=\infty$). The minimization is performed 100 times. This plot
    shows the averaged 
    $\xi^*$ and the one-sigma error bar for the 100 minimizations.}
    \label{fig:compare_ratio}
\end{center}
\end{figure}

Secondly, we minimize the $d$-D Rosenbrock functions with the 3 choices of $\sigma$, for 
$d= 2,3,4,5,6$. We report the average number of function evaluations required in order to 
obtain $\xi^*_n<5$ for 100 minimizations for each $d$. 
As the dimension $d$ increases, the inclusion of the noisy gradient
reduces the number of function evaluations.
\begin{figure}\begin{center}
    \includegraphics[width=13cm]{compare_bar_2.png}
    \caption{The number of function evaluations required to reduce $\xi^*_n$ to a
             target threshold ($\xi^*_n<5$). The minimization is performed under the 3
             scenarios $\sigma=10, 20,$ and $\infty$.}
    \label{fig:compare_bar}
\end{center}
\end{figure}

In conclusion, the inclusion of noisy gradient data in the Bayesian optimization reduces
the number of function evaluations required to achieve the desired accuracy
for the $d$-D Rosenbrock functions, especially for larger $d$.\\

\subsection{Optimize a 1-D porous media flow}
Consider a PDE 
\begin{equation}\begin{split}
    &\frac{\partial u}{\partial t} + \frac{\partial F(u)}{\partial x} = c(t,x)\,
    \quad x\in[0,1]\; t\in[0,1]\\
    &u(t=0,x) =u_0(x),\; u(t,x=0)=u(t,x=1)
    \label{BL eqn}
\end{split}\end{equation}
that models a 1-D, two-phase, porous media flow with periodic boundary condition. 
$u$ denotes the saturation of
one phase, and $1-u$ denotes the saturation of the other phase.
$0\le u\le 1$. $c(t,x)$ is a space-time dependent control which models the phase-1
injection rate.
The flux function $F(u)$ depends on the properties of the porous media and the fluids.
We optimize $c(t,x)$ 
such that $u(t=1,x)$ is close to a target function $u^*(x)$. \\

We parameterize the space-time-dependent control
$c(t,x)$ by $\mathbf{c}=(c_{ij})_{i=1,\cdots,m\;,j=1,\cdots,n}$:
\begin{equation}\begin{split}
    &c(t,x) = \sum_{i=1}^m \sum_{j=n}^s c_{ij}\exp\left(-(t-t_i)^2/l_t^2\right)
    \exp\left(-(x-x_j)^2/l_x^2\right)\\
    &t\in[0,1], x\in[0,1]
\end{split}\end{equation}
The objective function is given by
\begin{equation}
    \xi(c_{ij}) = \int_{x=0}^1 \left| u(t=1,x) -  u^*(x)\right|^2 + \lambda\sum_{ij} c_{ij}^2\,,
    \label{BL obj fun}
\end{equation}
We set Eqn.\eqref{BL eqn} to be Buckley-Leverett equation which models the flow driven by 
capillary pressure and Darcy's law \cite{Buckley Leverett}, whose flux is
\begin{equation}
    F(u) = \frac{u^2}{1+A(1-u)^2}\,.
    \label{BL flux}
\end{equation}
In the optimization of $\xi$ we assume $F$ is unknown, thus Eqn.\eqref{BL eqn} is a gray-box model.
We have demonstrated that twin model can give accurate gradient estimation for $\xi$ \cite{twin model}.\\

Specifically, we set $m=s=5$, $l_t=l_x=0.25$, $A=2$, $u^*(x) = 0.5$. $\lambda=0.01$.
$t_i$'s and $x_j$'s are equally spaced in [0,1].
We minimize Eqn.\eqref{BL obj fun} by either using or not using the estimated gradient provided
by the twin model, and compare their current best $\xi^*$'s after the same number of iterations.
Fig.\ref{fig:finalutx} and \ref{fig:opt_BL} show that using the twin model improves the optimization
result when the number of gray-box simulations is limited.
\\

\begin{figure}[H]\begin{center}
    \includegraphics[width=7cm]{opt_source.png}
    \includegraphics[width=7cm]{LD_utx.png}
    \caption{Left: the control $c(t,x)$ optimized using the twin model after
    100 gray-box simulations. Right: $u(t,x)$ obtained by the gray-box simulation using the 
    $c(t,x)$ on the left.}
    \label{fig:opt_source}
\end{center}
\end{figure}


\begin{figure}[H]\begin{center}
    \includegraphics[width=10cm]{finalutx.png}
    \caption{$u(t=1,x)$. The dashed cyan line indicates the solution obtained by setting
    $\mathbf{c}=\mathbf{0}$ (initial guess). The red line is the optimized solution 
    after 100 gray-box simulations without using the twin model. The green line is the optimized
    solution after 100 gray-box simulations using the twin model. The dashed black line is
    the target solution $u^*(x)$.}
    \label{fig:finalutx}
\end{center}
\end{figure}

\begin{figure}[H]\begin{center}
    \includegraphics[width=10cm]{opt_BL.png}
    \caption{The current best $\xi^*_n$ after $n$ gray-box simulations.
    The blue line is obtained by using the twin model, and the green line is obtained by using
    the twin model.} 
    \label{fig:opt_BL}
\end{center}
\end{figure}

\subsection{Optimize a Navier-Stokes flow}
We consider a steady-state compressible internal flow 
in a 2-D return bend channel governed by Navier-Stokes 
equations. 
\begin{equation}
    \frac{\partial}{\partial t}
    \begin{pmatrix}
        \rho \\ \rho u \\ \rho v\\ \rho E
    \end{pmatrix}
    + \frac{\partial}{\partial x} 
    \begin{pmatrix}
        \rho u\\
        \rho u^2 + p - \sigma_{xx}\\
        \rho uv - \sigma_{xy}\\
        u(E\rho+p) - \sigma_{xx} u - \sigma_{xy} v
    \end{pmatrix}
    + \frac{\partial}{\partial y}
    \begin{pmatrix}
        \rho v\\
        \rho uv-\sigma_{xy}\\
        \rho v^2+p-\sigma_{yy}\\
        v(E\rho+p) - \sigma_{xy} u -\sigma_{yy}v
    \end{pmatrix} 
    = \boldsymbol{0}
    \label{NSeqn}
\end{equation}
where
\begin{equation}\begin{split}
    \sigma_{xx} &= \mu \left(2 \frac{\partial u}{\partial x} - \frac{2}{3} \left(\frac{\partial u}{\partial x} 
    + \frac{\partial v}{\partial y}\right)\right)\\
    \sigma_{yy} &= \mu \left(2 \frac{\partial v}{\partial y} - \frac{2}{3} \left(\frac{\partial u}{\partial x} 
    + \frac{\partial v}{\partial y}\right)\right)\\
    \sigma_{xy}&=\mu\left(\frac{\partial u}{\partial y} + \frac{\partial v}{\partial x}\right)
\end{split}\end{equation}

Given the boundary geometry, the flow simulation 
provides the steady-state solution of the density $\rho$,
the velocity $(u,v)$, and the energy $E$.
The state equation $p=p(\rho, U)$ is assumed unknown, so the flow simulation is 
a gray-box model.
The details of gray-box simulation and the twin model setup are described in \cite{twin model}.
\begin{figure}[H]\begin{center}
    \includegraphics[width=12cm]{../numpad/test/ns_cases_spline/results/mesh/spline_2.png}
    \caption{The return bend geometry and the mesh for simulation. The innder and outer 
    boundaries at the bending region are parameterized by the $8$ red control points using B-Spline.
    The area of the bending section (the curved quadrilateral whose vertices are A,B,C,D) 
    is constrained to be a constant in the optimization.} 
    \label{fig:opt_BL}
\end{center}
\end{figure}

Let the area of the bending section of the return bend to be $V$.
We maximize the cross-sectional mass flow rate 
\begin{equation}
    \xi = - \int_{\textrm{outlet}} \rho_\infty u_\infty \big|_{\textrm{outlet}} \; dy=
    \int_{\textrm{inlet}} \rho_\infty u_\infty\big|_{\textrm{inlet}} \; dy
    \label{mass flux}
\end{equation}
constrained by $V$ being a constant,
by tuning the B-Spline control points. To avoid ill-posedness of the mesh, we
constrain the coordinates of the control points in the boxes shown in Fig.\ref{fig:opt_Ubend}.\\


The twin model has been demonstrated to provide accurate gradient estimation of $\xi$
with respect to the coordinates of the control points \cite{twin model}. 
Using the estimated gradient, we obtained faster objective function improvement, as shown 
in Fig.\ref{fig:opt_NS} with a limited number of gray-box simulations. 
\begin{figure}[H]\begin{center}
    \includegraphics[width=12cm]{opt_Ubend.png}
    \caption{The initial guess and the optimized control points coordinates. The blue points
     are the initial guesses, and the red are the optimized. The blue boxes indicate the
     constraints used to bound the control points to avoid ill-posed meshing.} 
    \label{fig:opt_Ubend}
\end{center}
\end{figure}

\begin{figure}[H]\begin{center}
    \includegraphics[width=12cm]{opt_NS.png}
    \caption{The current best $\xi^*$ after $n$ number of gray-box simulations. } 
    \label{fig:opt_NS}
\end{center}
\end{figure}

\section{Conclusion}
This paper presented a Bayesian optimization framework for optimization problems constrained
by gray-box PDE simulations that are expensive to evaluate.
The gray-box simulations provides the space-time or spatial solution, which permit the twin model
inference.
The twin model estimates the adjoint of the gray-box simulation and the gradient of the
objective function.
The estimated gradient, combined with the gray-box objective function evaluation,
is used in a Bayesian optimization framework.
We showed that the optimization framework generates a dense search sequence under some
assumptions of the estimated gradient.
The proposed optimization framework is then compared with the Bayesian optimization
using only the gray-box simulations.
We tested the optimization of the $n$-D Rosenbrock functions, the 1-D
Buckley-Leverett equation, and a 2-D return bend problem.
Using the same number of gray-box simulations, the proposed framework 
outperforms the framework without using the twin model in all these test cases.

%\newpage
%Convergence of GP-UCB with only function value sampling.
%$$
%\mathbb{P}\left[R_T/T \le \sqrt{C_\delta  \gamma_T}\right] \ge 1-\delta
%$$
%where
%$$
%\gamma_T \equiv \max_{|A|=T} I(y_A; f)
%$$
%The convergence rate depends on the mutual information between the sample and the objective function,
%which is bounded by the entropy of the objective function.\\
%
%Periodic GP on a unit circle:
%\begin{figure}\begin{center}
%    \includegraphics[width=13cm]{circle_var_sketch.png}\\
%    \includegraphics[width=13cm]{var_ratio_correct.png}
%\end{center}\end{figure}
%
%Multivariate Gaussian on a unit length:
%\begin{figure}\begin{center}
%    \includegraphics[width=13cm]{shannon_entropy_GP.png}
%\end{center}\end{figure}
%
%Prove:
%$$
%    H(f| \left\{\tilde{\nabla f_n}\right\}, \left\{f_n\right\}) > H(f| \tilde{\nabla f},\left\{f_n\right\})
%    > C\cdot H(f|\left\{f_n\right\})
%$$
%
%
%\section{Model validation}
%Cross validation, Q-Q plot
%Jones Eqn 9, ref [29]
%If no satisfactory result, not continue with the structure of the twin model. 
%
%\section{Initial points and Stopping rule and global search details}
%Space-Filling design
%EI < threshold
%EI multimodal: multistart + gradient approach unreliable. ref Jones

%\subsection{GP-UCB converges no slower than without the estimated gradient sampling}
%
%
%
%EI
%1. the sequence of evaluation points is dense in the search domain if J belongs to H.
%2. Assumptions: <1> X is compact subset of Rd, d>=1
%                <2> xi is centered GP
%                <3> covariance function k is continuous and positive definite
%definition of NEB property: equivalent conditions:
%		<1> y is adherent point of the set xn
%		<2> sigma2 goes to 0 when n goes to zero
%a sufficient condition for NEB property.
%Notations.
%A generalization of the EI criterion,
%	<1> rho continuous
%	<2> if sigma <=0, rho = 0
%	<3> if sigma > 0, rho > 0 
%
%Theorem 5. If Hilbert space (k) has NEB property, then
%for all xinit, and f in H, the sequence generated EI by is dense.
%
%Theorem 6. If Hilbert space has NEB property, then the sequence is P-almost surely dense.
%
%Theorem 4.
%Proposition 8. Let xn, yn be two sequences in X. Assume the sequence yn is convergence to y*,
%then each one implies the next one:
%	<1> y* is adherent point of xn
%	<2> sigma2(yn, xn) goes to 0 when n goes to infinity
%	<3> xi mean at yn goes to xi at y* when n goes to zero. for all functions in H.
%If H has a <spectral property> (the assumption sufficient for NEB property),
%then <3> goes to <1>, thus proving NEB property.
%
%First prove theorem 4. Then:
%Let nu n = sup rho n x, is the maximum acquisition function of EI criterion.
%Proposition 8, we have Lemma 9. lim inf nu n w = 0
%
%Using Lemma 9, Cauchy-Schwarz, and NEB property, we prove Theorem 5.
%
%
%
%H attached to xi, ref Bogachev 1998 (Cameron-Martin space of xi)
%UCB
%
%\section{Infer conservation laws by the space-time solution}
%\label{infer}
%\indent Because Eqns. \eqref{first equation} or \eqref{first equation steady}
%haves an unknown flux, the adjoint method cannot be applied to evaluate 
%$\frac{dJ}{dc}$. To enable the adjoint method in such a scenario, we will infer
%the unknown flux.\\
%%\indent We first discuss the reason that infering the conservation law is feasible.
%%Consider a general dynamical system
%%\begin{equation}
%%    \dot{u} = \mathcal{L}(u)\,,
%%    \label{general equation}
%%\end{equation}
%%where $u=\{u_1,\cdots, u_n\}$, $u_i = u_i(t,x)$, $i=1,\cdots,n$, $x\in \mathbb{R}^n$.
%%$\mathcal{L}$ is a differential operator known as the Hamiltonian of the system.
%%Inferring the differential operator can be difficult, however it is not necessary.
%%The flow quantities satisfy a conservation law, and their PDEs 
%%can be written as prototypes Eqn\eqref{first equation}
%%or Eqn\eqref{first equation steady}.
%%Therefore, the problem of adapting the physics of the physics-based surrogate reduces to the
%%problem of adjusting a set of functions $F_i$ or
%%$q_i$, for $i=1,\cdots,n$. \\
%
%\indent We use the space-time solution of the gray-box simulations 
%to infer the flux.
%There are several benefits by using the space-time solution \cite{hanmaster, ecmor}.
%Firstly, in conservation law simulations, the flow quantities only depend on the flow
%quantities in a previous time inside a domain of dependence.
%When the timestep is small,
%the domain of dependence can be small, as well. For example, for scalar conservation laws 
%without exogenous control,
%we can view solving the conservation law for one timestep $\Delta t$ at a spatial location
%as a mapping 
%$\mathbb{R}^{\omega_{\Delta t}} \rightarrow \mathbb{R}$, where $\omega_{\Delta t}\in \Omega$ 
%is the domain of dependence. 
%By applying such mapping repeatedly to all $x\in \Omega$ and $t\in[0,T]$
%(in addition to the boundary and initial conditions), we perform 
%a space-time simulation of the conservation law. 
%Generally, the size of $\omega_{\Delta t}$ is small.
%Therefore, in the discretized simulation of conservation laws,
%the number of discretized flow variables involved in $\omega_{\Delta t}$ is small, as well,
%making it feasible to infer the mapping.
%\\
%
%\begin{figure}[H]\begin{center}
%    \includegraphics[height=5cm]{locality.png}
%    \caption{Domain of dependence: in conservation law simulations,
%             the flow quantities at a given location
%             depend on the flow quantities at an older time only within its
%             domain of dependence. The two planes in this figure indicates the spatial 
%             solution at two adjacent timesteps.
%             The domain of dependence can be much smaller
%             than the overall spatial domain when the timestep is reasonably small.}
%    \label{locality}
%\end{center}\end{figure}
%
%\indent Secondly, the space-time solution at almost {every} space-time grid point can be viewed
%as a sample for the mapping $\mathbb{R}^\omega \rightarrow \mathbb{R}$. 
%Because the number of space-time grid points in gray-box simulations is generally large,
%we have a large number of samples to infer the mapping. 
%In Eqn.\eqref{first equation} and \eqref{first equation steady},
%such a mapping is determined by the flux.
%Therefore, we will have a large number of samples to infer the flux,
%making the inference potentially accurate.\\
%
%\indent Thirdly, in many optimization problems, the design space is high dimensional only because
%the design is space- and/or time-dependent. In order to parameterize the space-time dependent
%design, a large number of design variables will be employed. However, 
%the flow quantities only depend on the design variables in the domain of dependence.
%Therefore, even if the overall number of design variables is high, the number of design variables
%involved in the mapping is limited, making the inference problem potentially
%immune to the design space dimensionality.\\
%
%\indent Therefore, we propose to infer the flux in Eqn.\eqref{first equation} or
%\eqref{first equation steady}. The inferred
%PDE should yield a solution, $\tilde{\boldsymbol{u}}$, that matches 
%the solution of the gray-box simulation, $\boldsymbol{u}$.
%A simulator of the inferred PDE is called the {twin model}.
%
%%In the mean time, the \emph{physics} of the physics-based surrogate is still fixed offline
%%before the optimization. Can we use primal model samplings to
%%directly correct the physics of the surrogate? Bearing this question at mind,
%%we propose \emph{twin model}: a physics-based surrogate model with flexible physics.\\
%
%
%\section{Twin model inference as an optimization problem}
%\label{inverse}
%\indent 
%Conventionally, we have a given PDE, and want to compute
%its space-time solution. However, in a twin model, we want to infer the PDE
%to match a given space-time solution.
%Finding a suitable PDE, specifically a suitable flux function, can be viewed
%as an inverse problem, which can be solved by optimization.
%We define a metric for the mismatch of the space-time solutions.
%Given the same inputs (design variables, initial conditions, and boundary conditions), 
%a twin model should yield a space-time solution $\tilde{\boldsymbol{u}}$ such that 
%$\tilde{\boldsymbol{u}}$ is close to $\boldsymbol{u}$. 
%Suppose that the twin model and the 
%primal model use the same discretization; we use the following expression to quantify
%the mismatch:
%\begin{equation}
%    \mathcal{M} = \frac{1}{T}
%    \sum_{i=1}^{N}\sum_{k=1}^{M} \left(\tilde{\boldsymbol{u}}_{ik} 
%    - \boldsymbol{u}_{ik}\right)^2 \Delta t_k
%    \left| \Delta \mathbf{x}_i \right|
%    \label{minimizer twin model discrete}
%\end{equation}
%where $\left| \Delta \mathbf{x}_i \right|$ indicates the lengths (1-D), areas (2-D), or volumes (3-D) 
%of the grid.
%If the space- and/or time-grids are different, then a mapping $P$ from $u$ to $\tilde{u}$ is required.
%In this case, the mismatch is defined by
%\begin{equation}
%    \mathcal{M} = \frac{1}{T}
%    \sum_{i=1}^{N}\sum_{k=1}^{M} \left(\tilde{\boldsymbol{u}}_{ik} - P(\boldsymbol{u})_{ik}\right)^2 
%    \Delta t_k
%    \left| \Delta \mathbf{x}_i \right|
%    \label{minimizer twin model discrete mapping}
%\end{equation}
%For the present examples, we assume that the grids are the same for the purpose of simplicity.\\
%
%\indent 
%We parameterize the flux function and infer the parameterization that minimizes $\mathcal{M}$.
%Let the parameterized flux function be $G(\tilde{\boldsymbol{u}}, \xi)$,
%where $\xi$ values are the parameters.
%The inference problem is stated as follows:\\
%
%%\fbox{\parbox{\textwidth}{
%\indent Solve
%\begin{equation}
%    \xi^* = 
%    \arg\min_{\xi} \left\{
%    \mathcal{M}
%    + \lambda \|\xi\|^p  \right\}
%    \label{objective twin model}
%\end{equation}
%where $\tilde{\boldsymbol{u}}$
%is the discretized space-time solution of
%\begin{equation}
%    \dot{ \tilde{\boldsymbol{u}}} + \nabla \cdot
%    G(\tilde{\boldsymbol{u}}, \xi)
%    = \boldsymbol{q}(\tilde{\boldsymbol{u}},c)
%    \label{first equation 2}
%\end{equation}
% $\lambda\|\xi\|^p$
%is an $L_p$ norm regularization, and $\lambda>0$.
%The space-time discretization of Eqn.\eqref{first equation 2} is the same as the gray-box simulator.
%%}}
%\\
%
%\indent Similarly, if we are interested in the steady state solution,
%we have
%\begin{equation}
%    \mathcal{M} = \frac{1}{T}
%    \sum_{i=1}^{N} \left(\tilde{\boldsymbol{u}}_{i} - \boldsymbol{u}_{i}\right)^2
%    \left| \Delta \mathbf{x}_i \right|
%    \label{minimizer twin model discrete steady}
%\end{equation}
%
%\indent The inference problem is stated as follows:\\
%
%%\fbox{\parbox{\textwidth}{
%\indent Solve
%\begin{equation}
%    \xi^* = 
%    \arg\min_{\xi} \left\{
%    \mathcal{M} 
%    + \lambda \|\xi\|^p  \right\}
%    \label{objective twin model steady}
%\end{equation}
%where $u$ is the discretized spatial solution of the gray-box simulation, and $\tilde{u}$
%is the discretized spatial solution of
%\begin{equation}
%    \nabla \cdot
%    G(\tilde{\boldsymbol{u}}, \xi)
%    = \boldsymbol{q}(\tilde{\boldsymbol{u}},c)
%    \label{first equation 2 steady}
%\end{equation}
%%}}\\
%
%\indent The match of space-time solutions does not ensure the match of flux functions.
%The problem can be ill-posed to infer $F$ on a domain not covered by the gray-box solution $u$.
%In other words, the basis for modeling the flux may be over-complete.
%We will call a domain of $u$ ``excited'' if inferring $F$ is well-posed on that domain.
%The ill-posedness
%can be allieviated by basis selection. It has been shown that basis selection can be performed
%by Lasso regularization corresponding to $p=1$ \cite{Lasso variable selection}.
%We will set $p=1$ in this article. \\
%
%\indent Because the twin model is an open-box system, Eqns.\eqref{objective twin model} 
%and \eqref{objective twin model steady} can be solved by adjoint gradient-based methods.
%The parameterization of the twin model flux $G$ will be problem-dependent. We will discuss
%this topic in Section \ref{numerical example}.
%
%%\indent Next, we consider choosing an appropriate basis library $g$.
%%Many basis can be used, such as polynomial basis, Fourier basis, and wavelet basis.
%%For the example of inferring the 1D conservation law, we are interested in inferring the flux.
%%The addition of a constant to the flux function does not change the conservation law,
%%therefore we are actually interested in inferring the derivative of the flux.
%%We will use sigmoid functions as the basis, because the derivative of sigmoid functions
%%is almost zero except at the region close to its center.\\
%%
%%\noindent Although the objective is to minimize 
%%$
%%\sum_{i=1}^{N}\sum_{k=1}^{T} \left(\tilde{u}_{ik} - u_{ik}\right)^2 \Delta t_k
%%\left| \Delta \mathbf{x}_i \right|
%%$, we will not use it as the objective function directly and perform optimization
%%in one shot. If $\tilde{F}(u)$ deviates from $F(u)$ a lot, then $\tilde{u}(x,t)$
%%can deviate from ${u}(x,t)$ significantly even at a small $t$. 
%%Therefore, solving the twin model and its adjoint in $t=[0,T]$ without
%%an educated $\tilde{F}(u)$ can be a waste of computation 
%%resources. To improve efficiency, we propose a progressive optimization
%%procedure:\\
%%\fbox{\parbox{\textwidth}{
%%\begin{algorithm}[H]
%%    $\xi^*=\mathbf{0}$\;
%%    Set integers $2= i_1< \cdots < i_M=T$\;
%%    \For{$I=i_1,\cdots, i_M$}{
%%        Optimize
%%        $$
%%           \xi^* \leftarrow \arg\min_{\xi} 
%%           \sum_{i=1}^{N}\sum_{k=1}^{I} \left(\tilde{u}_{ik} - u_{ik}\right)^2 \Delta t_k 
%%           \left| \Delta \mathbf{x}_i \right|
%%        $$
%%        with initial guess $\xi^*$.
%%  }
%%  \caption{Progressive optimization procedure}
%%  \label{progressive algo}
%%\end{algorithm}
%%}}\\
%%
%%\noindent Notice $k=1$ corresponds to the initial condition. Since the twin model
%%uses the same initial condition as the black-box model, $\tilde{u}_{i1} - u_{i1}$
%%is always zero.
%%Choosing the integer sequence $i_1,\cdots,i_M$ can be problem dependent.
%%Our experience shows the sequence should be denser at small $i$, and sparser at larger $i$.
%%The tolerance of each sub-optimization problem does not need to be tight,
%%except for the last iteration where $I=T$.
%%In our problem, we use $i_l = \min\left\{ 1+2^l, T\right\}$,
%%a relative tolerance of $10\%$, and maximum $10$ iterations for the sub-optimization problems.\\
%
%%\section{Excited domain and basis selection}
%%\label{adaptive}
%%The basis for modelling the flux or the source term may be over-complete.
%%In other words, the inference may be ill-posed and not has a unique solution.
%%A twin model with a basis selection scheme should be able to adaptively
%%refine its basis on-the-fly during fitting the basis coefficients $\xi$.
%%For example, in the Buckley-Leverett equation example,
%%the value of $u(t,x)$, $t\in[0,T],\,x\in[0,1]$ is bounded, i.e.
%%$0< u_{\min}\le u(t,x)\le u_{\max} < 1$; therefore as long as 
%%$\nabla\tilde{F}(u) = \nabla F(u)$ for $u\in[u_{min},u_{max}]$, we will have
%%$\tilde{u}(t,x) = u(t,x)$ for $t\in[0,T]$ and $x\in[0,1]$.
%%In other words, in the numerical example, $\nabla \tilde{F}(u) = \nabla F(u)$ for $u\in [0,1]$
%%is a sufficient but not necessary condition
%%for $\tilde{u}(t,x) = u(t,x)$. 
%%Intuitively, for some domains $u$, $\nabla \tilde{F}(u)$ has to approximate $\nabla F(u)$
%%accurately in order to give a good space-time solution match.
%%We will call such domains \emph{excited domain}, written as $\mathcal{E}$. 
%%Notice the excited domain is not a domain of space or time.  
%%Clearly $\mathcal{E}$ depends on $u(t,x)$.
%%For $u$ not inside $\mathcal{E}$,
%%$\nabla \tilde{F}(u)$ will have little or no effect on the solution match; therefore
%%we may not certify $\nabla \tilde{F}(u)$'s accuracy outside $\mathcal{E}$
%%no matter how closely the solutions match.
%%Basis selections should only be navigated to $\mathcal{E}$.\\
%%
%%\noindent Consider a twin model solving
%%\begin{equation}
%%    \frac{\partial\tilde{u}(t,x)}{\partial t} + \nabla \cdot 
%%    \tilde{F}(\mathcal{D} \tilde{u}, \kappa) 
%%    = q(\tilde{u},c(t,x))\,, \quad \tilde{u}(t,x) \in \mathbb{R}^n\,,
%%    \label{twin equation def}
%%\end{equation}
%%We define the excited domain $\mathcal{E}$ by its complement $\bar{\mathcal{E}}$:\\
%%\fbox{\parbox{\textwidth}{
%%\begin{definition}
%%    Given a primal model, its discretized solution of $u(t,x)$, and
%%    a twin model Eqn\eqref{twin equation def},
%%    the excited domain $\mathcal{E}$ is a domain of $\tilde{F}(\cdot)$.
%%    Let the complement of $\mathcal{E}$ be $\bar{\mathcal{E}}$.
%%    Consider a perturbed twin model flux $\tilde{F}_{\delta}(\cdot) =F(\cdot)+ 
%%    \delta(\cdot)$.
%%    The perturbed twin model gives the discretized solution $\tilde{u}_{\delta}$.\\
%%
%%    A set $e \subseteq \bar{\mathcal{E}}$ if and only if
%%    $$ \frac{1}{T}
%%    \sum_{i=1}^{N}\sum_{k=1}^{T} \left(\tilde{u}_{\delta, ik} - u_{ik}\right)^2 \Delta t_k
%%    \left| \Delta \mathbf{x}_i \right|$$
%%    is a constant for
%%    any $\delta$ with $\textrm{support}[ \delta] = e$.
%%\end{definition}
%%}}\\
%%
%%\noindent This definition requires $F$ a prior, therefore it is not directly implementable.
%%The definition also requires enumeration of all possible $\delta$ to validate $\mathcal{E}$.
%%In practice, we can only validate a finite set of $\delta$, for example the basis function library
%%$g$.
%%\\
%%
%%\noindent Basis selection may be performed by regularization \cite{Lasso variable selection,
%%Critical review of variable selection}. 
%%For example, it has been shown that basis selection can be performed by having a
%%Lasso regularization term in the solution mismatch. In our problem, the metric of
%%solution mismatch with Lasso regularization would be
%%\begin{equation}
%%    \frac{1}{T}
%%    \sum_{i=1}^{N}\sum_{k=1}^{T} \left(\tilde{u}_{\epsilon\delta, ik} - u_{ik}\right)^2 \Delta t_k
%%    \left| \Delta \mathbf{x}_i \right|
%%    + \lambda \|\xi\|_1\,,
%%    \label{Lasso mismatch}
%%\end{equation}
%%where $\|\cdot\|_1$ is the $L_1$ norm. \\
%
%\section{Numerical examples}
%\label{numerical example}
%In this section, we demonstrate the twin model with two numerical examples.
%\subsection{Gradient estimation for a 1-D porous media flow}
%Consider a 1-D PDE
%\begin{equation}
%    \frac{\partial u}{\partial t} + \frac{\partial F(u)}{\partial x} = c\,\quad x\in[0,1]\; t\in[0,1]
%    \label{BL eqn}
%\end{equation}
%with periodic boundary condition
%\begin{equation}
%    u(x=0) = u(x=1)
%\end{equation}
%and initial condition
%\begin{equation}
%    u(t=0) = u_0
%\end{equation}
%Eqn.\eqref{BL eqn} can be used to model 1-D, two-phase, porous media flow, where $u$
%denotes the saturation of one phase. $0\le u\le 1$. $c=c(t,x)$ is a space-time dependent exogenous control.
%The flux function $F(u)$ depends on the properties of the porous media and the fluids.
%For example, the Buckley-Leverett equation models the flow driven by 
%capillary pressure and Darcy's law \cite{Buckley Leverett}, whose flux is
%\begin{equation}
%    F(u) = \frac{u^2}{1+A(1-u)^2}
%    \label{BL flux}
%\end{equation}
%where $A$ is a constant. In the following we assume the graybox simulation
%solves Eqn.\eqref{BL eqn} and \eqref{BL flux} with $A=2$.
%\\
%
%\indent Assuming that $F(u)$ is unknown, we will fit a twin model 
%using the graybox simulation.
%We parameterize the twin model according to Eqn.\eqref{first equation 2}
%\begin{equation}
%    \frac{\partial \tilde{u}}{\partial t} + \frac{\partial}{\partial x}\,
%    \left(\sum_{k=1}^m \xi_k g_{k}(\tilde{u})\right) = c
%    \label{twin model 3}
%\end{equation}
%with the same initial condition, boundary conditions, and exogenous control. 
%
%\indent Generally, the flux $F$ is a monotonic increasing function. To respect this fact, we
%choose $g$ values as a family of sigmoid functions 
%\begin{equation}
%    g_k(\tilde{u}) = \left(\tanh\left(\frac{\tilde{u}- \eta_k}{\sigma}\right)+1\right) \left.\right/2
%    \label{sigmoids}
%\end{equation}
%where $\eta_k$ and $\sigma$ are constants. Therefore, we just need to set $\xi\ge 0 $ 
%to enforce the monotonicity of the flux.
%We will use  a second-order finite volume discretization and 
%Crank-Nicolson time integration scheme in the twin model.
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=7cm]{fluxbasis.png}
%    \caption{An example of the flux basis functions $g_k(\tilde{u})$. 
%    These basis functions are distinguished by different colors.}
%\end{center}\end{figure}
%
%To infer the coefficients $\xi$, we minimize
%\begin{equation}
%    \int_0^1 \int_0^1 (u-\tilde{u})^2 dx dt+ \lambda \sum_{k=1}^m |\xi_k|
%\end{equation}
%We use the {low-memory Broyden-Fletcher-Goldfarb-Shannon} (BFGS) algorithm
%\cite{LBFGS} for the minimization. 
%L-BFGS approximates the Hessian using only the gradients at newer previous iterations,
%and inverses the approximated Hessian efficiently using the Sherman-Morrison formula.
%The gradient, $\frac{dJ}{d\xi_k}\,, k=1,\cdots, m$, 
%is computed by an automatic differentiation module \textit{numpad} \\
%$[$Q. Wang, https://github.com/qiqi/numpad.git.$]$\\
%
%
%\indent We set $c=0$, and test the quality of the twin model given several initial conditions.
%These initial conditions are shown in Fig. \ref{fig:initial condition}.
%For different initial conditions, the excited domain will be different.
%Let $u_{\max} = \max_{t,x}u(t,x)$ and $u_{\min} = \min_{t,x}u(t,x)$,
%we expect the inferred flux to match the true flux within $[u_{\min},u_{\max}]$.
%\begin{figure}[H]\begin{center}
%    \includegraphics[height=8cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/initial_conditions.png}
%    \caption{Initial condition $u_0(x)$. 
%    We choose a diverse set of initial conditions to test the twin model method.}
%    \label{fig:initial condition}
%\end{center}\end{figure}
%\indent In Eqn.\eqref{first equation},
%we can add a constant to the flux while yielding the same solution $u$. 
%Therefore, we compare $\frac{dF}{du}$ with $\frac{d\tilde{F}}{du}$, instead of comparing
%$F$ with $\tilde{F}$. We also show the
%space-time solution of the gray-box simulation, as well as the solution mismatch, for these initial conditions.\\
%\begin{figure}[H]\begin{center}
%    Flux gradient\hspace{2cm} Gray-box solution \hspace{2cm} Solution mismatch
%    \includegraphics[width=4.9cm,height=3.7cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/0_x_new.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/0_sol.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/0_err.png}
%    \label{fig:sol compare}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=4.9cm,height=3.7cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/1_x_new.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/1_sol.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/1_err.png}
%    \label{fig:sol compare}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=4.9cm,height=3.7cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/2_x_new.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/2_sol.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/2_err.png}
%    \label{fig:sol compare}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=4.9cm,height=3.7cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/3_x_new.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/3_sol.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/3_err.png}
%    \label{fig:sol compare}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=4.9cm,height=3.7cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/4_x_new.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/4_sol.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/4_err.png}
%    \label{fig:sol compare}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=4.9cm,height=3.7cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/5_x_new.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/5_sol.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/5_err.png}
%    \label{fig:sol compare}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=4.9cm,height=3.7cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/6_x_new.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/6_sol.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/6_err.png}
%    \label{fig:sol compare}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=4.9cm,height=3.7cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/7_x_new.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/7_sol.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/7_err.png}
%    \label{fig: sol compare}
%    \caption{The left column shows the gradient of $F$ (red line) with the gradient of $\tilde{F}$ (blue line). 
%             The middle column shows the space-time solutions of the gray-box model.
%             The right column shows the solution mismatch $\left|u-\tilde{u}\right|$. Each row corresponds to an initial condition.
%             The excited domain, $[u_{\min}, u_{\max}]$, is indicated by the green region in
%             the left column.
%             When the excited domain is large, the twin model's flux 
%             approximates the gray-box model's flux in a large range of $u$, but
%             the solution mismatch tends to be less accurate in the exicted domain. When the excited domain is small,
%             the twin model's flux approximates the gray-box model's flux in a small range of $u$, but the flux
%             approximation tends to be more accurate in the excited domain.}
%\end{center}
%
%\end{figure}
%
%Using the twin model, we can estimate the objective's gradient by applying the
%adjoint method to the twin model, i.e. we approximate
%$\frac{\partial J}{\partial u}$ with $\frac{\partial \tilde{J}}{\partial \tilde{u}}$.
%Suppose the gray-box model solves
%\begin{equation}
%    \frac{\partial u}{\partial t} + \frac{\partial}{\partial x}\,
%    \left(F(u)\right) = c
%\end{equation}
%for $c=0$, with $F(u)$ given by Eqn.\eqref{BL flux} with $A=2$. We have trained a twin model
%Eqn.\eqref{twin model 3} using the space-time solution. We are interested in
%the approximation quality of the twin model's gradient at $c=0$.
%$c$ is space-time dependent, therefore $\frac{dJ}{dc}$ is space-time dependent too.
%We compare $\frac{dJ}{dc}$ with $\frac{d\tilde{J}}{dc}$ in Fig. 5.
%%The results are shown in Fig. 5.
%%\begin{figure}[H]\begin{center}
%%    \includegraphics[width=7.5cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/0_Jc.png}
%%    \includegraphics[width=7.5cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/1_Jc.png}
%%\end{center}\end{figure}
%%\begin{figure}[H]\begin{center}
%%    \includegraphics[width=7.5cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/2_Jc.png}
%%    \includegraphics[width=7.5cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/3_Jc.png}
%%\end{center}\end{figure}
%%\begin{figure}[H]\begin{center}
%%    \includegraphics[width=7.5cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/4_Jc.png}
%%    \includegraphics[width=7.5cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/5_Jc.png}
%%\end{center}\end{figure}
%%\begin{figure}[H]\begin{center}
%%    \includegraphics[width=7.5cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/6_Jc.png}
%%    \includegraphics[width=7.5cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/7_Jc.png}
%%    \caption{Comparison of $J(c)$ (red line) with $\tilde{J}(c)$ (blue dashed line) for the 8 cases. 
%%    The black vertical line indicates
%%    $c=0$ where the twin models are trained. In all these cases, the twin model estimates
%%    the gradient of the objective function accurately.}
%%\end{center}\end{figure}
%
%
%\indent \begin{figure}[H]\begin{center}
%    $\frac{dJ}{dc}$ \hspace{4.3cm} $\frac{d\tilde{J}}{dc}$  \hspace{4cm} $\left|\frac{dJ}{dc} - \frac{d\tilde{J}}{dc}\right|$\\
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/0_adj_primal.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/0_adj_twin.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/0_adj_err.png}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/1_adj_primal.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/1_adj_twin.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/1_adj_err.png}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/2_adj_primal.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/2_adj_twin.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/2_adj_err.png}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/3_adj_primal.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/3_adj_twin.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/3_adj_err.png}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/4_adj_primal.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/4_adj_twin.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/4_adj_err.png}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/5_adj_primal.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/5_adj_twin.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/5_adj_err.png}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/6_adj_primal.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/6_adj_twin.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/6_adj_err.png}
%\end{center}\end{figure}
%\vspace{-1cm}
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/7_adj_primal.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/7_adj_twin.png}
%    \includegraphics[width=5.1cm]{/home/voila/Documents/2014GRAD/mirror/final/paper1/7_adj_err.png}
%    \label{fig:sol compare}
%    \caption{The left column shows $\frac{dJ}{dc}$ which is evaluated by the gray-box model. 
%             The middle column shows $\frac{d\tilde{J}}{dc}$ which is evaluated by the trained twin model.
%             The right column shows $\left|\frac{dJ}{dc} - \frac{d\tilde{J}}{dc}\right|$.
%             We observe that the gradient is more accurate when the excited domain is smaller.}
%\end{center}\end{figure}
%
%
%\indent The result is encouraging, as the gradient computed by the twin model provides
%a good approximation
%of the gradient of the primal model. We reiterate that the good approximation quality benefits from 
%the matching of the space-time solution.\\
%
%\subsection{Gradient estimation of Navier-Stokes flows}
%\indent In the second numerical test case, we consider
%a compressible internal flow in a 2-D return bend channel.
%The flow is driven by the pressure difference between the inlet and the outlet.
%The flow is governed by Navier-Stokes equations, Eqn.\eqref{NSeqn}.
%Navier-Stokes equations require an additional state equation, Eqn.\eqref{state equation}, for closure.
%Many models of the state equations have been developed, including the ideal gas equation, the
%van der Waals equation, and the Redlich-Kwong equation \cite{state eqns}.\\
%
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=12cm]{../numpad/test/ns_cases_spline/results/mesh/spline.png}
%    \caption{The return bend geometry and the mesh for simulation. The return bend is bounded by 
%    no-slip walls. The values of pressure
%    are fixed at the inlet and at the outlet. 
%    The inner and outer boundaries of the bend are generated by the control points
%    using quadratic B-spline.
%    We estimate the gradient of the steady state mass flux
%    to the red control points' coordinates.}
%    \label{NS mesh}
%\end{center}\end{figure}
%
%\indent The inner and outer boundaries at the bending section 
%are each generated by 6 control points using quadratic B-spline. 
%The control points are shown by the red and gray dots in Fig. 6. The gray control points are fixed 
%on the straight sections.
%The spanwise grid is generated by geometric grading. The streamwise
%grid at the straight and the bending section are each generated
%by uniform grading, except at the sponge region.
%The pressure at the outlet is set to be a constant $p_{out}$ while
%the total pressure at the inlet is set to be a constant $p_{t,in}$.
%Let $\rho_\infty$ be the steady state density, and
%$\boldsymbol{u}_\infty = (u_\infty, v_\infty)$ be the steady state Cartesian velocity.
%The steady state mass flux is
%\begin{equation}
%    J = - \int_{\textrm{outlet}} \rho_\infty u_\infty \big|_{\textrm{outlet}} \; dy=
%    \int_{\textrm{inlet}} \rho_\infty u_\infty\big|_{\textrm{inlet}} \; dy
%    \label{mass flux}
%\end{equation}
%We want to estimate the gradient of the steady state mass flux
%to the red control points' coordinates.
%\\
%
%\indent When the state equation of the fluid is unknown, the 
%adjoint method cannot be applied directly
%to estimate the gradient. We use the proposed twin model to infer the state
%equation from the steady state solution of the gray-box simulation.
%Assume that the gray-box simulation provides $\rho_\infty$, $\boldsymbol{u}_\infty$,
%and $E_\infty$. 
%Fig. 7 shows an example of the solution.
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=16cm]{../numpad/test/ns_cases_spline/results/gray_sol/gray0_spline.png}
%    \caption{An example of the steady state velocity, energy, and density provided 
%    by the gray-box simulation. In the velocity subplot, the 
%    magnitude of the velocity is overlayed with the velocity vectors.}
%    \label{gray sol}
%\end{center}\end{figure}
%
%\indent We parameterize the unknown state equation by
%\begin{equation}
%    p(\rho, U) = \sum_{i=1}^{N_\rho} \sum_{j=1}^{N_U} \alpha_{ij} R_i(\rho) S_j(U) 
%    + p_0
%    \label{parameterization}
%\end{equation}
%where
%\begin{equation}
%    R_i(\rho) = \exp\left(\frac{-(\rho-\rho_i)^2}{\sigma_\rho}\right)\,,\quad i=1,\cdots,N_\rho 
%\end{equation}
%\begin{equation}
%    S_j(U) = \frac{1}{2}\left(\tanh\left( \frac{U-U_j}{\sigma_U}\right) +1\right)
%    \quad j = 1,\cdots,N_U\,.
%\end{equation}
%$R_i$ are radial basis functions, and $S_j$ are sigmoid functions.
%Let the density of the gray-box solution be in the range of $[\rho_{\min}, \rho_{\max}]$, and
%let the internal energy of the gray-box solution be in the range of $[U_{\min}, U_{\max}]$.
%We set $\rho_i$, $i=1,\cdots,N_\rho$ to be equally spaced in $[\rho_{\min}, \rho_{\max}]$, and set
%$U_j$, $j=1,\cdots,N_U$ to be equally spaced in $[U_{\min}, U_{\max}]$.
%We constrain $\alpha$ values to be positive to respect the fact that pressure
%monotonically increases with the internal energy.
%$p_0$ is a scalar. We set $\sigma_\rho=\frac{\rho_{\max}-\rho_{\min}}{N_\rho}$, 
%$\sigma_U = \frac{U_{\max}-U_{\min}}{N_U}$.\\
%
%\indent We define the solution mismatch, Eqn.\eqref{minimizer twin model discrete steady}, as
%\begin{equation}
%    \mathcal{M} = w_\rho \|\tilde{\rho}_{\infty} - \rho_{\infty}\|^2
%                + w_u
%                \|\tilde{u}_{\infty}- u_{\infty}\|^2
%                + w_v
%                \|\tilde{v}_{\infty}- v_{\infty}\|^2
%                + w_E
%                \|\tilde{E}_{\infty} - E_\infty\|^2
%    \label{NS mismatch}
%\end{equation}
%where $w_\rho$, $w_u$, $w_v$, and $w_E$ are positive weight constants.
%$\|\cdot\|$ is the $L_2$ norm.
%To infer the state equation we solve the optimization problem:
%\begin{equation}
%    \min_{\alpha, p_0} \left\{\mathcal{M} +\lambda 
%    \sum_{i=1}^{N_\rho}\sum_{j=1}^{N_U} \big|\alpha_{ij}\big|
%    \right\}
%    \label{NS optimize}
%\end{equation}
%
%\indent We need to choose suitable weights $w_\rho$, $w_u$, $w_v$, and $w_E$ in
%Eqn.\eqref{NS optimize}. To select these weights, 
%we first set $\alpha$ and $p_0$ values to several randomly guessed values. 
%Using these guessed state equation, we 
%obtain $\|\tilde{\rho}_{\infty} - \rho_{\infty}\|^2$,
%$\|\tilde{u}_{\infty}- u_{\infty}\|^2 $,
%$ \|\tilde{v}_{\infty}- v_{\infty}\|^2$,
%and $\|\tilde{E}_{\infty} - E_\infty\|^2$.
%The weights are chosen to be
%\begin{equation}\begin{split}
%    w_\rho &= \frac{1}{\left<\|\tilde{\rho}_{\infty} - \rho_{\infty}\|^2\right>}\\
%    w_u &= \frac{1}{\left<\|\tilde{u}_{\infty} - 
%    {u}_{\infty}\|^2\right>}\\
%    w_v &= \frac{1}{\left<\|\tilde{v}_{\infty} - 
%    v_{\infty}\|^2\right>}\\
%    w_E &= \frac{1}{\left<\|\tilde{E}_{\infty} - 
%    E_{\infty}\|^2\right>}
%\end{split}
%\label{NS weights}
%\end{equation}
%where $\left<\cdot\right>$ denotes the sample average of the randomly-guessed state equations.
%In this way, $\tilde{\rho}_\infty-\rho_\infty$, 
%$\tilde{u}_{\infty}-u_\infty$, $\tilde{v}_{\infty}-v_\infty$,
%and $\tilde{E}_\infty-E_\infty$ will, on average, contribute
%similarly to the solution mismatch for the randomly-guessed state equations.\\
%
%\indent We tested three example state equations in the graybox simulator:
%the ideal gas equation, the van der Waals equation, and the Redlich-Kwong equation:
%\begin{equation}\begin{split}
%    p_{ig} &= (\gamma-1) U\\
%    p_{vdw} &= \frac{(\gamma-1)U}{1-b_{vdw}\rho} - a_{vdw}\rho^2\\
%    p_{rk} &= \frac{(\gamma-1)U}{1-b_{rk}\rho} - 
%    \frac{a_{rk}\rho^{5/2}}{((\gamma-1)U)^{1/2}(1+b_{rk}\rho)}
%\end{split}\label{NS state equations}
%\end{equation}
%where $a_{vdw}$, $b_{vdw}$, $a_{rk}$, $b_{rk}$ are constants. In the following testcases, we choose
%$a_{vdw}=10^4$, $b_{vdw}=0.1$, $a_{rk}=10^7$, $b_{rk}=0.1$.\\
%
%\indent 
%By solving Eqn \eqref{NS optimize}, we obtain the solution mismatch for the state equations.
%The solution mismatch,
% $\left|\tilde{\rho} -\rho\right|$, $\left|\tilde{\boldsymbol{u}}- \boldsymbol{u}\right|$, 
%and $\left|\tilde{E}-E\right|$,
%between the inferred twin model's solution and the graybox solution is shown in Fig. 8.
%\begin{figure}[H]\begin{center}
%    \centering Ideal gas\\
%    \includegraphics[width=16cm]{../numpad/test/ns_cases_spline/results/train_sol_error/err_0_spline.png}\\
%\end{center}\end{figure}
%\begin{figure}[H]\begin{center}
%    \centering van der Waals gas\\
%    \includegraphics[width=16cm]{../numpad/test/ns_cases_spline/results/train_sol_error/err_1_spline.png}
%\end{center}\end{figure}
%\begin{figure}[H]\begin{center}
%    \centering Redlich-Kwong gas
%    \includegraphics[width=16cm]{../numpad/test/ns_cases_spline/results/train_sol_error/err_2_spline.png}\\
%    \caption{The solution mismatch between the twin model's solution and the graybox solution for the
%    ideal gas equation, the van der Waals equation, and the Redlich-Kwong equation. 
%    The first group of images is for the ideal gas, the second group of images is for the van der Waals
%    gas, and the third group of images is for the Redlich-Kwong gas. For each gas, we show 
%    the solution mismatch of velocity, energy, and density. For all
%    three test cases, the relative error of the velocity, energy, and density is small.}
%    \label{fig:NS sol err}
%\end{center}\end{figure}
%
%\indent Let the $(\rho, U)$ be the gray-box steady state solution's 
%density and internal energy at all the spatial
%gridpoint, and let $H(\rho, U)$ be its convex hull. We expect the the estimated state equation
%to be more accurate inside $H(\rho, U)$ than outside $H(\rho, U)$.
%The inferred state equation, $p = p(\rho, U)$, is shown in Fig. 9.
%\begin{figure}[H]\begin{center}
%    \centering Ideal gas\\
%    \includegraphics[width=16cm]{../numpad/test/ns_cases_spline/results/train_state_eqn/state_ideal_gas_spline.png}
%\end{center}\end{figure}
%\begin{figure}[H]\begin{center}
%    \centering van der Waals gas\\
%    \includegraphics[width=16cm]{../numpad/test/ns_cases_spline/results/train_state_eqn/state_vdw_gas_spline.png}
%\end{center}\end{figure}
%\begin{figure}[H]\begin{center}
%    \centering Redlich-Kwong gas\\
%    \includegraphics[width=16cm]{../numpad/test/ns_cases_spline/results/train_state_eqn/state_rk_gas_spline.png}
%    \caption{The inferred state equation and the gray-box state equation for the three gases. 
%    The left column shows the inferred state equation, and the right column shows the 
%    gray-box state equation.
%    In all the three test cases, the inferred state equation approximates the gray-box state
%    equation accurately inside $H(\rho, U)$ indicated by the dashed line.}
%    \label{fig:NS state eqn}
%\end{center}\end{figure}
%
%\indent Using the inferred state equation, we are able to compute the
%gradient of the mass flux to the countrol points at the bending section.
%For example, the gradient and the perturbed boundary for the ideal gas are shown in Fig. 10.
%For all the three gases, the difference between the gray-box gradient and the
%twin model gradient is hardly visible.\\ 
%\begin{figure}[H]\begin{center}
%    \includegraphics[width=7cm]{../numpad/test/ns_cases_spline/results/outflux_geo_grad/geo_grad_0_spline.png}
%    \includegraphics[width=7cm]{../numpad/test/ns_cases_spline/results/outflux_geo_grad/perturb_0.png}
%    \caption{The left column shows the gradient of the outflux to the control points for the 
%    ideal gas. 
%    The wide gray arrow is the gradient evaluated by the gray-box model, while
%    the thin black arrow is the gradient evaluated by the twin model.
%    The right column shows a perturbed boundary according to the gradient. 
%    The blue dashed line is computed by the gray-box model's gradient, while the
%    red dashed line is computed by the twin model's gradient.}
%\end{center}\end{figure}
%
%\indent 
%We summarized the estimated gradient computed by the twin model
%in Table \ref{tab: idea gas gradient}, which is compared
%with the gradient computed by the gray-box model. For each gas,
%we compare the x-component and the y-component of the gradients.
%Twin model demonstrates to estimate the gradients accurately.
%\renewcommand{\arraystretch}{0.7}
%\begin{center}
%\captionof{table}{The gradient of the mass flux to the control points' coordinates}
%\label{tab: idea gas gradient}
%\begin{tabular}{|c|c|C{12mm}|C{12mm}|C{12mm}|C{12mm}|C{12mm}|C{12mm}|C{12mm}|C{12mm}|}
%\hline\hline
%\multicolumn{2}{|c|}{Ideal gas}&\multicolumn{4}{c|}{gradient at inner boundary}&\multicolumn{4}{c|}{gradient at outer boundary}\\
%\cline{1-10}
%x&Graybox&-2.407&-9.079&-7.727&-2.085&0.661&6.013&7.756&1.843\\
%\cline{2-10}
%&Twin model&-2.406&-9.071&-7.722&-2.084&0.661&6.008&7.751&1.841\\
%\hline
%y&Graybox&-0.971&-2.852&1.792&1.472&1.190&0.596&-1.854&-3.097\\
%\cline{2-10}
%&Twin model&-0.972&-2.849&1.792&1.473&1.189&0.595&-1.853&-3.094\\
%\hline\hline
%
%\multicolumn{2}{|c|}{VDW gas}&\multicolumn{4}{c|}{gradient at inner boundary}&\multicolumn{4}{c|}{gradient at outer boundary}\\
%\cline{1-10}
%x&Graybox&-2.389&-9.091&-7.736&-2.073&0.660&6.003&7.756&1.846\\
%\cline{2-10}
%&Twin model&-2.392&-9.085&-7.739&-2.078&0.659&6.006&7.759&1.846\\
%\hline
%y&Graybox&-0.942&-2.848&1.785&1.443&1.192&0.593&-1.858&-3.105\\
%\cline{2-10}
%&Twin model&-0.948&-2.846&1.789&1.452&1.188&0.593&-1.858&-3.104\\
%\hline\hline
%
%\multicolumn{2}{|c|}{RK gas}&\multicolumn{4}{c|}{gradient at inner boundary}&\multicolumn{4}{c|}{gradient at outer boundary}\\
%\cline{1-10}
%x&Graybox&-2.429&-9.064&-7.749&-2.122&0.663&6.045&7.773&1.837\\
%\cline{2-10}
%&Twin model&-2.412&-9.039&-7.702&-2.092&0.660&6.000&7.731&1.832\\
%\hline
%y&Graybox&-1.010&-2.848&1.820&1.536&1.183&0.603&-1.851&-3.081\\
%\cline{2-10}
%&Twin model&-0.995&-2.844&1.812&1.519&1.190&0.596&-1.845&-3.076\\
%\hline\hline
%
%\end{tabular}
%\end{center}
%
%
%
%\section{Conclusion}
%We propose a method to estimate the objective's gradient when the simulator is a gray-box conservation
%law simulator and does not
%implement the adjoint method. 
%The proposed method uses the space-time or spatial solution of the gray-box simulation to
%infer a twin model. 
%There are several benefits to use the space-time or spatial solution. Firstly, in many conservation law
%simulations, flow quantities have a small domain of dependence. Secondly, the space-time or spatial
%solution from a single simulation provides a large number of samples for the inference. 
%Thirdly, in many high-dimensional design problems, the design variables are space-time or spatially distributed, 
%so the inference's input dimension does not scale up with the design dimension.
%The twin model method enables adjoint computation. We use the gradient computed 
%by the twin model to estimate the gradient of the gray-box simulation.\\
%
%The twin model method is demonstrated on a 1-D porous media flow problem and a
%2-D Navier-Stokes problem. In the 1-D problem  
%the flux function is unknown. We are able to infer the flux function in the excited domain. 
%Using the inferred twin model, we estimate the gradient of the objective to the 
%space-time dependent control.
%In the 2-D problem, the state equation is unknown. We are able to infer
%the state equation using the steady state solution of the gray-box model.
%Using the inferred state equation, we estimate the gradient of the mass flux
%to the coordinates of the control points.
%Our research shows that gradient can be efficiently estimated by adjoint method even if 
%the simulator is gray-box.\\
%
%The twin model enables adjoint computation for gray-box simulations. 
%In the future, we plan to apply the twin model to high-dimensional optimization problems.
%
%
%
\newpage
\begin{thebibliography}{1}
\bibitem{twin model}
Chen, Han, and Qiqi Wang. 
"Adjoint-based gradient estimation from gray-box solutions of unknown conservation laws." arXiv preprint arXiv:1511.04576 (2015).

\bibitem{water flooding control}
D. Brouwer et al.,
Dynamic optimization of waterflooding with smart wells using optimal control theory,
{SPE Journal},
volume 9, number 4, 2004

% first oil reservoir optimization
\bibitem{first reservoir opt}
W. F. Ramirez,
Application of optimal control theory to enhanced oil recovery,
Elsevier, 1987.

\bibitem{adjoint well placement}
M. Zandvliet et al.,
Adjoint-based well-placement optimization under production constraints,
{SPE Journal}, volume 13, number 4, 2008.

% return bend optimization
\bibitem{ubend rans opt 1}
T. Verstraete et al., 
Optimization of a U-bend for minimal pressure loss in internal cooling channels  Part I: Numerical method,
{Journal of Turbomachinery},
volume 135, number 5, 2013.

\bibitem{ubend rans opt 2}
F. Coletti et al.,
Optimization of a U-Bend for minimal pressure loss in internal cooling channels  Part II: Experimental validation,
{Journal of Turbomachinery},
volume 135, number 5, 2013.

% Buckley
\bibitem{Buckley Leverett}
S. E. Buckley et al.,
Mechanism of fluid displacement in sands,
{Transactions of the AIME},
volume 146, number 1, 1942.

% BSIM
\bibitem{hanmaster} 
H. Chen,
Blackbox stencil interpolation method for model reduction,
Master thesis, Massachusetts Institute of Technology, 2012.

%ECMOR
\bibitem{ecmor}
H. Chen et al.,
Data-driven model inference and its application to optimal control under reservoir uncertainty,
14th European Conference of Mathematics of Oil Recovery, 2014.

% adjoint original
\bibitem{adjoint}
J. L. Lions,
Optimal control of systems governed by partial differential equations,
{Springer-Verlag}, 1971.

% adjoint aerodynamics
\bibitem{adjoint aerodynamics}
A. Jameson,
Aerodynamic design via control theory,
{Journal of Scientific Computing},
volume 3, number 3, 1988.

% adjoint history matching 
\bibitem{adjoint history matching}
W. H. Chen et al.,
A new algorithm for automatic history matching,
{Society of Petroleum Engineering Journal},
volume 14, number 6, 1974.

% adjoint reservoir optimization
\bibitem{adjoint reservoir optimal control}
W. F. Ramirez et al.,
Optimal injection policies for enhanced oil recovery:
part 1: theory and computational strategies.
{Society of Petroleum Engineering Journal},
volume 24, number 3, 1984.

% quasi-Newton methods
\bibitem{quasiNewton}
J. E. Dennis et al.,
Quasi-Newton methods, motivation and theory.
SIAM Review,
volume 19, number 1, 1977.

% DFO review
\bibitem{gradfreereview}
L. M. Rios et al.,
Derivative-free optimization: A review of algorithms and comparison of software implementations.
{Journal of Global Optimization}, 
volume 56, number 3, 2013.

% LBFGS
\bibitem{LBFGS}
J. Nocedal,
Updating quasi-Newton matrices with limited storage,
Mathematics of Computation, 
volume 35, number 151, 1980.

% Lasso
\bibitem{Lasso variable selection}
R. Tibshirani,
Regression shrinkage and selection via the lasso.
{Journal of the Royal Statistical Society, Series B (Methodological)},
1996.

% gas state equations
\bibitem{state eqns}
J. W. Murdock,
Fundamental fluid mechanics for the practicing engineer,
{CRC Press}, 1993.

\bibitem{Pattern Search Convergence}
Torczon, Virginia. 
"On the convergence of pattern search algorithms." 
SIAM Journal on optimization 7.1 (1997): 1-25.

\bibitem{Pattern Search Convergence MFO}
Booker, Andrew J., et al. 
"A rigorous framework for optimization of expensive functions by surrogates." 
Structural optimization 17.1 (1999): 1-13.

\bibitem{w.o.l.g}
Rasmussen, C.E. and Williams, C.K.I.
"Gaussian Process for Machine Learning."
MIT Press, 2006.

\bibitem{derivative RKHS}
Zhou, Ding-Xuan. "Derivative reproducing properties for kernel methods in learning theory." Journal of computational and Applied Mathematics 220.1 (2008): 456-463.

\bibitem{convergen EI}
Vazquez, Emmanuel, and Julien Bect,
"Convergence properties of the expected improvement algorithm with fixed mean and covariance functions." Journal of Statistical Planning and inference 140.11 (2010): 3088-3095.

\bibitem{converge Bull}
Bull, Adam D. "Convergence rates of efficient global optimization algorithms." The Journal of Machine Learning Research 12 (2011): 2879-2904.

\bibitem{GP bandit}
Srinivas, Niranjan, et al. "Gaussian process optimization in the bandit setting: No regret and experimental design." arXiv preprint arXiv:0912.3995 (2009).

\bibitem{jones test}
D.R.  Jones.   A  taxonomy  of  global  optimization  methods  based  on  response  surfaces.
Journal of Global Optimization.
21(4):345-383, 2001.


\bibitem{RKHS book}
Berlinet, Alain, and Christine Thomas-Agnan. 
"Reproducing kernel Hilbert spaces in probability and statistics."
Springer Science and Business Media, 2011.

\bibitem{practical Bayesian}
Snoek, Jasper, Hugo Larochelle, and Ryan P. Adams. "Practical Bayesian optimization of machine learning algorithms." Advances in neural information processing systems. 2012.

\bibitem{review EI}
Mokus, J. "On Bayesian methods for seeking the extremum." Optimization Techniques IFIP Technical Conference. Springer Berlin Heidelberg, 1975.

\bibitem{coKriging1}
H. Chung and J. J. Alonso. Using gradients to construct cokriging approximation
models for high-dimensional design optimization problems. AIAA Paper, 317:14-
17, 2002.

\bibitem{coKriging2}
A. I. J. Forrester, A. S6bester, and A. J. Keane. Multi-fidelity optimization via
surrogate modelling. Proceedings of the Royal Society A: Mathematical, Physical
and Engineering Science, 463(2088):3251-3269, 2007.


\bibitem{KennedyOhagan1}
Kennedy, Marc C., and Anthony O'Hagan. 
"Predicting the output from a complex computer code when fast approximations are available." 
Biometrika 87.1 (2000): 1-13.

\bibitem{Locatelli}
Locatelli, Marco. "Bayesian algorithms for one-dimensional global optimization." Journal of Global Optimization 10, no. 1 (1997): 57-76.

\bibitem{RKHS aronszajn}
Aronszajn, Nachman. 
"Theory of reproducing kernels." 
Transactions of the American mathematical society (1950): 337-404.

\bibitem{Hilbert space Showalter Book}
Showalter, Ralph E.. 
"Hilbert space methods for partial differential equations."
Courier Corporation, 2010.

\bibitem{jones1998}
Jones, Donald R., Matthias Schonlau, and William J. Welch. 
"Efficient global optimization of expensive black-box functions." 
Journal of Global optimization 13.4 (1998): 455-492.

\bibitem{trustregionconn}
Conn, Andrew R., Katya Scheinberg, and Lus N. Vicente. 
"Global convergence of general derivative-free trust-region algorithms to first-and second-order critical points." 
SIAM Journal on Optimization 20.1 (2009): 387-415.

\bibitem{trustregionwild}
Wild, Stefan M., and Christine Shoemaker. 
"Global convergence of radial basis function trust-region algorithms for derivative-free optimization." 
SIAM Review 55.2 (2013): 349-371.

\bibitem{Alexandrov trust region}
Alexandrov, Natalia M., et al. "A trust-region framework for managing the use of approximation models in optimization." Structural Optimization 15.1 (1998): 16-23.

\bibitem{inexactgradient1}
Carter, Richard G. 
"Numerical experience with a class of algorithms for nonlinear optimization using inexact function and gradient information." 
SIAM Journal on Scientific Computing 14.2 (1993): 368-388.

\bibitem{Mockus Bayesian opt}
J Mockus, V Tiesis, and A Zilinskas
"The application of Bayesian methods for seeking the extreme."
Towards Global Optimization, 2 (1978): 117-129

\bibitem{Rosenbrock}
H. H. Rosenbrock
"An automated method for finding the greatest or least value of a function."
The Computer Journal, 3 (1960)L: 175-184

\bibitem{RemiRosenbrock}
Lam, Remi, Allaire, Douglas L., and Willcox, Karen E.
"Multifidelity Optimization using Statistical Surrogate Modeling for Non-Hierarchical 
Information Sources"
56th AIAA/ASCE/AHS/ASC Structures, Structural Dynamics, and Materials Conference Kissimmee, Florida

\end{thebibliography}


\end{document}
%
%
%% =============== TRASHED SCRIPT =======================
%
%%\indent We give a formal definition of $\mathcal{E}$ below\\
%%\fbox{\parbox{\textwidth}{
%%\begin{definition}
%%    Given a primal model with flux $F(\cdot)$ and a twin model with flux $\tilde{F}(\cdot)$,
%%    their space-time solutions are $u$ and $\tilde{u}$ respectively.
%%    The excited domain $\mathcal{E}$ is the union of all domains 
%%    of $\tilde{F}(\cdot)$ satisfying the property:\\
%%    For any $\epsilon>0$, there exists $\delta>0$, such that:
%%    if $\|\tilde{u}-u\|_1<\delta$, then $\|\nabla \tilde{F} - \nabla F\|_2 < \epsilon$ on 
%%    $\mathcal{E}$.\\
%%    $\|\cdot\|_1$, $\|\cdot\|_2$ are norms to be chosen.
%%\end{definition}
%%}}\\
%%
%%\indent How do we determine $\mathcal{E}$? 
%%To gain some insights, consider
%%a 1D PDE with unknown $F(\cdot)$
%%\begin{equation}
%%    \frac{\partial u}{\partial t} + \frac{\partial F(u)}{\partial x} = 0, \quad
%%    t\in[0,T], x\in(-\infty,\infty)
%%    \label{true model Eu proof}
%%\end{equation}
%%with an initial condition $u_0(x)$.
%%Suppose we want to fit a twin model
%%\begin{equation}
%%    \frac{\partial \tilde{u}}{\partial t} + \frac{\partial \tilde{F}(\tilde{u})}{\partial x} = 0, 
%%    \quad t\in[0,T], x\in(-\infty,\infty)
%%    \label{twin model Eu proof}
%%\end{equation}
%%Our question is: for which $u$ is inferring $F^\prime(u)$ feasible? In other words: if we are able to
%%match $\tilde{u}(t,x)$ with $u(t,x)$, 
%%for which $u$ can we certify $\tilde{F}^\prime$'s accuracy?\\
%%
%%\noindent To answer this question, we give the following theorem.
%%The proof is given in appendix \ref{appendix 1}.
%%The excited domain $\mathcal{E}$ given by Eqn\eqref{excited domain} is illustrated 
%%in Fig \ref{fig:demo_theorem_1}.\\
%%\fbox{\parbox{\textwidth}{
%%\begin{theorem}
%%Given the same initial condition $u_0(x)$, suppose Eqn\eqref{true model Eu proof}'s solution
%%is $u(t,x)$, and \eqref{twin model Eu proof}'s solution is $\tilde{u}(t,x)$, where
%%$t\in[0,T]$, $x\in(-\infty,\infty)$.
%%Assume $F^\prime(u)$ is Lipschitz continuous with constant $L$.
%%Also assume $u_0(x)$ satisfies 
%%\begin{enumerate}
%%    \item $u_{\min}\le u_0(x)\le u_{\max}$
%%    \item $u_0(x)=0$ for $x\in (-\infty, x_1]\bigcup [x_2,\infty)$, $x_2>x_1$
%%    \item $u_0(x) \neq 0$ for all $x$
%%    \item $u_0(x)$ is Lipschitz continuous with constant $K$.
%%\end{enumerate}
%%Define \emph{excited domain}:
%%\begin{equation}
%%    \mathcal{E}(\gamma) \equiv \left\{
%%    u \in [u_{\min},u_{\max}] \left| \exists x\in\mathbb{R}\,
%%    \textrm{such that}\, u=u_0(x) \,\textrm{and}\, \left|\frac{du_0}{dx}\right|>\gamma>0\right.
%%    \right\}
%%    \label{excited domain}
%%\end{equation}
%%For any $\epsilon>0$, there exists $\delta(\gamma)>0$, such that:
%%if $\|\tilde{u}-u\|_{L_\infty} <\delta$, then
%%$\|\tilde{F}^\prime -F^\prime \|_{L_\infty} < \epsilon$ on $\mathcal{E} (\gamma)$.
%%\label{theorem: 1}
%%\end{theorem}
%%}}
%%\\
%%
%%\begin{figure}[H]
%%    \begin{center}
%%        \includegraphics[height=3.3cm]{demo_theorem_1.png}
%%        \caption{Illustration of $\mathcal{E}$ in theorem \ref{theorem: 1}, 
%%                 the blue line shows $u_0(x)$. $\mathcal{E}$ is is colored green.}
%%        \label{fig:demo_theorem_1}
%%    \end{center}
%%\end{figure}
%%
%%\noindent However, in realistic problems, generally $u$ has more than 1 dimensions.
%%Besides, the primal model is a discretized PDE. It will be hard, if not impossible, 
%%to give a close-form expression for $\mathcal{E}$. So we have to 
%%determine $\mathcal{E}$ numerically. 
%
%
%

